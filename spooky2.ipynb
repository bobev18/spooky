{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import mglearn\n",
    "from IPython.display import display\n",
    "\n",
    "#%matplotlib notebook\n",
    "\n",
    "import sklearn\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_rows = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 2) (8392, 1) (8392, 3)\n",
      "{'author'}\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "\n",
    "# use this if in fmi-hw... repo\n",
    "# train = pd.read_csv(\"data/spooky/train.zip\", index_col=['id'])\n",
    "# test = pd.read_csv(\"data/spooky/test.zip\", index_col=['id'])\n",
    "# sample_submission = pd.read_csv(\"data/spooky/sample_submission.zip\", index_col=['id'])\n",
    "\n",
    "train = pd.read_csv(\"data/train.zip\", index_col=['id'])\n",
    "test = pd.read_csv(\"data/test.zip\", index_col=['id'])\n",
    "sample_submission = pd.read_csv(\"data/sample_submission.zip\", index_col=['id'])\n",
    "\n",
    "\n",
    "print(train.shape, test.shape, sample_submission.shape)\n",
    "print(set(train.columns) - set(test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id26305</th>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id17569</th>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id11008</th>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id27763</th>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id12958</th>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text author\n",
       "id                                                               \n",
       "id26305  This process, however, afforded me no means of...    EAP\n",
       "id17569  It never once occurred to me that the fumbling...    HPL\n",
       "id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Идеи за фичъри:\n",
    "    \n",
    "* CountVectorizer, Tfidf\n",
    "* Preprocessing - stop words, lematization\n",
    "* Други фичъри - бр. думи , бр. стоп думи, бр. пунктуация, бр. ГЛАВНИ букви и т.н.\n",
    "* Намиране на общи теми чрез LDA\n",
    "* Word Embeddings с невронни мрежи\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Първо - baseline модел"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.78783701,  0.79635305,  0.79509579])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', CountVectorizer()),\n",
    "    ('clf', LinearSVC())\n",
    "])\n",
    "\n",
    "cross_val_score(pipeline, train.text, train.author, cv=3, n_jobs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "prediction = cross_val_predict(pipeline, train.text, train.author, cv=3, n_jobs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues,\n",
    "                          figsize=(9, 7)):\n",
    "    matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    if normalize:\n",
    "        matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(matrix, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = matrix.max() / 2.\n",
    "    for i, j in itertools.product(range(matrix.shape[0]), range(matrix.shape[1])):\n",
    "        plt.text(j, i, format(matrix[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 size=int((figsize[0] / 10) * 38),\n",
    "                 color=\"white\" if matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.793094642219\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f603fe06ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline.fit(train.text, train.author)\n",
    "authors = pipeline.classes_\n",
    "print(accuracy_score(train.author, prediction))\n",
    "plot_confusion_matrix(train.author, prediction, classes=authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f603f585cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(train.author, prediction, classes=authors, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Да пробваме с RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "pipeline = Pipeline([\n",
    "    ('features', CountVectorizer()),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# print(cross_val_score(pipeline, train.text, train.author, cv=3, n_jobs=3))\n",
    "# print(cross_val_score(pipeline, train.text, train.author, \n",
    "#                 cv=3, n_jobs=3, scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ 0.62300858  0.61798958  0.60551724]\n",
    "\n",
    "[-1.36418837 -1.38716468 -1.43783028]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', CountVectorizer()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "# print(cross_val_score(pipeline, train.text, train.author, cv=3, n_jobs=3))\n",
    "# print(cross_val_score(pipeline, train.text, train.author, cv=3, n_jobs=3, \n",
    "#                       scoring='neg_log_loss'))\n",
    "\n",
    "# Получихме малко по-добри резултати"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ 0.81449142  0.81673307  0.81348659]\n",
    "\n",
    "[-0.47678328 -0.47558895 -0.47131481]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следващото е за да имаме \"explore\" датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "explore = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # бр. думи в текста\n",
    "# explore['words'] = explore.text.apply(lambda s: len(str(s).split()))\n",
    "\n",
    "# # бр. уникални думи\n",
    "# explore['unique_words'] = explore.text.apply(lambda s: len(set(str(s).split())))\n",
    "\n",
    "# # бр. символи\n",
    "# explore['symbols'] = explore.text.str.len()\n",
    "\n",
    "# # бр. уникални символи\n",
    "# explore['unique_symbols'] = explore.text.apply(lambda s: len(set(str(s))))\n",
    "\n",
    "# import string\n",
    "\n",
    "# # бр. главни букви\n",
    "# explore['capital_letters'] = explore.text.apply(lambda s: sum([str.isupper(c) for c in str(s)]))\n",
    "\n",
    "# # бр. на думи съдържащи само главни буква\n",
    "# explore['only_capital_letter_words'] = explore.text.apply(lambda s: sum([str.isupper(w) for w in str(s).split()]))\n",
    "\n",
    "# # средна дължина на дума\n",
    "# explore['average_word_lenght'] = explore.text.apply(lambda s: np.mean([len(w) for w in str(s).split()]))\n",
    "\n",
    "# # бр. цифрите\n",
    "# explore['digits'] = explore.text.apply(lambda s: sum([str.isdigit(c) for c in str(s)]))\n",
    "\n",
    "# # бр. на препинателни знаци\n",
    "# explore[\"punctuation\"] = train.text.apply(lambda s: sum([c in string.punctuation for c in str(s)]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# features_names = list(set(explore.columns) - {'text', 'author'})\n",
    "# print('numeric features', features_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stemmed</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id26305</th>\n",
       "      <td>thi process, however, afford me no mean of asc...</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id17569</th>\n",
       "      <td>It never onc occur to me that the fumbl might ...</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id11008</th>\n",
       "      <td>In hi left hand wa a gold snuff box, from whic...</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id27763</th>\n",
       "      <td>how love is spring As we look from windsor ter...</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id12958</th>\n",
       "      <td>find noth else, not even gold, the superintend...</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   stemmed  \\\n",
       "id                                                           \n",
       "id26305  thi process, however, afford me no mean of asc...   \n",
       "id17569  It never onc occur to me that the fumbl might ...   \n",
       "id11008  In hi left hand wa a gold snuff box, from whic...   \n",
       "id27763  how love is spring As we look from windsor ter...   \n",
       "id12958  find noth else, not even gold, the superintend...   \n",
       "\n",
       "                                                      text  \n",
       "id                                                          \n",
       "id26305  This process, however, afforded me no means of...  \n",
       "id17569  It never once occurred to me that the fumbling...  \n",
       "id11008  In his left hand was a gold snuff box, from wh...  \n",
       "id27763  How lovely is spring As we looked from Windsor...  \n",
       "id12958  Finding nothing else, not even gold, the Super...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stem = PorterStemmer()\n",
    "\n",
    "explore['stemmed'] = explore.text.apply(lambda t: \" \".join([stem.stem(w) for w in t.split()])) \n",
    "\n",
    "explore[['stemmed', 'text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.78477328,  0.78562672,  0.78482759])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', CountVectorizer()),\n",
    "    ('clf', LinearSVC())\n",
    "])\n",
    "\n",
    "cross_val_score(pipeline, explore.stemmed, train.author, cv=3, n_jobs=3)\n",
    "\n",
    "# Резултати от същия pipeline използвайки колона text:\n",
    "# array([ 0.78783701,  0.79635305,  0.79509579])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Допълнителните фичъри не сработиха, стеминга също. \n",
    "\n",
    "Остават да пробвам:\n",
    "\n",
    "* Оптимизиране на модела с CountVectorizer.\n",
    "* Добавяне на още фичъри, от латентни пространства (LDA) - topic modeling.\n",
    "* Word embeddings с невронни мрежи.\n",
    "* Стакинг на класификатори.\n",
    "\n",
    "За сега ще разгледаме само оптимизирането на модела."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Първо да опишем параметрите за търсене в трансфомацията (CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "\n",
    "params_count_word = {\"features__ngram_range\": [(1,1), (1,2), (1,3)],\n",
    "                      \"features__analyzer\": ['word'],\n",
    "                      \"features__max_df\":[1.0, 0.9, 0.8, 0.7, 0.6, 0.5],\n",
    "                      \"features__min_df\":[2, 3, 5, 10],\n",
    "                      \"features__lowercase\": [False, True],\n",
    "                      \"features__stop_words\": [None, stopwords]}\n",
    "\n",
    "params_count_char = {\"features__ngram_range\": [(1,4), (1,5), (1,6)],\n",
    "                      \"features__analyzer\": ['char'],\n",
    "                      \"features__max_df\":[1.0, 0.9, 0.8, 0.7, 0.6, 0.5],\n",
    "                      \"features__min_df\":[2, 3, 5, 10],\n",
    "                      \"features__lowercase\": [False, True],\n",
    "                      \"features__stop_words\": [None, stopwords]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def report(results, n_top=5):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import log_loss\n",
    "def random_search():\n",
    "    params = {\n",
    "        \"clf__C\": [0.01, 0.1, 0.3, 1, 3, 10],\n",
    "        \"clf__class_weight\": [None, 'balanced']\n",
    "    }\n",
    "\n",
    "    params.update(params_count_word)\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('features', CountVectorizer()),\n",
    "        ('clf', LogisticRegression())\n",
    "    ])\n",
    "\n",
    "    random_search = RandomizedSearchCV(pipeline, param_distributions=params, \n",
    "                                       scoring='neg_log_loss',\n",
    "                                       n_iter=20, cv=3, n_jobs=2)\n",
    "\n",
    "    random_search.fit(train.text, train.author)\n",
    "    report(random_search.cv_results_)\n",
    "\n",
    "# random_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "== original ==\n",
    "\n",
    "Model with rank: 1\n",
    "Mean validation score: -0.475 (std: 0.002)\n",
    "Parameters: {'features__stop_words': None, 'features__ngram_range': (1, 1), 'features__lowercase': True, 'features__analyzer': 'word', 'clf__class_weight': None, 'clf__C': 1}\n",
    "\n",
    "Model with rank: 2\n",
    "Mean validation score: -0.482 (std: 0.002)\n",
    "Parameters: {'features__stop_words': None, 'features__ngram_range': (1, 2), 'features__lowercase': True, 'features__analyzer': 'word', 'clf__class_weight': None, 'clf__C': 1}\n",
    "\n",
    "Model with rank: 3\n",
    "Mean validation score: -0.486 (std: 0.001)\n",
    "Parameters: {'features__stop_words': None, 'features__ngram_range': (1, 1), 'features__lowercase': True, 'features__analyzer': 'word', 'clf__class_weight': 'balanced', 'clf__C': 3}\n",
    "\n",
    "Model with rank: 4\n",
    "Mean validation score: -0.508 (std: 0.004)\n",
    "Parameters: {'features__stop_words': None, 'features__ngram_range': (1, 2), 'features__lowercase': False, 'features__analyzer': 'word', 'clf__class_weight': 'balanced', 'clf__C': 0.3}\n",
    "\n",
    "Model with rank: 5\n",
    "Mean validation score: -0.525 (std: 0.004)\n",
    "Parameters: {'features__stop_words': None, 'features__ngram_range': (1, 3), 'features__lowercase': True, 'features__analyzer': 'word', 'clf__class_weight': 'balanced', 'clf__C': 0.3}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "== local results ==\n",
    "\n",
    "Model with rank: 1\n",
    "Mean validation score: -0.474 (std: 0.002)\n",
    "Parameters: {'features__stop_words': None, 'features__ngram_range': (1, 2), 'features__min_df': 2, 'features__max_df': 0.9, 'features__lowercase': True, 'features__analyzer': 'word', 'clf__class_weight': None, 'clf__C': 1}\n",
    "\n",
    "Model with rank: 2\n",
    "Mean validation score: -0.513 (std: 0.002)\n",
    "Parameters: {'features__stop_words': yes, 'features__ngram_range': (1, 3), 'features__min_df': 2, 'features__max_df': 0.5, 'features__lowercase': True, 'features__analyzer': 'word', 'clf__class_weight': None, 'clf__C': 3}\n",
    "\n",
    "Model with rank: 3\n",
    "Mean validation score: -0.515 (std: 0.004)\n",
    "Parameters: {'features__stop_words': None, 'features__ngram_range': (1, 1), 'features__min_df': 2, 'features__max_df': 0.8, 'features__lowercase': True, 'features__analyzer': 'word', 'clf__class_weight': 'balanced', 'clf__C': 0.3}\n",
    "\n",
    "Model with rank: 4\n",
    "Mean validation score: -0.547 (std: 0.003)\n",
    "Parameters: {'features__stop_words': yes, 'features__ngram_range': (1, 1), 'features__min_df': 2, 'features__max_df': 0.8, 'features__lowercase': False, 'features__analyzer': 'word', 'clf__class_weight': None, 'clf__C': 0.3}\n",
    "\n",
    "Model with rank: 5\n",
    "Mean validation score: -0.547 (std: 0.004)\n",
    "Parameters: {'features__stop_words': yes, 'features__ngram_range': (1, 3), 'features__min_df': 3, 'features__max_df': 0.9, 'features__lowercase': False, 'features__analyzer': 'word', 'clf__class_weight': None, 'clf__C': 0.3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tfidf = Term-frequency inverse document-frequency\n",
    "\n",
    "* Идеята е да сложи тежести и значимост на всички думи или n-grams.\n",
    "* Напр. \"новина\" е доста често срещана дума и може да бъде в различни контексти. \n",
    "* За разлика от \"електроенцефалограф\", която е много по-рядко срещана и директно дава медицински контекст.\n",
    "\n",
    "\n",
    "* TF брои колко пъти се среща думата в текущия текст (пасаж, изречение, документ, семпъл).\n",
    "* IDF брои колко пъти тази дума се среща изцяло в корпуса с които тренираме."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def random_search():\n",
    "    params = {\n",
    "        \"clf__C\": [0.01, 0.1, 0.3, 1, 3, 10],\n",
    "        \"clf__class_weight\": [None, 'balanced']\n",
    "    }\n",
    "\n",
    "    params.update(params_count_word)\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('features', TfidfVectorizer()),\n",
    "        ('clf', LogisticRegression())\n",
    "    ])\n",
    "\n",
    "    random_search = RandomizedSearchCV(pipeline, param_distributions=params, \n",
    "                                       scoring='neg_log_loss',\n",
    "                                       n_iter=20, cv=3, n_jobs=2)\n",
    "\n",
    "    random_search.fit(train.text, train.author)\n",
    "    report(random_search.cv_results_)\n",
    "\n",
    "# random_search() # предишния най-добър резултат:  -0.475 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "== original ==\n",
    "\n",
    "Model with rank: 1\n",
    "Mean validation score: -0.469 (std: 0.005)\n",
    "Parameters: {'features__stop_words': None, 'features__ngram_range': (1, 2), 'features__min_df': 2, 'features__max_df': 1.0, 'features__lowercase': True, 'features__analyzer': 'word', 'clf__class_weight': 'balanced', 'clf__C': 10}\n",
    "\n",
    "Model with rank: 2\n",
    "Mean validation score: -0.471 (std: 0.006)\n",
    "Parameters: {'features__stop_words': None, 'features__ngram_range': (1, 2), 'features__min_df': 3, 'features__max_df': 0.5, 'features__lowercase': True, 'features__analyzer': 'word', 'clf__class_weight': None, 'clf__C': 10}\n",
    "\n",
    "Model with rank: 3\n",
    "Mean validation score: -0.483 (std: 0.008)\n",
    "Parameters: {'features__stop_words': None, 'features__ngram_range': (1, 2), 'features__min_df': 5, 'features__max_df': 0.8, 'features__lowercase': False, 'features__analyzer': 'word', 'clf__class_weight': 'balanced', 'clf__C': 10}\n",
    "\n",
    "Model with rank: 4\n",
    "Mean validation score: -0.495 (std: 0.002)\n",
    "Parameters: {'features__stop_words': yes, 'features__ngram_range': (1, 2), 'features__min_df': 2, 'features__max_df': 0.6, 'features__lowercase': True, 'features__analyzer': 'word', 'clf__class_weight': 'balanced', 'clf__C': 10}\n",
    "\n",
    "Model with rank: 5\n",
    "Mean validation score: -0.522 (std: 0.005)\n",
    "Parameters: {'features__stop_words': None, 'features__ngram_range': (1, 3), 'features__min_df': 10, 'features__max_df': 0.5, 'features__lowercase': True, 'features__analyzer': 'word', 'clf__class_weight': 'balanced', 'clf__C': 10}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "== local result ==\n",
    "\n",
    "Model with rank: 1\n",
    "Mean validation score: -0.468 (std: 0.005)\n",
    "Parameters: {'features__stop_words': None, 'features__ngram_range': (1, 2), 'features__min_df': 2, 'features__max_df': 1.0, 'features__lowercase': False, 'features__analyzer': 'word', 'clf__class_weight': None, 'clf__C': 10}\n",
    "\n",
    "Model with rank: 2\n",
    "Mean validation score: -0.484 (std: 0.003)\n",
    "Parameters: {'features__stop_words': yes, 'features__ngram_range': (1, 2), 'features__min_df': 2, 'features__max_df': 0.7, 'features__lowercase': False, 'features__analyzer': 'word', 'clf__class_weight': None, 'clf__C': 10}\n",
    "\n",
    "Model with rank: 3\n",
    "Mean validation score: -0.484 (std: 0.003)\n",
    "Parameters: {'features__stop_words': yes, 'features__ngram_range': (1, 2), 'features__min_df': 2, 'features__max_df': 0.8, 'features__lowercase': False, 'features__analyzer': 'word', 'clf__class_weight': 'balanced', 'clf__C': 10}\n",
    "\n",
    "Model with rank: 4\n",
    "Mean validation score: -0.488 (std: 0.008)\n",
    "Parameters: {'features__stop_words': None, 'features__ngram_range': (1, 1), 'features__min_df': 5, 'features__max_df': 0.7, 'features__lowercase': False, 'features__analyzer': 'word', 'clf__class_weight': None, 'clf__C': 10}\n",
    "\n",
    "Model with rank: 5\n",
    "Mean validation score: -0.495 (std: 0.004)\n",
    "Parameters: {'features__stop_words': yes, 'features__ngram_range': (1, 3), 'features__min_df': 3, 'features__max_df': 1.0, 'features__lowercase': False, 'features__analyzer': 'word', 'clf__class_weight': None, 'clf__C': 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Има леко подобрение в `LogLoss`.\n",
    "\n",
    "Да пробваме да сменим и класификатора с друг класически за класификация на текст: `Naive Bayes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def random_search():\n",
    "    params = {\n",
    "        \"clf__alpha\": [0.01, 0.1, 0.5, 1, 2]\n",
    "    }\n",
    "\n",
    "    params.update(params_count_word)\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('features', TfidfVectorizer()),\n",
    "        ('clf', MultinomialNB())\n",
    "    ])\n",
    "\n",
    "    random_search = RandomizedSearchCV(pipeline, param_distributions=params, \n",
    "                                       scoring='neg_log_loss',\n",
    "                                       n_iter=20, cv=3, n_jobs=2)\n",
    "\n",
    "    random_search.fit(train.text, train.author)\n",
    "    report(random_search.cv_results_)\n",
    "\n",
    "# random_search()  # Предишния най-добър резултат: -0.469"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "== original ==\n",
    "\n",
    "Model with rank: 1\n",
    "Mean validation score: -0.423 (std: 0.003)\n",
    "Parameters: {'features__stop_words': None, 'features__ngram_range': (1, 2), 'features__min_df': 2, 'features__max_df': 0.8, 'features__lowercase': False, 'features__analyzer': 'word', 'clf__alpha': 0.01}\n",
    "\n",
    "Model with rank: 2\n",
    "Mean validation score: -0.465 (std: 0.003)\n",
    "Parameters: {'features__stop_words': None, 'features__ngram_range': (1, 1), 'features__min_df': 3, 'features__max_df': 0.9, 'features__lowercase': True, 'features__analyzer': 'word', 'clf__alpha': 0.01}\n",
    "\n",
    "Model with rank: 3\n",
    "Mean validation score: -0.469 (std: 0.004)\n",
    "Parameters: {'features__stop_words': None, 'features__ngram_range': (1, 3), 'features__min_df': 5, 'features__max_df': 0.9, 'features__lowercase': True, 'features__analyzer': 'word', 'clf__alpha': 0.1}\n",
    "\n",
    "Model with rank: 4\n",
    "Mean validation score: -0.495 (std: 0.002)\n",
    "Parameters: {'features__stop_words': yes, 'features__ngram_range': (1, 3), 'features__min_df': 5, 'features__max_df': 0.8, 'features__lowercase': False, 'features__analyzer': 'word', 'clf__alpha': 0.1}\n",
    "\n",
    "Model with rank: 5\n",
    "Mean validation score: -0.496 (std: 0.004)\n",
    "Parameters: {'features__stop_words': yes, 'features__ngram_range': (1, 3), 'features__min_df': 5, 'features__max_df': 0.6, 'features__lowercase': False, 'features__analyzer': 'word', 'clf__alpha': 0.01}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "== local ==\n",
    "\n",
    "Model with rank: 1\n",
    "Mean validation score: -0.422 (std: 0.003)\n",
    "Parameters: {'features__stop_words': None, 'features__ngram_range': (1, 3), 'features__min_df': 2, 'features__max_df': 0.5, 'features__lowercase': True, 'features__analyzer': 'word', 'clf__alpha': 0.1}\n",
    "\n",
    "Model with rank: 2\n",
    "Mean validation score: -0.496 (std: 0.004)\n",
    "Parameters: {'features__stop_words': yes, 'features__ngram_range': (1, 2), 'features__min_df': 5, 'features__max_df': 0.7, 'features__lowercase': False, 'features__analyzer': 'word', 'clf__alpha': 0.01}\n",
    "\n",
    "Model with rank: 3\n",
    "Mean validation score: -0.502 (std: 0.003)\n",
    "Parameters: {'features__stop_words': yes, 'features__ngram_range': (1, 2), 'features__min_df': 5, 'features__max_df': 0.5, 'features__lowercase': True, 'features__analyzer': 'word', 'clf__alpha': 0.1}\n",
    "\n",
    "Model with rank: 3\n",
    "Mean validation score: -0.502 (std: 0.003)\n",
    "Parameters: {'features__stop_words': yes, 'features__ngram_range': (1, 2), 'features__min_df': 5, 'features__max_df': 0.9, 'features__lowercase': True, 'features__analyzer': 'word', 'clf__alpha': 0.1}\n",
    "\n",
    "Model with rank: 5\n",
    "Mean validation score: -0.524 (std: 0.003)\n",
    "Parameters: {'features__stop_words': None, 'features__ngram_range': (1, 2), 'features__min_df': 10, 'features__max_df': 0.7, 'features__lowercase': False, 'features__analyzer': 'word', 'clf__alpha': 0.1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Тук има още подобрение в метриката.\n",
    "\n",
    "Искам да го пробвам и със stemming.\n",
    "\n",
    "Освен това се вижда, че избира най-ниската предоставена стойност за `alpha`, може би трябва да пробвам с още по-ниски."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def random_search():\n",
    "    params = {\n",
    "        \"clf__alpha\": [0.001, 0.005, 0.01, 0.05, 0.1, 0.3]\n",
    "    }\n",
    "\n",
    "    params.update(params_count_word)\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('features', TfidfVectorizer()),\n",
    "        ('clf', MultinomialNB())\n",
    "    ])\n",
    "\n",
    "    random_search = RandomizedSearchCV(pipeline, param_distributions=params, \n",
    "                                       scoring='neg_log_loss',\n",
    "                                       n_iter=20, cv=3, n_jobs=4)\n",
    "\n",
    "    random_search.fit(explore.stemmed, train.author)\n",
    "    report(random_search.cv_results_)\n",
    "    \n",
    "# random_search()  # -0.423"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Model with rank: 1\n",
    "Mean validation score: -0.438 (std: 0.002)\n",
    "Parameters: {'features__stop_words': None, 'features__ngram_range': (1, 2), 'features__min_df': 2, 'features__max_df': 0.6, 'features__lowercase': False, 'features__analyzer': 'word', 'clf__alpha': 0.01}\n",
    "\n",
    "Model with rank: 2\n",
    "Mean validation score: -0.443 (std: 0.004)\n",
    "Parameters: {'features__stop_words': None, 'features__ngram_range': (1, 3), 'features__min_df': 3, 'features__max_df': 0.6, 'features__lowercase': True, 'features__analyzer': 'word', 'clf__alpha': 0.05}\n",
    "\n",
    "Model with rank: 3\n",
    "Mean validation score: -0.453 (std: 0.002)\n",
    "Parameters: {'features__stop_words': None, 'features__ngram_range': (1, 3), 'features__min_df': 2, 'features__max_df': 1.0, 'features__lowercase': False, 'features__analyzer': 'word', 'clf__alpha': 0.01}\n",
    "\n",
    "Model with rank: 4\n",
    "Mean validation score: -0.471 (std: 0.003)\n",
    "Parameters: {'features__stop_words': None, 'features__ngram_range': (1, 2), 'features__min_df': 5, 'features__max_df': 1.0, 'features__lowercase': False, 'features__analyzer': 'word', 'clf__alpha': 0.01}\n",
    "\n",
    "Model with rank: 5\n",
    "Mean validation score: -0.472 (std: 0.004)\n",
    "Parameters: {'features__stop_words': None, 'features__ngram_range': (1, 3), 'features__min_df': 5, 'features__max_df': 0.5, 'features__lowercase': False, 'features__analyzer': 'word', 'clf__alpha': 0.05}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Dropout, Lambda, Concatenate\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras import regularizers, objectives, metrics\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "documents = train.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['process,',\n",
      "  'however,',\n",
      "  'afforded',\n",
      "  'means',\n",
      "  'ascertaining',\n",
      "  'dimensions',\n",
      "  'might',\n",
      "  'make',\n",
      "  'return',\n",
      "  'point',\n",
      "  'whence',\n",
      "  'set',\n",
      "  'out,',\n",
      "  'without',\n",
      "  'aware',\n",
      "  'perfectly',\n",
      "  'uniform',\n",
      "  'seemed',\n",
      "  'wall.'],\n",
      " ['never', 'occurred', 'fumbling', 'might', 'mere', 'mistake.'],\n",
      " ['left',\n",
      "  'hand',\n",
      "  'gold',\n",
      "  'snuff',\n",
      "  'box,',\n",
      "  'which,',\n",
      "  'hill,',\n",
      "  'cutting',\n",
      "  'manner',\n",
      "  'fantastic',\n",
      "  'steps,',\n",
      "  'took',\n",
      "  'snuff',\n",
      "  'incessantly',\n",
      "  'air',\n",
      "  'greatest',\n",
      "  'possible',\n",
      "  'self',\n",
      "  'satisfaction.'],\n",
      " ['lovely',\n",
      "  'spring',\n",
      "  'looked',\n",
      "  'windsor',\n",
      "  'terrace',\n",
      "  'sixteen',\n",
      "  'fertile',\n",
      "  'counties',\n",
      "  'spread',\n",
      "  'beneath,',\n",
      "  'speckled',\n",
      "  'happy',\n",
      "  'cottages',\n",
      "  'wealthier',\n",
      "  'towns,',\n",
      "  'looked',\n",
      "  'former',\n",
      "  'years,',\n",
      "  'heart',\n",
      "  'cheering',\n",
      "  'fair.'],\n",
      " ['finding',\n",
      "  'nothing',\n",
      "  'else,',\n",
      "  'even',\n",
      "  'gold,',\n",
      "  'superintendent',\n",
      "  'abandoned',\n",
      "  'perplexed',\n",
      "  'look',\n",
      "  'occasionally',\n",
      "  'steals',\n",
      "  'countenance',\n",
      "  'sits',\n",
      "  'thinking',\n",
      "  'desk.'],\n",
      " ['youth',\n",
      "  'passed',\n",
      "  'solitude,',\n",
      "  'best',\n",
      "  'years',\n",
      "  'spent',\n",
      "  'gentle',\n",
      "  'feminine',\n",
      "  'refined',\n",
      "  'groundwork',\n",
      "  'character',\n",
      "  'cannot',\n",
      "  'overcome',\n",
      "  'intense',\n",
      "  'distaste',\n",
      "  'usual',\n",
      "  'brutality',\n",
      "  'exercised',\n",
      "  'board',\n",
      "  'never',\n",
      "  'believed',\n",
      "  'necessary,',\n",
      "  'heard',\n",
      "  'mariner',\n",
      "  'equally',\n",
      "  'noted',\n",
      "  'heart',\n",
      "  'respect',\n",
      "  'obedience',\n",
      "  'paid',\n",
      "  'crew,',\n",
      "  'felt',\n",
      "  'peculiarly',\n",
      "  'fortunate',\n",
      "  'able',\n",
      "  'secure',\n",
      "  'services.'],\n",
      " ['astronomer,',\n",
      "  'perhaps,',\n",
      "  'point,',\n",
      "  'took',\n",
      "  'refuge',\n",
      "  'suggestion',\n",
      "  'non',\n",
      "  'luminosity;',\n",
      "  'analogy',\n",
      "  'suddenly',\n",
      "  'let',\n",
      "  'fall.'],\n",
      " ['surcingle', 'hung', 'body.'],\n",
      " ['knew',\n",
      "  'could',\n",
      "  'say',\n",
      "  'without',\n",
      "  'brought',\n",
      "  'think',\n",
      "  'thus',\n",
      "  'theories',\n",
      "  'since,',\n",
      "  'discussed',\n",
      "  'subject',\n",
      "  'long',\n",
      "  'ago,',\n",
      "  'mentioned',\n",
      "  'singularly,',\n",
      "  'yet',\n",
      "  'little',\n",
      "  'notice,',\n",
      "  'vague',\n",
      "  'guesses',\n",
      "  'noble',\n",
      "  'greek',\n",
      "  'met',\n",
      "  'confirmation',\n",
      "  'late',\n",
      "  'felt',\n",
      "  'could',\n",
      "  'avoid',\n",
      "  'casting',\n",
      "  'eyes',\n",
      "  'upward',\n",
      "  'great',\n",
      "  'orion,',\n",
      "  'certainly',\n",
      "  'expected',\n",
      "  'would',\n",
      "  'so.'],\n",
      " ['confess',\n",
      "  'neither',\n",
      "  'structure',\n",
      "  'languages,',\n",
      "  'code',\n",
      "  'governments,',\n",
      "  'politics',\n",
      "  'various',\n",
      "  'states',\n",
      "  'possessed',\n",
      "  'attractions',\n",
      "  'me.']]\n"
     ]
    }
   ],
   "source": [
    "# remove common words and tokenize\n",
    "# stoplist = set('for a of the and to in'.split())\n",
    "texts = [[word for word in document.lower().split() if word not in stopwords] #was stoplist\n",
    "         for document in documents]\n",
    "\n",
    "# remove words that appear only once\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "\n",
    "from pprint import pprint  # pretty-printer\n",
    "pprint(texts[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "FILENAME = \"processed_texts.pickle\"\n",
    "\n",
    "# create pickle\n",
    "# pickle.dump(texts, open(FILENAME, \"wb\" ))\n",
    "\n",
    "# depicke\n",
    "texts = pickle.load(open(FILENAME, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22328\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save('spooky.dict')  # store the dictionary, for future reference\n",
    "print(len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "# corpora.MmCorpus.serialize('spooky.mm', corpus)  # store to disk, for later use\n",
    "\n",
    "#restore from disk\n",
    "corpus = corpora.mmcorpus.MmCorpus('spooky.mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## similarities.docsim.Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=12)\n",
    "index = similarities.docsim.Similarity('/home/bob/tmp', corpus, num_features=len(dictionary)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(545, 1.0), (1137, 1.0), (1598, 1.0), (1599, 1.0), (1600, 1.0), (1601, 1.0)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[156]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sims = index[tfidf[corpus[0]]]\n",
    "sims = index[corpus[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = list(enumerate(sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.99999982), (1, 0.093658581), (2, 0.0), (3, 0.0), (4, 0.0)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.99999982),\n",
       " (14771, 0.260133),\n",
       " (8578, 0.22941573),\n",
       " (12208, 0.22941573),\n",
       " (12524, 0.22941573)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "    \n",
    "top = sorted(results, key=itemgetter(1), reverse=True)\n",
    "top[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.similarities.docsim.Similarity"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in df:      This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall. \n",
      "\n",
      "as doc:     This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall. \n",
      "\n",
      "as tokens:  ['process,', 'however,', 'afforded', 'means', 'ascertaining', 'dimensions', 'might', 'make', 'return', 'point', 'whence', 'set', 'out,', 'without', 'aware', 'perfectly', 'uniform', 'seemed', 'wall.'] \n",
      "\n",
      "as vec:     [(0, 1.0), (1, 1.0), (2, 1.0), (3, 1.0), (4, 1.0), (5, 1.0), (6, 1.0), (7, 1.0), (8, 1.0), (9, 1.0), (10, 1.0), (11, 1.0), (12, 1.0), (13, 1.0), (14, 1.0), (15, 1.0), (16, 1.0), (17, 1.0), (18, 1.0)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('in df:     ', train.text[0],'\\n')\n",
    "print('as doc:    ', documents[0],'\\n')\n",
    "print('as tokens: ', texts[0],'\\n')\n",
    "print('as vec:    ', corpus[0],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsorted sims to vec[0] [ 0.99999982  0.09365858  0.          0.          0.          0.          0.\n",
      "  0.          0.03673591  0.        ]\n"
     ]
    }
   ],
   "source": [
    "print('unsorted sims to vec[0]', sims[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.\n",
      "1.0 EAP This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall. \n",
      "\n",
      "0.260133 EAP The expression of his smile, however, was by no means unpleasing, as might be supposed; but it had no variation whatever. \n",
      "\n",
      "0.229416 EAP Much, however, might be ascertained. \n",
      "\n",
      "0.229416 EAP I reapproached the wall. \n",
      "\n",
      "0.229416 HPL They seemed to hate and fear him at the same time, and he seemed to return these sentiments. \n",
      "\n",
      "0.229416 EAP There was nothing in this, however, to make him sob. \n",
      "\n",
      "0.216295 EAP At first he stared at me as if he found it impossible to comprehend the witticism of my remark; but as its point seemed slowly to make its way into his brain, his eyes, in the same proportion, seemed protruding from their sockets. \n",
      "\n",
      "0.205196 EAP With my aversion to this cat, however, its partiality for myself seemed to increase. \n",
      "\n",
      "0.205196 EAP For the sisters, however, I could make no excuse. \n",
      "\n",
      "0.205196 EAP I did not care, however, to contest the point with him. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(documents[0])\n",
    "for sim in top[:10]:\n",
    "    print(sim[1], train.author[sim[0]], documents[sim[0]],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чисто технически може да се направи някакво осредняване на резултати - например резултатите от сравнение към documents[0] и сравнение към documents[2] - като и двата записа са \"EAP\". Но не ми се вижда правилно.\n",
    "\n",
    "Не мисля, че посоката е подходяща за нашия случай, защото сравненията са директни, вместо да се извлича \"пространство\" което да генерализира стила/автора. По-добре да проверим какво друго има в gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JoblibValueError\n",
      "___________________________________________________________________________\n",
      "Multiprocessing exception:\n",
      "...........................................................................\n",
      "/usr/lib/python3.6/runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n",
      "    188         sys.exit(msg)\n",
      "    189     main_globals = sys.modules[\"__main__\"].__dict__\n",
      "    190     if alter_argv:\n",
      "    191         sys.argv[0] = mod_spec.origin\n",
      "    192     return _run_code(code, main_globals, None,\n",
      "--> 193                      \"__main__\", mod_spec)\n",
      "        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py')\n",
      "    194 \n",
      "    195 def run_module(mod_name, init_globals=None,\n",
      "    196                run_name=None, alter_sys=False):\n",
      "    197     \"\"\"Execute a module's code without importing it\n",
      "\n",
      "...........................................................................\n",
      "/usr/lib/python3.6/runpy.py in _run_code(code=<code object <module> at 0x7f608565f1e0, file \"/...3.6/site-packages/ipykernel_launcher.py\", line 5>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/home/bob/.local/lib/python3.6/site-packages/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/home/bob/.local/lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/home/bob/.l.../python3.6/site-packages/ipykernel/kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), pkg_name='', script_name=None)\n",
      "     80                        __cached__ = cached,\n",
      "     81                        __doc__ = None,\n",
      "     82                        __loader__ = loader,\n",
      "     83                        __package__ = pkg_name,\n",
      "     84                        __spec__ = mod_spec)\n",
      "---> 85     exec(code, run_globals)\n",
      "        code = <code object <module> at 0x7f608565f1e0, file \"/...3.6/site-packages/ipykernel_launcher.py\", line 5>\n",
      "        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/home/bob/.local/lib/python3.6/site-packages/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/home/bob/.local/lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/home/bob/.l.../python3.6/site-packages/ipykernel/kernelapp.py'>, ...}\n",
      "     86     return run_globals\n",
      "     87 \n",
      "     88 def _run_module_code(code, init_globals=None,\n",
      "     89                     mod_name=None, mod_spec=None,\n",
      "\n",
      "...........................................................................\n",
      "/home/bob/.local/lib/python3.6/site-packages/ipykernel_launcher.py in <module>()\n",
      "     11     # This is added back by InteractiveShellApp.init_path()\n",
      "     12     if sys.path[0] == '':\n",
      "     13         del sys.path[0]\n",
      "     14 \n",
      "     15     from ipykernel import kernelapp as app\n",
      "---> 16     app.launch_new_instance()\n",
      "\n",
      "...........................................................................\n",
      "/home/bob/.local/lib/python3.6/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n",
      "    653 \n",
      "    654         If a global instance already exists, this reinitializes and starts it\n",
      "    655         \"\"\"\n",
      "    656         app = cls.instance(**kwargs)\n",
      "    657         app.initialize(argv)\n",
      "--> 658         app.start()\n",
      "        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n",
      "    659 \n",
      "    660 #-----------------------------------------------------------------------------\n",
      "    661 # utility functions, for convenience\n",
      "    662 #-----------------------------------------------------------------------------\n",
      "\n",
      "...........................................................................\n",
      "/home/bob/.local/lib/python3.6/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n",
      "    473         if self.poller is not None:\n",
      "    474             self.poller.start()\n",
      "    475         self.kernel.start()\n",
      "    476         self.io_loop = ioloop.IOLoop.current()\n",
      "    477         try:\n",
      "--> 478             self.io_loop.start()\n",
      "        self.io_loop.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n",
      "    479         except KeyboardInterrupt:\n",
      "    480             pass\n",
      "    481 \n",
      "    482 launch_new_instance = IPKernelApp.launch_instance\n",
      "\n",
      "...........................................................................\n",
      "/home/bob/.local/lib/python3.6/site-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n",
      "    172             )\n",
      "    173         return loop\n",
      "    174     \n",
      "    175     def start(self):\n",
      "    176         try:\n",
      "--> 177             super(ZMQIOLoop, self).start()\n",
      "        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n",
      "    178         except ZMQError as e:\n",
      "    179             if e.errno == ETERM:\n",
      "    180                 # quietly return on ETERM\n",
      "    181                 pass\n",
      "\n",
      "...........................................................................\n",
      "/home/bob/.local/lib/python3.6/site-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n",
      "    883                 self._events.update(event_pairs)\n",
      "    884                 while self._events:\n",
      "    885                     fd, events = self._events.popitem()\n",
      "    886                     try:\n",
      "    887                         fd_obj, handler_func = self._handlers[fd]\n",
      "--> 888                         handler_func(fd_obj, events)\n",
      "        handler_func = <function wrap.<locals>.null_wrapper>\n",
      "        fd_obj = <zmq.sugar.socket.Socket object>\n",
      "        events = 1\n",
      "    889                     except (OSError, IOError) as e:\n",
      "    890                         if errno_from_exception(e) == errno.EPIPE:\n",
      "    891                             # Happens when the client closes the connection\n",
      "    892                             pass\n",
      "\n",
      "...........................................................................\n",
      "/home/bob/.local/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n",
      "    272         # Fast path when there are no active contexts.\n",
      "    273         def null_wrapper(*args, **kwargs):\n",
      "    274             try:\n",
      "    275                 current_state = _state.contexts\n",
      "    276                 _state.contexts = cap_contexts[0]\n",
      "--> 277                 return fn(*args, **kwargs)\n",
      "        args = (<zmq.sugar.socket.Socket object>, 1)\n",
      "        kwargs = {}\n",
      "    278             finally:\n",
      "    279                 _state.contexts = current_state\n",
      "    280         null_wrapper._wrapped = True\n",
      "    281         return null_wrapper\n",
      "\n",
      "...........................................................................\n",
      "/home/bob/.local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n",
      "    435             # dispatch events:\n",
      "    436             if events & IOLoop.ERROR:\n",
      "    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n",
      "    438                 return\n",
      "    439             if events & IOLoop.READ:\n",
      "--> 440                 self._handle_recv()\n",
      "        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n",
      "    441                 if not self.socket:\n",
      "    442                     return\n",
      "    443             if events & IOLoop.WRITE:\n",
      "    444                 self._handle_send()\n",
      "\n",
      "...........................................................................\n",
      "/home/bob/.local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n",
      "    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n",
      "    468         else:\n",
      "    469             if self._recv_callback:\n",
      "    470                 callback = self._recv_callback\n",
      "    471                 # self._recv_callback = None\n",
      "--> 472                 self._run_callback(callback, msg)\n",
      "        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n",
      "        callback = <function wrap.<locals>.null_wrapper>\n",
      "        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n",
      "    473                 \n",
      "    474         # self.update_state()\n",
      "    475         \n",
      "    476 \n",
      "\n",
      "...........................................................................\n",
      "/home/bob/.local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n",
      "    409         close our socket.\"\"\"\n",
      "    410         try:\n",
      "    411             # Use a NullContext to ensure that all StackContexts are run\n",
      "    412             # inside our blanket exception handler rather than outside.\n",
      "    413             with stack_context.NullContext():\n",
      "--> 414                 callback(*args, **kwargs)\n",
      "        callback = <function wrap.<locals>.null_wrapper>\n",
      "        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n",
      "        kwargs = {}\n",
      "    415         except:\n",
      "    416             gen_log.error(\"Uncaught exception, closing connection.\",\n",
      "    417                           exc_info=True)\n",
      "    418             # Close the socket on an uncaught exception from a user callback\n",
      "\n",
      "...........................................................................\n",
      "/home/bob/.local/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n",
      "    272         # Fast path when there are no active contexts.\n",
      "    273         def null_wrapper(*args, **kwargs):\n",
      "    274             try:\n",
      "    275                 current_state = _state.contexts\n",
      "    276                 _state.contexts = cap_contexts[0]\n",
      "--> 277                 return fn(*args, **kwargs)\n",
      "        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n",
      "        kwargs = {}\n",
      "    278             finally:\n",
      "    279                 _state.contexts = current_state\n",
      "    280         null_wrapper._wrapped = True\n",
      "    281         return null_wrapper\n",
      "\n",
      "...........................................................................\n",
      "/home/bob/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n",
      "    276         if self.control_stream:\n",
      "    277             self.control_stream.on_recv(self.dispatch_control, copy=False)\n",
      "    278 \n",
      "    279         def make_dispatcher(stream):\n",
      "    280             def dispatcher(msg):\n",
      "--> 281                 return self.dispatch_shell(stream, msg)\n",
      "        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n",
      "    282             return dispatcher\n",
      "    283 \n",
      "    284         for s in self.shell_streams:\n",
      "    285             s.on_recv(make_dispatcher(s), copy=False)\n",
      "\n",
      "...........................................................................\n",
      "/home/bob/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': \"from gensim import sklearn_api\\n\\nfrom sklearn.ens..._log_loss'))\\nexcept ValueError as e:\\n    print(e)\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 1, 3, 12, 3, 20, 407993, tzinfo=tzutc()), 'msg_id': '207F737F95354F0BAD382A75B510CD9D', 'msg_type': 'execute_request', 'session': '512E4502520F4C58B500E845BCAB949B', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '207F737F95354F0BAD382A75B510CD9D', 'msg_type': 'execute_request', 'parent_header': {}})\n",
      "    227             self.log.warn(\"Unknown message type: %r\", msg_type)\n",
      "    228         else:\n",
      "    229             self.log.debug(\"%s: %s\", msg_type, msg)\n",
      "    230             self.pre_handler_hook()\n",
      "    231             try:\n",
      "--> 232                 handler(stream, idents, msg)\n",
      "        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n",
      "        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n",
      "        idents = [b'512E4502520F4C58B500E845BCAB949B']\n",
      "        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': \"from gensim import sklearn_api\\n\\nfrom sklearn.ens..._log_loss'))\\nexcept ValueError as e:\\n    print(e)\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 1, 3, 12, 3, 20, 407993, tzinfo=tzutc()), 'msg_id': '207F737F95354F0BAD382A75B510CD9D', 'msg_type': 'execute_request', 'session': '512E4502520F4C58B500E845BCAB949B', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '207F737F95354F0BAD382A75B510CD9D', 'msg_type': 'execute_request', 'parent_header': {}}\n",
      "    233             except Exception:\n",
      "    234                 self.log.error(\"Exception in message handler:\", exc_info=True)\n",
      "    235             finally:\n",
      "    236                 self.post_handler_hook()\n",
      "\n",
      "...........................................................................\n",
      "/home/bob/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'512E4502520F4C58B500E845BCAB949B'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': \"from gensim import sklearn_api\\n\\nfrom sklearn.ens..._log_loss'))\\nexcept ValueError as e:\\n    print(e)\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 1, 3, 12, 3, 20, 407993, tzinfo=tzutc()), 'msg_id': '207F737F95354F0BAD382A75B510CD9D', 'msg_type': 'execute_request', 'session': '512E4502520F4C58B500E845BCAB949B', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '207F737F95354F0BAD382A75B510CD9D', 'msg_type': 'execute_request', 'parent_header': {}})\n",
      "    392         if not silent:\n",
      "    393             self.execution_count += 1\n",
      "    394             self._publish_execute_input(code, parent, self.execution_count)\n",
      "    395 \n",
      "    396         reply_content = self.do_execute(code, silent, store_history,\n",
      "--> 397                                         user_expressions, allow_stdin)\n",
      "        user_expressions = {}\n",
      "        allow_stdin = True\n",
      "    398 \n",
      "    399         # Flush output before sending the reply.\n",
      "    400         sys.stdout.flush()\n",
      "    401         sys.stderr.flush()\n",
      "\n",
      "...........................................................................\n",
      "/home/bob/.local/lib/python3.6/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code=\"from gensim import sklearn_api\\n\\nfrom sklearn.ens..._log_loss'))\\nexcept ValueError as e:\\n    print(e)\", silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n",
      "    203 \n",
      "    204         self._forward_input(allow_stdin)\n",
      "    205 \n",
      "    206         reply_content = {}\n",
      "    207         try:\n",
      "--> 208             res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "        res = undefined\n",
      "        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n",
      "        code = \"from gensim import sklearn_api\\n\\nfrom sklearn.ens..._log_loss'))\\nexcept ValueError as e:\\n    print(e)\"\n",
      "        store_history = True\n",
      "        silent = False\n",
      "    209         finally:\n",
      "    210             self._restore_input()\n",
      "    211 \n",
      "    212         if res.error_before_exec is not None:\n",
      "\n",
      "...........................................................................\n",
      "/home/bob/.local/lib/python3.6/site-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=(\"from gensim import sklearn_api\\n\\nfrom sklearn.ens..._log_loss'))\\nexcept ValueError as e:\\n    print(e)\",), **kwargs={'silent': False, 'store_history': True})\n",
      "    528             )\n",
      "    529         self.payload_manager.write_payload(payload)\n",
      "    530 \n",
      "    531     def run_cell(self, *args, **kwargs):\n",
      "    532         self._last_traceback = None\n",
      "--> 533         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n",
      "        args = (\"from gensim import sklearn_api\\n\\nfrom sklearn.ens..._log_loss'))\\nexcept ValueError as e:\\n    print(e)\",)\n",
      "        kwargs = {'silent': False, 'store_history': True}\n",
      "    534 \n",
      "    535     def _showtraceback(self, etype, evalue, stb):\n",
      "    536         # try to preserve ordering of tracebacks and print statements\n",
      "    537         sys.stdout.flush()\n",
      "\n",
      "...........................................................................\n",
      "/home/bob/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=\"from gensim import sklearn_api\\n\\nfrom sklearn.ens..._log_loss'))\\nexcept ValueError as e:\\n    print(e)\", store_history=True, silent=False, shell_futures=True)\n",
      "   2723                 self.displayhook.exec_result = result\n",
      "   2724 \n",
      "   2725                 # Execute the user code\n",
      "   2726                 interactivity = \"none\" if silent else self.ast_node_interactivity\n",
      "   2727                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n",
      "-> 2728                    interactivity=interactivity, compiler=compiler, result=result)\n",
      "        interactivity = 'last_expr'\n",
      "        compiler = <IPython.core.compilerop.CachingCompiler object>\n",
      "   2729                 \n",
      "   2730                 self.last_execution_succeeded = not has_raised\n",
      "   2731                 self.last_execution_result = result\n",
      "   2732 \n",
      "\n",
      "...........................................................................\n",
      "/home/bob/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.Assign object>, <_ast.Try object>], cell_name='<ipython-input-74-fc4cc2318033>', interactivity='none', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 7f60282551d0, executi..._before_exec=None error_in_exec=None result=None>)\n",
      "   2845 \n",
      "   2846         try:\n",
      "   2847             for i, node in enumerate(to_run_exec):\n",
      "   2848                 mod = ast.Module([node])\n",
      "   2849                 code = compiler(mod, cell_name, \"exec\")\n",
      "-> 2850                 if self.run_code(code, result):\n",
      "        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n",
      "        code = <code object <module> at 0x7f6026d6ae40, file \"<ipython-input-74-fc4cc2318033>\", line 9>\n",
      "        result = <ExecutionResult object at 7f60282551d0, executi..._before_exec=None error_in_exec=None result=None>\n",
      "   2851                     return True\n",
      "   2852 \n",
      "   2853             for i, node in enumerate(to_run_interactive):\n",
      "   2854                 mod = ast.Interactive([node])\n",
      "\n",
      "...........................................................................\n",
      "/home/bob/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7f6026d6ae40, file \"<ipython-input-74-fc4cc2318033>\", line 9>, result=<ExecutionResult object at 7f60282551d0, executi..._before_exec=None error_in_exec=None result=None>)\n",
      "   2905         outflag = True  # happens in more places, so it's easier as default\n",
      "   2906         try:\n",
      "   2907             try:\n",
      "   2908                 self.hooks.pre_run_code_hook()\n",
      "   2909                 #rprint('Running code', repr(code_obj)) # dbg\n",
      "-> 2910                 exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "        code_obj = <code object <module> at 0x7f6026d6ae40, file \"<ipython-input-74-fc4cc2318033>\", line 9>\n",
      "        self.user_global_ns = {'Adam': <class 'keras.optimizers.Adam'>, 'Concatenate': <class 'keras.layers.merge.Concatenate'>, 'Conv2D': <class 'keras.layers.convolutional.Conv2D'>, 'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'Dense': <class 'keras.layers.core.Dense'>, 'Dropout': <class 'keras.layers.core.Dropout'>, 'FILENAME': 'processed_texts.pickle', 'HashingVectorizer': <class 'sklearn.feature_extraction.text.HashingVectorizer'>, 'In': ['', \"# corpus = [dictionary.doc2bow(text) for text in...k\\ncorpus = corpora.mmcorpus.MmCorpus('spooky.mm')\", \"import numpy as np\\nimport matplotlib.pyplot as p...lib', 'inline')\\n\\npd.options.display.max_rows = 10\", '# Dataset\\n\\n# use this if in fmi-hw... repo\\n# tra...pe)\\nprint(set(train.columns) - set(test.columns))', 'train.head(5)', 'from sklearn.pipeline import Pipeline\\nfrom sklea...om sklearn.model_selection import cross_val_score', \"pipeline = Pipeline([\\n    ('features', CountVect...peline, train.text, train.author, cv=3, n_jobs=3)\", 'from sklearn.model_selection import cross_val_pr...peline, train.text, train.author, cv=3, n_jobs=3)', \"import itertools\\nfrom sklearn.metrics import con...l('True label')\\n    plt.xlabel('Predicted label')\", 'pipeline.fit(train.text, train.author)\\nauthors =...matrix(train.author, prediction, classes=authors)', 'plot_confusion_matrix(train.author, prediction, classes=authors, normalize=True)', \"from sklearn.ensemble import RandomForestClassif...         cv=3, n_jobs=3, scoring='neg_log_loss'))\", \"from sklearn.linear_model import LogisticRegress...log_loss'))\\n\\n# Получихме малко по-добри резултати\", 'explore = train.copy()', \"# # бр. думи в текста\\n# explore['words'] = explo... sum([c in string.punctuation for c in str(s)]) )\", \"import nltk\\n# nltk.download('stopwords')\\nstopwords = nltk.corpus.stopwords.words('english')\", \"import nltk\\nnltk.download('stopwords')\\nstopwords = nltk.corpus.stopwords.words('english')\", \"import nltk\\n# nltk.download('stopwords')\\nstopwords = nltk.corpus.stopwords.words('english')\", \"# features_names = list(set(explore.columns) - {...or'})\\n# print('numeric features', features_names)\", \"from nltk.stem import WordNetLemmatizer\\nfrom nlt....split()])) \\n\\nexplore[['stemmed', 'text']].head()\", ...], 'Input': <function Input>, ...}\n",
      "        self.user_ns = {'Adam': <class 'keras.optimizers.Adam'>, 'Concatenate': <class 'keras.layers.merge.Concatenate'>, 'Conv2D': <class 'keras.layers.convolutional.Conv2D'>, 'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'Dense': <class 'keras.layers.core.Dense'>, 'Dropout': <class 'keras.layers.core.Dropout'>, 'FILENAME': 'processed_texts.pickle', 'HashingVectorizer': <class 'sklearn.feature_extraction.text.HashingVectorizer'>, 'In': ['', \"# corpus = [dictionary.doc2bow(text) for text in...k\\ncorpus = corpora.mmcorpus.MmCorpus('spooky.mm')\", \"import numpy as np\\nimport matplotlib.pyplot as p...lib', 'inline')\\n\\npd.options.display.max_rows = 10\", '# Dataset\\n\\n# use this if in fmi-hw... repo\\n# tra...pe)\\nprint(set(train.columns) - set(test.columns))', 'train.head(5)', 'from sklearn.pipeline import Pipeline\\nfrom sklea...om sklearn.model_selection import cross_val_score', \"pipeline = Pipeline([\\n    ('features', CountVect...peline, train.text, train.author, cv=3, n_jobs=3)\", 'from sklearn.model_selection import cross_val_pr...peline, train.text, train.author, cv=3, n_jobs=3)', \"import itertools\\nfrom sklearn.metrics import con...l('True label')\\n    plt.xlabel('Predicted label')\", 'pipeline.fit(train.text, train.author)\\nauthors =...matrix(train.author, prediction, classes=authors)', 'plot_confusion_matrix(train.author, prediction, classes=authors, normalize=True)', \"from sklearn.ensemble import RandomForestClassif...         cv=3, n_jobs=3, scoring='neg_log_loss'))\", \"from sklearn.linear_model import LogisticRegress...log_loss'))\\n\\n# Получихме малко по-добри резултати\", 'explore = train.copy()', \"# # бр. думи в текста\\n# explore['words'] = explo... sum([c in string.punctuation for c in str(s)]) )\", \"import nltk\\n# nltk.download('stopwords')\\nstopwords = nltk.corpus.stopwords.words('english')\", \"import nltk\\nnltk.download('stopwords')\\nstopwords = nltk.corpus.stopwords.words('english')\", \"import nltk\\n# nltk.download('stopwords')\\nstopwords = nltk.corpus.stopwords.words('english')\", \"# features_names = list(set(explore.columns) - {...or'})\\n# print('numeric features', features_names)\", \"from nltk.stem import WordNetLemmatizer\\nfrom nlt....split()])) \\n\\nexplore[['stemmed', 'text']].head()\", ...], 'Input': <function Input>, ...}\n",
      "   2911             finally:\n",
      "   2912                 # Reset our crash handler in place\n",
      "   2913                 sys.excepthook = old_excepthook\n",
      "   2914         except SystemExit as e:\n",
      "\n",
      "...........................................................................\n",
      "/home/bob/gits/spooky/<ipython-input-74-fc4cc2318033> in <module>()\n",
      "      5     ('features', sklearn_api.text2bow.Text2BowTransformer()),\n",
      "      6     ('clf', RandomForestClassifier())\n",
      "      7 ])\n",
      "      8 \n",
      "      9 try:\n",
      "---> 10     print(cross_val_score(pipeline, train.text, train.author, cv=3, n_jobs=3))\n",
      "     11     print(cross_val_score(pipeline, train.text, train.author, \n",
      "     12                     cv=3, n_jobs=3, scoring='neg_log_loss'))\n",
      "     13 except ValueError as e:\n",
      "     14     print(e)\n",
      "\n",
      "...........................................................................\n",
      "/home/bob/.local/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in cross_val_score(estimator=Pipeline(memory=None,\n",
      "     steps=[('features', T...None, verbose=0,\n",
      "            warm_start=False))]), X=id\n",
      "id26305    This process, however, afforded me...d it ...\n",
      "Name: text, Length: 19579, dtype: object, y=id\n",
      "id26305    EAP\n",
      "id17569    HPL\n",
      "id11008    EAP\n",
      "...   HPL\n",
      "Name: author, Length: 19579, dtype: object, groups=None, scoring=None, cv=3, n_jobs=3, verbose=0, fit_params=None, pre_dispatch='2*n_jobs')\n",
      "    337     cv_results = cross_validate(estimator=estimator, X=X, y=y, groups=groups,\n",
      "    338                                 scoring={'score': scorer}, cv=cv,\n",
      "    339                                 return_train_score=False,\n",
      "    340                                 n_jobs=n_jobs, verbose=verbose,\n",
      "    341                                 fit_params=fit_params,\n",
      "--> 342                                 pre_dispatch=pre_dispatch)\n",
      "        pre_dispatch = '2*n_jobs'\n",
      "    343     return cv_results['test_score']\n",
      "    344 \n",
      "    345 \n",
      "    346 def _fit_and_score(estimator, X, y, scorer, train, test, verbose,\n",
      "\n",
      "...........................................................................\n",
      "/home/bob/.local/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in cross_validate(estimator=Pipeline(memory=None,\n",
      "     steps=[('features', T...None, verbose=0,\n",
      "            warm_start=False))]), X=id\n",
      "id26305    This process, however, afforded me...d it ...\n",
      "Name: text, Length: 19579, dtype: object, y=id\n",
      "id26305    EAP\n",
      "id17569    HPL\n",
      "id11008    EAP\n",
      "...   HPL\n",
      "Name: author, Length: 19579, dtype: object, groups=None, scoring={'score': <function _passthrough_scorer>}, cv=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=3, verbose=0, fit_params=None, pre_dispatch='2*n_jobs', return_train_score=False)\n",
      "    201     scores = parallel(\n",
      "    202         delayed(_fit_and_score)(\n",
      "    203             clone(estimator), X, y, scorers, train, test, verbose, None,\n",
      "    204             fit_params, return_train_score=return_train_score,\n",
      "    205             return_times=True)\n",
      "--> 206         for train, test in cv.split(X, y, groups))\n",
      "        cv.split = <bound method StratifiedKFold.split of Stratifie...ld(n_splits=3, random_state=None, shuffle=False)>\n",
      "        X = id\n",
      "id26305    This process, however, afforded me...d it ...\n",
      "Name: text, Length: 19579, dtype: object\n",
      "        y = id\n",
      "id26305    EAP\n",
      "id17569    HPL\n",
      "id11008    EAP\n",
      "...   HPL\n",
      "Name: author, Length: 19579, dtype: object\n",
      "        groups = None\n",
      "    207 \n",
      "    208     if return_train_score:\n",
      "    209         train_scores, test_scores, fit_times, score_times = zip(*scores)\n",
      "    210         train_scores = _aggregate_score_dicts(train_scores)\n",
      "\n",
      "...........................................................................\n",
      "/home/bob/.local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=3), iterable=<generator object cross_validate.<locals>.<genexpr>>)\n",
      "    784             if pre_dispatch == \"all\" or n_jobs == 1:\n",
      "    785                 # The iterable was consumed all at once by the above for loop.\n",
      "    786                 # No need to wait for async callbacks to trigger to\n",
      "    787                 # consumption.\n",
      "    788                 self._iterating = False\n",
      "--> 789             self.retrieve()\n",
      "        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=3)>\n",
      "    790             # Make sure that we get a last message telling us we are done\n",
      "    791             elapsed_time = time.time() - self._start_time\n",
      "    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n",
      "    793                         (len(self._output), len(self._output),\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "Sub-process traceback:\n",
      "---------------------------------------------------------------------------\n",
      "ValueError                                         Wed Jan  3 04:03:23 2018\n",
      "PID: 18552                                   Python 3.6.3: /usr/bin/python3\n",
      "...........................................................................\n",
      "/home/bob/.local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n",
      "    126     def __init__(self, iterator_slice):\n",
      "    127         self.items = list(iterator_slice)\n",
      "    128         self._size = len(self.items)\n",
      "    129 \n",
      "    130     def __call__(self):\n",
      "--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n",
      "        self.items = [(<function _fit_and_score>, (Pipeline(memory=None,\n",
      "     steps=[('features', T...None, verbose=0,\n",
      "            warm_start=False))]), id\n",
      "id26305    This process, however, afforded me...d it ...\n",
      "Name: text, Length: 19579, dtype: object, id\n",
      "id26305    EAP\n",
      "id17569    HPL\n",
      "id11008    EAP\n",
      "...   HPL\n",
      "Name: author, Length: 19579, dtype: object, {'score': <function _passthrough_scorer>}, array([ 6475,  6476,  6478, ..., 19576, 19577, 19578]), array([   0,    1,    2, ..., 6604, 6605, 6606]), 0, None, None), {'return_times': True, 'return_train_score': False})]\n",
      "    132 \n",
      "    133     def __len__(self):\n",
      "    134         return self._size\n",
      "    135 \n",
      "\n",
      "...........................................................................\n",
      "/home/bob/.local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n",
      "    126     def __init__(self, iterator_slice):\n",
      "    127         self.items = list(iterator_slice)\n",
      "    128         self._size = len(self.items)\n",
      "    129 \n",
      "    130     def __call__(self):\n",
      "--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n",
      "        func = <function _fit_and_score>\n",
      "        args = (Pipeline(memory=None,\n",
      "     steps=[('features', T...None, verbose=0,\n",
      "            warm_start=False))]), id\n",
      "id26305    This process, however, afforded me...d it ...\n",
      "Name: text, Length: 19579, dtype: object, id\n",
      "id26305    EAP\n",
      "id17569    HPL\n",
      "id11008    EAP\n",
      "...   HPL\n",
      "Name: author, Length: 19579, dtype: object, {'score': <function _passthrough_scorer>}, array([ 6475,  6476,  6478, ..., 19576, 19577, 19578]), array([   0,    1,    2, ..., 6604, 6605, 6606]), 0, None, None)\n",
      "        kwargs = {'return_times': True, 'return_train_score': False}\n",
      "    132 \n",
      "    133     def __len__(self):\n",
      "    134         return self._size\n",
      "    135 \n",
      "\n",
      "...........................................................................\n",
      "/home/bob/.local/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(memory=None,\n",
      "     steps=[('features', T...None, verbose=0,\n",
      "            warm_start=False))]), X=id\n",
      "id26305    This process, however, afforded me...d it ...\n",
      "Name: text, Length: 19579, dtype: object, y=id\n",
      "id26305    EAP\n",
      "id17569    HPL\n",
      "id11008    EAP\n",
      "...   HPL\n",
      "Name: author, Length: 19579, dtype: object, scorer={'score': <function _passthrough_scorer>}, train=array([ 6475,  6476,  6478, ..., 19576, 19577, 19578]), test=array([   0,    1,    2, ..., 6604, 6605, 6606]), verbose=0, parameters=None, fit_params={}, return_train_score=False, return_parameters=False, return_n_test_samples=False, return_times=True, error_score='raise')\n",
      "    453 \n",
      "    454     try:\n",
      "    455         if y_train is None:\n",
      "    456             estimator.fit(X_train, **fit_params)\n",
      "    457         else:\n",
      "--> 458             estimator.fit(X_train, y_train, **fit_params)\n",
      "        estimator.fit = <bound method Pipeline.fit of Pipeline(memory=No...one, verbose=0,\n",
      "            warm_start=False))])>\n",
      "        X_train = id\n",
      "id15739    Its close resemblance to the medic...d it ...\n",
      "Name: text, Length: 13051, dtype: object\n",
      "        y_train = id\n",
      "id15739    EAP\n",
      "id19588    EAP\n",
      "id11059    EAP\n",
      "...   HPL\n",
      "Name: author, Length: 13051, dtype: object\n",
      "        fit_params = {}\n",
      "    459 \n",
      "    460     except Exception as e:\n",
      "    461         # Note fit time as time until error\n",
      "    462         fit_time = time.time() - start_time\n",
      "\n",
      "...........................................................................\n",
      "/home/bob/.local/lib/python3.6/site-packages/sklearn/pipeline.py in fit(self=Pipeline(memory=None,\n",
      "     steps=[('features', T...None, verbose=0,\n",
      "            warm_start=False))]), X=id\n",
      "id15739    Its close resemblance to the medic...d it ...\n",
      "Name: text, Length: 13051, dtype: object, y=id\n",
      "id15739    EAP\n",
      "id19588    EAP\n",
      "id11059    EAP\n",
      "...   HPL\n",
      "Name: author, Length: 13051, dtype: object, **fit_params={})\n",
      "    245         self : Pipeline\n",
      "    246             This estimator\n",
      "    247         \"\"\"\n",
      "    248         Xt, fit_params = self._fit(X, y, **fit_params)\n",
      "    249         if self._final_estimator is not None:\n",
      "--> 250             self._final_estimator.fit(Xt, y, **fit_params)\n",
      "        self._final_estimator.fit = <bound method BaseForest.fit of RandomForestClas...e=None, verbose=0,\n",
      "            warm_start=False)>\n",
      "        Xt = [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 2), (11, 2), (12, 1), (13, 1)], [(10, 1), (11, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1)], [(1, 1), (10, 5), (14, 1), (18, 1), (19, 1), (22, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 2), (34, 1), (35, 1), (36, 1), (37, 2), (38, 1), ...], [(10, 1), (19, 2), (23, 1), (30, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 2), (56, 1), (57, 2), (58, 1), (59, 1)], [(10, 2), (36, 1), (47, 2), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 2), (66, 1), (67, 2), (68, 1), (69, 2), (70, 1)], [(47, 1), (53, 1), (61, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1)], [(10, 7), (11, 3), (18, 1), (22, 1), (31, 2), (37, 3), (46, 1), (64, 1), (65, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 2), (88, 2), ...], [(10, 1), (19, 1), (22, 1), (24, 1), (29, 1), (30, 1), (47, 1), (51, 1), (53, 1), (56, 1), (61, 1), (102, 1), (113, 1), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 1), ...], [(10, 1), (11, 1), (30, 1), (31, 1), (47, 2), (52, 2), (61, 1), (91, 1), (98, 1), (103, 2), (124, 1), (125, 1), (126, 1), (127, 1), (128, 1), (129, 1), (130, 2), (131, 1), (132, 1)], [(10, 5), (19, 3), (22, 2), (49, 1), (53, 1), (61, 2), (104, 1), (133, 1), (134, 1), (135, 1), (136, 1), (137, 1), (138, 1), (139, 1), (140, 1), (141, 1), (142, 1), (143, 1), (144, 1), (145, 1), ...], [(10, 1), (11, 2), (61, 1), (72, 1), (155, 1), (156, 1), (157, 1), (158, 1), (159, 1), (160, 1), (161, 1), (162, 1), (163, 1), (164, 1), (165, 1), (166, 1)], [(10, 4), (11, 1), (19, 1), (22, 2), (29, 2), (53, 1), (57, 1), (61, 1), (167, 1), (168, 1), (169, 1), (170, 1), (171, 1), (172, 1), (173, 1), (174, 1), (175, 1), (176, 1), (177, 1), (178, 1), ...], [(10, 1), (15, 1), (19, 3), (24, 3), (29, 4), (41, 1), (49, 1), (56, 1), (57, 2), (61, 1), (165, 1), (190, 1), (191, 1), (192, 1), (193, 1), (194, 1), (195, 1), (196, 1), (197, 1), (198, 1), ...], [(10, 5), (11, 1), (19, 3), (22, 2), (24, 1), (30, 1), (47, 2), (61, 2), (65, 1), (66, 1), (71, 1), (72, 1), (103, 1), (111, 1), (124, 1), (131, 1), (217, 1), (218, 1), (219, 1), (220, 1), ...], [(10, 4), (11, 2), (19, 4), (22, 1), (24, 2), (29, 2), (36, 1), (45, 2), (47, 1), (53, 3), (56, 2), (61, 2), (69, 2), (112, 1), (127, 1), (241, 1), (242, 1), (243, 1), (244, 1), (245, 2), ...], [(10, 1), (11, 2), (19, 1), (24, 1), (30, 1), (45, 1), (47, 2), (53, 1), (56, 1), (69, 1), (104, 1), (112, 1), (131, 1), (182, 1), (185, 1), (270, 1), (271, 1), (277, 1), (278, 1), (279, 1), ...], [(10, 3), (11, 2), (22, 1), (29, 3), (36, 2), (53, 1), (64, 1), (65, 1), (91, 1), (105, 2), (141, 1), (145, 1), (213, 1), (219, 1), (222, 1), (290, 1), (291, 1), (292, 1), (293, 1), (294, 1), ...], [(10, 1), (29, 2), (37, 1), (53, 1), (61, 1), (64, 1), (237, 1), (305, 1), (306, 1), (307, 1), (308, 1), (309, 1), (310, 1), (311, 1), (312, 1), (313, 1), (314, 1), (315, 1)], [(10, 1), (316, 1), (317, 1), (318, 1)], [(18, 2), (36, 1), (52, 1), (92, 1), (130, 1), (319, 1), (320, 1), (321, 1), (322, 1)], ...]\n",
      "        y = id\n",
      "id15739    EAP\n",
      "id19588    EAP\n",
      "id11059    EAP\n",
      "...   HPL\n",
      "Name: author, Length: 13051, dtype: object\n",
      "        fit_params = {}\n",
      "    251         return self\n",
      "    252 \n",
      "    253     def fit_transform(self, X, y=None, **fit_params):\n",
      "    254         \"\"\"Fit the model and transform with the final estimator\n",
      "\n",
      "...........................................................................\n",
      "/home/bob/.local/lib/python3.6/site-packages/sklearn/ensemble/forest.py in fit(self=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n",
      "            warm_start=False), X=[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 2), (11, 2), (12, 1), (13, 1)], [(10, 1), (11, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1)], [(1, 1), (10, 5), (14, 1), (18, 1), (19, 1), (22, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 2), (34, 1), (35, 1), (36, 1), (37, 2), (38, 1), ...], [(10, 1), (19, 2), (23, 1), (30, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 2), (56, 1), (57, 2), (58, 1), (59, 1)], [(10, 2), (36, 1), (47, 2), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 2), (66, 1), (67, 2), (68, 1), (69, 2), (70, 1)], [(47, 1), (53, 1), (61, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1)], [(10, 7), (11, 3), (18, 1), (22, 1), (31, 2), (37, 3), (46, 1), (64, 1), (65, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 2), (88, 2), ...], [(10, 1), (19, 1), (22, 1), (24, 1), (29, 1), (30, 1), (47, 1), (51, 1), (53, 1), (56, 1), (61, 1), (102, 1), (113, 1), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 1), ...], [(10, 1), (11, 1), (30, 1), (31, 1), (47, 2), (52, 2), (61, 1), (91, 1), (98, 1), (103, 2), (124, 1), (125, 1), (126, 1), (127, 1), (128, 1), (129, 1), (130, 2), (131, 1), (132, 1)], [(10, 5), (19, 3), (22, 2), (49, 1), (53, 1), (61, 2), (104, 1), (133, 1), (134, 1), (135, 1), (136, 1), (137, 1), (138, 1), (139, 1), (140, 1), (141, 1), (142, 1), (143, 1), (144, 1), (145, 1), ...], [(10, 1), (11, 2), (61, 1), (72, 1), (155, 1), (156, 1), (157, 1), (158, 1), (159, 1), (160, 1), (161, 1), (162, 1), (163, 1), (164, 1), (165, 1), (166, 1)], [(10, 4), (11, 1), (19, 1), (22, 2), (29, 2), (53, 1), (57, 1), (61, 1), (167, 1), (168, 1), (169, 1), (170, 1), (171, 1), (172, 1), (173, 1), (174, 1), (175, 1), (176, 1), (177, 1), (178, 1), ...], [(10, 1), (15, 1), (19, 3), (24, 3), (29, 4), (41, 1), (49, 1), (56, 1), (57, 2), (61, 1), (165, 1), (190, 1), (191, 1), (192, 1), (193, 1), (194, 1), (195, 1), (196, 1), (197, 1), (198, 1), ...], [(10, 5), (11, 1), (19, 3), (22, 2), (24, 1), (30, 1), (47, 2), (61, 2), (65, 1), (66, 1), (71, 1), (72, 1), (103, 1), (111, 1), (124, 1), (131, 1), (217, 1), (218, 1), (219, 1), (220, 1), ...], [(10, 4), (11, 2), (19, 4), (22, 1), (24, 2), (29, 2), (36, 1), (45, 2), (47, 1), (53, 3), (56, 2), (61, 2), (69, 2), (112, 1), (127, 1), (241, 1), (242, 1), (243, 1), (244, 1), (245, 2), ...], [(10, 1), (11, 2), (19, 1), (24, 1), (30, 1), (45, 1), (47, 2), (53, 1), (56, 1), (69, 1), (104, 1), (112, 1), (131, 1), (182, 1), (185, 1), (270, 1), (271, 1), (277, 1), (278, 1), (279, 1), ...], [(10, 3), (11, 2), (22, 1), (29, 3), (36, 2), (53, 1), (64, 1), (65, 1), (91, 1), (105, 2), (141, 1), (145, 1), (213, 1), (219, 1), (222, 1), (290, 1), (291, 1), (292, 1), (293, 1), (294, 1), ...], [(10, 1), (29, 2), (37, 1), (53, 1), (61, 1), (64, 1), (237, 1), (305, 1), (306, 1), (307, 1), (308, 1), (309, 1), (310, 1), (311, 1), (312, 1), (313, 1), (314, 1), (315, 1)], [(10, 1), (316, 1), (317, 1), (318, 1)], [(18, 2), (36, 1), (52, 1), (92, 1), (130, 1), (319, 1), (320, 1), (321, 1), (322, 1)], ...], y=id\n",
      "id15739    EAP\n",
      "id19588    EAP\n",
      "id11059    EAP\n",
      "...   HPL\n",
      "Name: author, Length: 13051, dtype: object, sample_weight=None)\n",
      "    242         -------\n",
      "    243         self : object\n",
      "    244             Returns self.\n",
      "    245         \"\"\"\n",
      "    246         # Validate or convert input data\n",
      "--> 247         X = check_array(X, accept_sparse=\"csc\", dtype=DTYPE)\n",
      "        X = [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 2), (11, 2), (12, 1), (13, 1)], [(10, 1), (11, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1)], [(1, 1), (10, 5), (14, 1), (18, 1), (19, 1), (22, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 2), (34, 1), (35, 1), (36, 1), (37, 2), (38, 1), ...], [(10, 1), (19, 2), (23, 1), (30, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 2), (56, 1), (57, 2), (58, 1), (59, 1)], [(10, 2), (36, 1), (47, 2), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 2), (66, 1), (67, 2), (68, 1), (69, 2), (70, 1)], [(47, 1), (53, 1), (61, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1)], [(10, 7), (11, 3), (18, 1), (22, 1), (31, 2), (37, 3), (46, 1), (64, 1), (65, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 2), (88, 2), ...], [(10, 1), (19, 1), (22, 1), (24, 1), (29, 1), (30, 1), (47, 1), (51, 1), (53, 1), (56, 1), (61, 1), (102, 1), (113, 1), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 1), ...], [(10, 1), (11, 1), (30, 1), (31, 1), (47, 2), (52, 2), (61, 1), (91, 1), (98, 1), (103, 2), (124, 1), (125, 1), (126, 1), (127, 1), (128, 1), (129, 1), (130, 2), (131, 1), (132, 1)], [(10, 5), (19, 3), (22, 2), (49, 1), (53, 1), (61, 2), (104, 1), (133, 1), (134, 1), (135, 1), (136, 1), (137, 1), (138, 1), (139, 1), (140, 1), (141, 1), (142, 1), (143, 1), (144, 1), (145, 1), ...], [(10, 1), (11, 2), (61, 1), (72, 1), (155, 1), (156, 1), (157, 1), (158, 1), (159, 1), (160, 1), (161, 1), (162, 1), (163, 1), (164, 1), (165, 1), (166, 1)], [(10, 4), (11, 1), (19, 1), (22, 2), (29, 2), (53, 1), (57, 1), (61, 1), (167, 1), (168, 1), (169, 1), (170, 1), (171, 1), (172, 1), (173, 1), (174, 1), (175, 1), (176, 1), (177, 1), (178, 1), ...], [(10, 1), (15, 1), (19, 3), (24, 3), (29, 4), (41, 1), (49, 1), (56, 1), (57, 2), (61, 1), (165, 1), (190, 1), (191, 1), (192, 1), (193, 1), (194, 1), (195, 1), (196, 1), (197, 1), (198, 1), ...], [(10, 5), (11, 1), (19, 3), (22, 2), (24, 1), (30, 1), (47, 2), (61, 2), (65, 1), (66, 1), (71, 1), (72, 1), (103, 1), (111, 1), (124, 1), (131, 1), (217, 1), (218, 1), (219, 1), (220, 1), ...], [(10, 4), (11, 2), (19, 4), (22, 1), (24, 2), (29, 2), (36, 1), (45, 2), (47, 1), (53, 3), (56, 2), (61, 2), (69, 2), (112, 1), (127, 1), (241, 1), (242, 1), (243, 1), (244, 1), (245, 2), ...], [(10, 1), (11, 2), (19, 1), (24, 1), (30, 1), (45, 1), (47, 2), (53, 1), (56, 1), (69, 1), (104, 1), (112, 1), (131, 1), (182, 1), (185, 1), (270, 1), (271, 1), (277, 1), (278, 1), (279, 1), ...], [(10, 3), (11, 2), (22, 1), (29, 3), (36, 2), (53, 1), (64, 1), (65, 1), (91, 1), (105, 2), (141, 1), (145, 1), (213, 1), (219, 1), (222, 1), (290, 1), (291, 1), (292, 1), (293, 1), (294, 1), ...], [(10, 1), (29, 2), (37, 1), (53, 1), (61, 1), (64, 1), (237, 1), (305, 1), (306, 1), (307, 1), (308, 1), (309, 1), (310, 1), (311, 1), (312, 1), (313, 1), (314, 1), (315, 1)], [(10, 1), (316, 1), (317, 1), (318, 1)], [(18, 2), (36, 1), (52, 1), (92, 1), (130, 1), (319, 1), (320, 1), (321, 1), (322, 1)], ...]\n",
      "    248         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)\n",
      "    249         if sample_weight is not None:\n",
      "    250             sample_weight = check_array(sample_weight, ensure_2d=False)\n",
      "    251         if issparse(X):\n",
      "\n",
      "...........................................................................\n",
      "/home/bob/.local/lib/python3.6/site-packages/sklearn/utils/validation.py in check_array(array=[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 2), (11, 2), (12, 1), (13, 1)], [(10, 1), (11, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1)], [(1, 1), (10, 5), (14, 1), (18, 1), (19, 1), (22, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 2), (34, 1), (35, 1), (36, 1), (37, 2), (38, 1), ...], [(10, 1), (19, 2), (23, 1), (30, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 2), (56, 1), (57, 2), (58, 1), (59, 1)], [(10, 2), (36, 1), (47, 2), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 2), (66, 1), (67, 2), (68, 1), (69, 2), (70, 1)], [(47, 1), (53, 1), (61, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1)], [(10, 7), (11, 3), (18, 1), (22, 1), (31, 2), (37, 3), (46, 1), (64, 1), (65, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 2), (88, 2), ...], [(10, 1), (19, 1), (22, 1), (24, 1), (29, 1), (30, 1), (47, 1), (51, 1), (53, 1), (56, 1), (61, 1), (102, 1), (113, 1), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 1), ...], [(10, 1), (11, 1), (30, 1), (31, 1), (47, 2), (52, 2), (61, 1), (91, 1), (98, 1), (103, 2), (124, 1), (125, 1), (126, 1), (127, 1), (128, 1), (129, 1), (130, 2), (131, 1), (132, 1)], [(10, 5), (19, 3), (22, 2), (49, 1), (53, 1), (61, 2), (104, 1), (133, 1), (134, 1), (135, 1), (136, 1), (137, 1), (138, 1), (139, 1), (140, 1), (141, 1), (142, 1), (143, 1), (144, 1), (145, 1), ...], [(10, 1), (11, 2), (61, 1), (72, 1), (155, 1), (156, 1), (157, 1), (158, 1), (159, 1), (160, 1), (161, 1), (162, 1), (163, 1), (164, 1), (165, 1), (166, 1)], [(10, 4), (11, 1), (19, 1), (22, 2), (29, 2), (53, 1), (57, 1), (61, 1), (167, 1), (168, 1), (169, 1), (170, 1), (171, 1), (172, 1), (173, 1), (174, 1), (175, 1), (176, 1), (177, 1), (178, 1), ...], [(10, 1), (15, 1), (19, 3), (24, 3), (29, 4), (41, 1), (49, 1), (56, 1), (57, 2), (61, 1), (165, 1), (190, 1), (191, 1), (192, 1), (193, 1), (194, 1), (195, 1), (196, 1), (197, 1), (198, 1), ...], [(10, 5), (11, 1), (19, 3), (22, 2), (24, 1), (30, 1), (47, 2), (61, 2), (65, 1), (66, 1), (71, 1), (72, 1), (103, 1), (111, 1), (124, 1), (131, 1), (217, 1), (218, 1), (219, 1), (220, 1), ...], [(10, 4), (11, 2), (19, 4), (22, 1), (24, 2), (29, 2), (36, 1), (45, 2), (47, 1), (53, 3), (56, 2), (61, 2), (69, 2), (112, 1), (127, 1), (241, 1), (242, 1), (243, 1), (244, 1), (245, 2), ...], [(10, 1), (11, 2), (19, 1), (24, 1), (30, 1), (45, 1), (47, 2), (53, 1), (56, 1), (69, 1), (104, 1), (112, 1), (131, 1), (182, 1), (185, 1), (270, 1), (271, 1), (277, 1), (278, 1), (279, 1), ...], [(10, 3), (11, 2), (22, 1), (29, 3), (36, 2), (53, 1), (64, 1), (65, 1), (91, 1), (105, 2), (141, 1), (145, 1), (213, 1), (219, 1), (222, 1), (290, 1), (291, 1), (292, 1), (293, 1), (294, 1), ...], [(10, 1), (29, 2), (37, 1), (53, 1), (61, 1), (64, 1), (237, 1), (305, 1), (306, 1), (307, 1), (308, 1), (309, 1), (310, 1), (311, 1), (312, 1), (313, 1), (314, 1), (315, 1)], [(10, 1), (316, 1), (317, 1), (318, 1)], [(18, 2), (36, 1), (52, 1), (92, 1), (130, 1), (319, 1), (320, 1), (321, 1), (322, 1)], ...], accept_sparse='csc', dtype=<class 'numpy.float32'>, order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=False, estimator=None)\n",
      "    428 \n",
      "    429     if sp.issparse(array):\n",
      "    430         array = _ensure_sparse_format(array, accept_sparse, dtype, copy,\n",
      "    431                                       force_all_finite)\n",
      "    432     else:\n",
      "--> 433         array = np.array(array, dtype=dtype, order=order, copy=copy)\n",
      "        array = [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 2), (11, 2), (12, 1), (13, 1)], [(10, 1), (11, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1)], [(1, 1), (10, 5), (14, 1), (18, 1), (19, 1), (22, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 2), (34, 1), (35, 1), (36, 1), (37, 2), (38, 1), ...], [(10, 1), (19, 2), (23, 1), (30, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 2), (56, 1), (57, 2), (58, 1), (59, 1)], [(10, 2), (36, 1), (47, 2), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 2), (66, 1), (67, 2), (68, 1), (69, 2), (70, 1)], [(47, 1), (53, 1), (61, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1)], [(10, 7), (11, 3), (18, 1), (22, 1), (31, 2), (37, 3), (46, 1), (64, 1), (65, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 2), (88, 2), ...], [(10, 1), (19, 1), (22, 1), (24, 1), (29, 1), (30, 1), (47, 1), (51, 1), (53, 1), (56, 1), (61, 1), (102, 1), (113, 1), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 1), ...], [(10, 1), (11, 1), (30, 1), (31, 1), (47, 2), (52, 2), (61, 1), (91, 1), (98, 1), (103, 2), (124, 1), (125, 1), (126, 1), (127, 1), (128, 1), (129, 1), (130, 2), (131, 1), (132, 1)], [(10, 5), (19, 3), (22, 2), (49, 1), (53, 1), (61, 2), (104, 1), (133, 1), (134, 1), (135, 1), (136, 1), (137, 1), (138, 1), (139, 1), (140, 1), (141, 1), (142, 1), (143, 1), (144, 1), (145, 1), ...], [(10, 1), (11, 2), (61, 1), (72, 1), (155, 1), (156, 1), (157, 1), (158, 1), (159, 1), (160, 1), (161, 1), (162, 1), (163, 1), (164, 1), (165, 1), (166, 1)], [(10, 4), (11, 1), (19, 1), (22, 2), (29, 2), (53, 1), (57, 1), (61, 1), (167, 1), (168, 1), (169, 1), (170, 1), (171, 1), (172, 1), (173, 1), (174, 1), (175, 1), (176, 1), (177, 1), (178, 1), ...], [(10, 1), (15, 1), (19, 3), (24, 3), (29, 4), (41, 1), (49, 1), (56, 1), (57, 2), (61, 1), (165, 1), (190, 1), (191, 1), (192, 1), (193, 1), (194, 1), (195, 1), (196, 1), (197, 1), (198, 1), ...], [(10, 5), (11, 1), (19, 3), (22, 2), (24, 1), (30, 1), (47, 2), (61, 2), (65, 1), (66, 1), (71, 1), (72, 1), (103, 1), (111, 1), (124, 1), (131, 1), (217, 1), (218, 1), (219, 1), (220, 1), ...], [(10, 4), (11, 2), (19, 4), (22, 1), (24, 2), (29, 2), (36, 1), (45, 2), (47, 1), (53, 3), (56, 2), (61, 2), (69, 2), (112, 1), (127, 1), (241, 1), (242, 1), (243, 1), (244, 1), (245, 2), ...], [(10, 1), (11, 2), (19, 1), (24, 1), (30, 1), (45, 1), (47, 2), (53, 1), (56, 1), (69, 1), (104, 1), (112, 1), (131, 1), (182, 1), (185, 1), (270, 1), (271, 1), (277, 1), (278, 1), (279, 1), ...], [(10, 3), (11, 2), (22, 1), (29, 3), (36, 2), (53, 1), (64, 1), (65, 1), (91, 1), (105, 2), (141, 1), (145, 1), (213, 1), (219, 1), (222, 1), (290, 1), (291, 1), (292, 1), (293, 1), (294, 1), ...], [(10, 1), (29, 2), (37, 1), (53, 1), (61, 1), (64, 1), (237, 1), (305, 1), (306, 1), (307, 1), (308, 1), (309, 1), (310, 1), (311, 1), (312, 1), (313, 1), (314, 1), (315, 1)], [(10, 1), (316, 1), (317, 1), (318, 1)], [(18, 2), (36, 1), (52, 1), (92, 1), (130, 1), (319, 1), (320, 1), (321, 1), (322, 1)], ...]\n",
      "        dtype = <class 'numpy.float32'>\n",
      "        order = None\n",
      "        copy = False\n",
      "    434 \n",
      "    435         if ensure_2d:\n",
      "    436             if array.ndim == 1:\n",
      "    437                 raise ValueError(\n",
      "\n",
      "ValueError: setting an array element with a sequence.\n",
      "___________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from gensim import sklearn_api\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "pipeline = Pipeline([\n",
    "    ('features', sklearn_api.text2bow.Text2BowTransformer()),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "try:\n",
    "    print(cross_val_score(pipeline, train.text, train.author, cv=3, n_jobs=3))\n",
    "    print(cross_val_score(pipeline, train.text, train.author, \n",
    "                    cv=3, n_jobs=3, scoring='neg_log_loss'))\n",
    "except ValueError as e:\n",
    "#     print(e)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ValueError: setting an array element with a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.sklearn_api.text2bow.Text2BowTransformer'>\n",
      "<class 'list'>\n",
      "This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall. [(0, 2), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 3), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 4), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1)] \n",
      " Dropping of its own accord upon his exit or perhaps purposely closed, it had become fastened by the spring; and it was the retention of this spring which had been mistaken by the police for that of the nail, farther inquiry being thus considered unnecessary. [(3, 1), (7, 1), (13, 1), (20, 3), (29, 4), (44, 1), (60, 1), (70, 1), (71, 1), (78, 2), (95, 2), (133, 1), (143, 2), (174, 1), (178, 1), (201, 2), (220, 1), (321, 1), (323, 1), (352, 1), (403, 1), (542, 1), (1485, 1), (1779, 1), (1798, 1), (1873, 1), (2219, 1), (2889, 1), (4672, 1), (4673, 1), (4674, 1), (4675, 1), (4676, 1), (4677, 1), (4678, 1), (4679, 1)]\n"
     ]
    }
   ],
   "source": [
    "something = sklearn_api.text2bow.Text2BowTransformer()\n",
    "print(type(something))\n",
    "other = something.fit_transform(train.text)\n",
    "print(type(other))\n",
    "print(train.text[0], other[0], '\\n', train.text[656], other[656])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is this supposed to work?!?\n",
    "\n",
    "`tfidf.fit_transform(corpus)` returns matrix, while the `sklearn_api.text2bow.Text2BowTransformer().fit_transform(corpus)` returns some \"BOW\" format\n",
    "\n",
    "OK - this might work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', sklearn_api.text2bow.Text2BowTransformer()),\n",
    "    ('lda', sklearn_api.ldamodel.LdaTransformer()),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# print(cross_val_score(pipeline, train.text, train.author, cv=3, n_jobs=3))\n",
    "# print(cross_val_score(pipeline, train.text, train.author, \n",
    "#                 cv=3, n_jobs=3, scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ 0.48360907  0.48850751  0.48613027]<br>\n",
    "[-2.1033608  -2.09791398 -2.30065565]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преди да пообработим текста да видим Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', sklearn_api.text2bow.Text2BowTransformer()),\n",
    "    ('lda', sklearn_api.ldamodel.LdaTransformer()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bob/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n",
      "/home/bob/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n",
      "/home/bob/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.53722426  0.53463071  0.53563218]\n",
      "CPU times: user 430 ms, sys: 169 ms, total: 599 ms\n",
      "Wall time: 3min 41s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# print(cross_val_score(pipeline, train.text, train.author, cv=3, n_jobs=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ 0.53722426  0.53463071  0.53563218]<br>\n",
    "CPU times: user 430 ms, sys: 169 ms, total: 599 ms<br>\n",
    "Wall time: 3min 41s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bob/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n",
      "/home/bob/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n",
      "/home/bob/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.96678405 -0.97080789 -0.97363342]\n",
      "CPU times: user 453 ms, sys: 121 ms, total: 573 ms\n",
      "Wall time: 3min 39s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# print(cross_val_score(pipeline, train.text, train.author, \n",
    "#                 cv=3, n_jobs=3, scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[-0.96678405 -0.97080789 -0.97363342]<br>\n",
    "CPU times: user 453 ms, sys: 121 ms, total: 573 ms<br>\n",
    "Wall time: 3min 39s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Още един опит преди да окастрим текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', sklearn_api.text2bow.Text2BowTransformer()),\n",
    "    ('lda', sklearn_api.ldamodel.LdaTransformer(num_topics=21)),\n",
    "    ('clf', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.98861154 -0.97565475 -0.99821168]\n",
      "CPU times: user 381 ms, sys: 99.9 ms, total: 481 ms\n",
      "Wall time: 2min 8s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# print(cross_val_score(pipeline, train.text, train.author, \n",
    "#                 cv=3, n_jobs=3, scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[-0.98861154 -0.97565475 -0.99821168]<br>\n",
    "CPU times: user 381 ms, sys: 99.9 ms, total: 481 ms<br>\n",
    "Wall time: 2min 8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "id26305    thi process, however, afford me no mean of asc...\n",
       "id17569    It never onc occur to me that the fumbl might ...\n",
       "id11008    In hi left hand wa a gold snuff box, from whic...\n",
       "id27763    how love is spring As we look from windsor ter...\n",
       "id12958    find noth else, not even gold, the superintend...\n",
       "                                 ...                        \n",
       "id17718    I could have fancied, while I look at it, that...\n",
       "id08973     the lid clench themselv togeth as if in a spasm.\n",
       "id05267    mai il faut agir that is to say, a frenchman n...\n",
       "id17513    for an item of news like this, it strike us it...\n",
       "id00393    He laid a gnarl claw on my shoulder, and it se...\n",
       "Name: stemmed, Length: 19579, dtype: object"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explore.stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "id26305    thi process  however  afford mean ascertain di...\n",
       "id17569                        onc occur fumbl mere mistake \n",
       "id11008    hi left hand wa gold snuff box  which  caper h...\n",
       "id27763    love spring look windsor terrac sixteen fertil...\n",
       "id12958    noth else  gold  superintend abandon hi attemp...\n",
       "                                 ...                        \n",
       "id17718    fancied  look it  emin landscap painter built ...\n",
       "id08973                    lid clench themselv togeth spasm \n",
       "id05267      mai il faut agir say  frenchman faint outright \n",
       "id17513    item news like this  strike wa veri coolli rec...\n",
       "id00393    laid gnarl claw shoulder  shake wa altogeth mi...\n",
       "Name: stemmed2, Length: 19579, dtype: object"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import preprocess_string, strip_punctuation, remove_stopwords\n",
    "\n",
    "# CUSTOM_FILTERS = [strip_punctuation, remove_stopwords]\n",
    "# explore['stemmed2'] = [ preprocess_string(s, CUSTOM_FILTERS) for s in explore.stemmed]\n",
    "# explore.stemmed2\n",
    "\n",
    "explore['stemmed2'] = [ strip_punctuation(remove_stopwords(s.lower())) for s in explore.stemmed]\n",
    "explore.stemmed2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It never onc occur to me that the fumbl might be a mere mistake. ||| onc occur fumbl mere mistake \n"
     ]
    }
   ],
   "source": [
    "print(explore.loc['id17569'].stemmed, '|||', explore.loc['id17569'].stemmed2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline-a си е същия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.05550683 -1.02313271 -1.02899542]\n",
      "CPU times: user 347 ms, sys: 92.8 ms, total: 440 ms\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# print(cross_val_score(pipeline, explore.stemmed2, train.author, \n",
    "#                 cv=3, n_jobs=3, scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[-1.05550683 -1.02313271 -1.02899542]<br>\n",
    "CPU times: user 347 ms, sys: 92.8 ms, total: 440 ms<br>\n",
    "Wall time: 1min 27s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поне е по-бързо. Да видим tfidf и повече фичъри."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow', sklearn_api.text2bow.Text2BowTransformer()),\n",
    "    ('features', sklearn_api.tfidf.TfIdfTransformer()),\n",
    "    ('lda', sklearn_api.ldamodel.LdaTransformer(num_topics=501)),\n",
    "    ('clf', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bob/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n",
      "/home/bob/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n",
      "/home/bob/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.94941507 -0.95248997 -0.95026712]\n",
      "CPU times: user 1.18 s, sys: 223 ms, total: 1.4 s\n",
      "Wall time: 19min 4s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# print(cross_val_score(pipeline, train.text, train.author, \n",
    "#                 cv=3, n_jobs=3, scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[-0.94941507 -0.95248997 -0.95026712]<br>\n",
    "CPU times: user 1.18 s, sys: 223 ms, total: 1.4 s<br>\n",
    "Wall time: 19min 4s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is frustrating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow', sklearn_api.text2bow.Text2BowTransformer()),\n",
    "    ('features', sklearn_api.tfidf.TfIdfTransformer()),\n",
    "    ('lda', sklearn_api.ldamodel.LdaTransformer()),\n",
    "    ('clf', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bob/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n",
      "/home/bob/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n",
      "/home/bob/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.03061917 -1.03984701 -1.03884094]\n",
      "CPU times: user 387 ms, sys: 124 ms, total: 511 ms\n",
      "Wall time: 2min 33s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# print(cross_val_score(pipeline, train.text, train.author, \n",
    "#                 cv=3, n_jobs=3, scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[-1.03061917 -1.03984701 -1.03884094]<br>\n",
    "CPU times: user 387 ms, sys: 124 ms, total: 511 ms<br>\n",
    "Wall time: 2min 33s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow', sklearn_api.text2bow.Text2BowTransformer()),\n",
    "    ('features', sklearn_api.tfidf.TfIdfTransformer()),\n",
    "    ('lda', sklearn_api.ldamodel.LdaTransformer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bob/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n",
      "/home/bob/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n",
      "/home/bob/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.06731359 -1.07019065 -1.07019197]\n",
      "CPU times: user 377 ms, sys: 142 ms, total: 519 ms\n",
      "Wall time: 2min 29s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# print(cross_val_score(pipeline, train.text, train.author, \n",
    "#                 cv=3, n_jobs=3, scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[-1.06731359 -1.07019065 -1.07019197]<br>\n",
    "CPU times: user 377 ms, sys: 142 ms, total: 519 ms<br>\n",
    "Wall time: 2min 29s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "От къде е тая разлика? Може ли text2bow да чупи? Или е от липсата на хипер парамерти?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(56, 1.0),\n",
       " (155, 1.0),\n",
       " (299, 1.0),\n",
       " (421, 1.0),\n",
       " (1309, 1.0),\n",
       " (1603, 1.0),\n",
       " (1621, 1.0),\n",
       " (1686, 1.0),\n",
       " (1988, 1.0),\n",
       " (2038, 1.0),\n",
       " (2728, 1.0),\n",
       " (4607, 1.0),\n",
       " (4608, 1.0),\n",
       " (4609, 1.0),\n",
       " (4610, 1.0),\n",
       " (4611, 1.0)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[656]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(19579 documents, 22328 features, 240166 non-zero entries)\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corpus е минат с word2vec; ще прескоча tfidf за момента и ще пусна LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[(0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6), (7, 7), (8, 8), (9, 9)]\n",
      "[(0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6), (7, 7), (8, 8), (9, 9)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "<class 'gensim.corpora.dictionary.Dictionary'>\n"
     ]
    }
   ],
   "source": [
    "id2word=dict([(i, s) for i, s in enumerate(dictionary)])\n",
    "print([ id2word[z] for z in range(10) ])\n",
    "print(list(id2word.items())[:10])\n",
    "what = list(enumerate(dictionary))\n",
    "print(what[:10])\n",
    "print(list(dictionary)[:10])\n",
    "print(type(dictionary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bob/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "# lda = LdaModel(corpus=corpus, id2word=dict([(i, s) for i, s in enumerate(dictionary)]))\n",
    "lda = LdaModel(corpus=corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['afforded', 'ascertaining', 'aware', 'dimensions', 'however,', 'make', 'means', 'might', 'out,', 'perfectly']\n"
     ]
    }
   ],
   "source": [
    "print([dictionary[z] for z in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.071*\"886\" + 0.065*\"3256\" + 0.059*\"1937\" + 0.034*\"3007\" + 0.025*\"270\" + 0.024*\"439\" + 0.019*\"1858\" + 0.018*\"1318\" + 0.017*\"269\" + 0.017*\"9126\"'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(44,\n",
       "  '0.110*\"920\" + 0.038*\"4810\" + 0.035*\"1451\" + 0.025*\"56\" + 0.023*\"2802\" + 0.021*\"774\" + 0.021*\"916\" + 0.017*\"9134\" + 0.017*\"1225\" + 0.016*\"5671\"'),\n",
       " (42,\n",
       "  '0.053*\"79\" + 0.050*\"898\" + 0.048*\"496\" + 0.035*\"716\" + 0.034*\"655\" + 0.030*\"258\" + 0.022*\"3557\" + 0.018*\"122\" + 0.017*\"158\" + 0.015*\"66\"'),\n",
       " (2,\n",
       "  '0.066*\"1071\" + 0.058*\"173\" + 0.047*\"1857\" + 0.040*\"986\" + 0.039*\"2374\" + 0.026*\"108\" + 0.025*\"4934\" + 0.022*\"1716\" + 0.017*\"2976\" + 0.017*\"1714\"'),\n",
       " (55,\n",
       "  '0.092*\"377\" + 0.084*\"194\" + 0.071*\"2414\" + 0.019*\"2417\" + 0.018*\"7153\" + 0.018*\"3047\" + 0.017*\"1944\" + 0.017*\"2451\" + 0.016*\"2669\" + 0.015*\"3657\"'),\n",
       " (84,\n",
       "  '0.081*\"1125\" + 0.037*\"144\" + 0.037*\"1956\" + 0.033*\"1957\" + 0.033*\"1955\" + 0.031*\"3278\" + 0.031*\"5937\" + 0.030*\"3938\" + 0.027*\"3534\" + 0.024*\"2604\"'),\n",
       " (6,\n",
       "  '0.122*\"608\" + 0.079*\"70\" + 0.073*\"662\" + 0.042*\"970\" + 0.036*\"442\" + 0.032*\"2710\" + 0.028*\"3227\" + 0.024*\"136\" + 0.022*\"132\" + 0.020*\"6632\"'),\n",
       " (91,\n",
       "  '0.057*\"3860\" + 0.036*\"992\" + 0.035*\"1086\" + 0.035*\"4358\" + 0.026*\"2506\" + 0.024*\"5236\" + 0.023*\"2062\" + 0.022*\"7006\" + 0.021*\"3095\" + 0.020*\"265\"'),\n",
       " (94,\n",
       "  '0.126*\"1006\" + 0.062*\"5033\" + 0.052*\"2383\" + 0.050*\"1325\" + 0.048*\"531\" + 0.045*\"646\" + 0.043*\"273\" + 0.041*\"762\" + 0.034*\"2420\" + 0.033*\"991\"'),\n",
       " (17,\n",
       "  '0.093*\"1136\" + 0.066*\"1401\" + 0.048*\"729\" + 0.040*\"1100\" + 0.036*\"191\" + 0.036*\"930\" + 0.034*\"182\" + 0.030*\"945\" + 0.028*\"728\" + 0.028*\"2568\"'),\n",
       " (12,\n",
       "  '0.046*\"121\" + 0.039*\"1084\" + 0.037*\"8816\" + 0.033*\"1082\" + 0.033*\"611\" + 0.033*\"3140\" + 0.025*\"514\" + 0.025*\"565\" + 0.024*\"1077\" + 0.023*\"6291\"')]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.show_topics(num_topics=10, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait, if the output is like x*\"\\d+?\" does this mean that the training took numbers instead of words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-9cd9ec4e9499>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLdaModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    745\u001b[0m                         \u001b[0mpass_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_no\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlencorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m                     )\n\u001b[0;32m--> 747\u001b[0;31m                     \u001b[0mgammat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_estep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_alpha\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mdo_estep\u001b[0;34m(self, chunk, state)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollect_sstats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msstats\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumdocs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# avoids calling len(chunk) on a generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteger_types\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteger\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m                 \u001b[0;31m# make sure the term IDs are ints, otherwise np will get upset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m                 \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m                 \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteger_types\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteger\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m                 \u001b[0;31m# make sure the term IDs are ints, otherwise np will get upset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m                 \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m                 \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    lda = LdaModel(texts, id2word=dict([(i, s) for i, s in enumerate(dictionary)]))\n",
    "    lda.show_topics(num_topics=10, num_words=10)\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['process,',\n",
       " 'however,',\n",
       " 'afforded',\n",
       " 'means',\n",
       " 'ascertaining',\n",
       " 'dimensions',\n",
       " 'might',\n",
       " 'make',\n",
       " 'return',\n",
       " 'point',\n",
       " 'whence',\n",
       " 'set',\n",
       " 'out,',\n",
       " 'without',\n",
       " 'aware',\n",
       " 'perfectly',\n",
       " 'uniform',\n",
       " 'seemed',\n",
       " 'wall.']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "show_topics() got an unexpected keyword argument 'topics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-b27671ccfa85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mti\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'topic %d: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mti\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s/%.2f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: show_topics() got an unexpected keyword argument 'topics'"
     ]
    }
   ],
   "source": [
    "topics = lda.show_topics(topics=-1, topn=10, formatted=False)\n",
    "for ti, topic in enumerate(topics):\n",
    "    print('topic %d: %s' % (ti, ' '.join('%s/%.2f' % (t[1], t[0]) for t in topic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "JoblibValueError",
     "evalue": "JoblibValueError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/usr/lib/python3.6/runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/usr/lib/python3.6/runpy.py in _run_code(code=<code object <module> at 0x7f608565f1e0, file \"/...3.6/site-packages/ipykernel_launcher.py\", line 5>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/home/bob/.local/lib/python3.6/site-packages/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/home/bob/.local/lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/home/bob/.l.../python3.6/site-packages/ipykernel/kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x7f608565f1e0, file \"/...3.6/site-packages/ipykernel_launcher.py\", line 5>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/home/bob/.local/lib/python3.6/site-packages/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/home/bob/.local/lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/home/bob/.l.../python3.6/site-packages/ipykernel/kernelapp.py'>, ...}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    473         if self.poller is not None:\n    474             self.poller.start()\n    475         self.kernel.start()\n    476         self.io_loop = ioloop.IOLoop.current()\n    477         try:\n--> 478             self.io_loop.start()\n        self.io_loop.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    479         except KeyboardInterrupt:\n    480             pass\n    481 \n    482 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    172             )\n    173         return loop\n    174     \n    175     def start(self):\n    176         try:\n--> 177             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    178         except ZMQError as e:\n    179             if e.errno == ETERM:\n    180                 # quietly return on ETERM\n    181                 pass\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    883                 self._events.update(event_pairs)\n    884                 while self._events:\n    885                     fd, events = self._events.popitem()\n    886                     try:\n    887                         fd_obj, handler_func = self._handlers[fd]\n--> 888                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    889                     except (OSError, IOError) as e:\n    890                         if errno_from_exception(e) == errno.EPIPE:\n    891                             # Happens when the client closes the connection\n    892                             pass\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    276         if self.control_stream:\n    277             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    278 \n    279         def make_dispatcher(stream):\n    280             def dispatcher(msg):\n--> 281                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    282             return dispatcher\n    283 \n    284         for s in self.shell_streams:\n    285             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': \"%%time\\nprint(cross_val_score(pipeline, train.tex...         cv=3, n_jobs=3, scoring='neg_log_loss'))\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 1, 3, 14, 40, 37, 109981, tzinfo=tzutc()), 'msg_id': '8D23B63CFDE24DCC8A42E9FBCF10A1A6', 'msg_type': 'execute_request', 'session': '512E4502520F4C58B500E845BCAB949B', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '8D23B63CFDE24DCC8A42E9FBCF10A1A6', 'msg_type': 'execute_request', 'parent_header': {}})\n    227             self.log.warn(\"Unknown message type: %r\", msg_type)\n    228         else:\n    229             self.log.debug(\"%s: %s\", msg_type, msg)\n    230             self.pre_handler_hook()\n    231             try:\n--> 232                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'512E4502520F4C58B500E845BCAB949B']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': \"%%time\\nprint(cross_val_score(pipeline, train.tex...         cv=3, n_jobs=3, scoring='neg_log_loss'))\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 1, 3, 14, 40, 37, 109981, tzinfo=tzutc()), 'msg_id': '8D23B63CFDE24DCC8A42E9FBCF10A1A6', 'msg_type': 'execute_request', 'session': '512E4502520F4C58B500E845BCAB949B', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '8D23B63CFDE24DCC8A42E9FBCF10A1A6', 'msg_type': 'execute_request', 'parent_header': {}}\n    233             except Exception:\n    234                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    235             finally:\n    236                 self.post_handler_hook()\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'512E4502520F4C58B500E845BCAB949B'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': \"%%time\\nprint(cross_val_score(pipeline, train.tex...         cv=3, n_jobs=3, scoring='neg_log_loss'))\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 1, 3, 14, 40, 37, 109981, tzinfo=tzutc()), 'msg_id': '8D23B63CFDE24DCC8A42E9FBCF10A1A6', 'msg_type': 'execute_request', 'session': '512E4502520F4C58B500E845BCAB949B', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '8D23B63CFDE24DCC8A42E9FBCF10A1A6', 'msg_type': 'execute_request', 'parent_header': {}})\n    392         if not silent:\n    393             self.execution_count += 1\n    394             self._publish_execute_input(code, parent, self.execution_count)\n    395 \n    396         reply_content = self.do_execute(code, silent, store_history,\n--> 397                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    398 \n    399         # Flush output before sending the reply.\n    400         sys.stdout.flush()\n    401         sys.stderr.flush()\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code=\"%%time\\nprint(cross_val_score(pipeline, train.tex...         cv=3, n_jobs=3, scoring='neg_log_loss'))\", silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    203 \n    204         self._forward_input(allow_stdin)\n    205 \n    206         reply_content = {}\n    207         try:\n--> 208             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = \"%%time\\nprint(cross_val_score(pipeline, train.tex...         cv=3, n_jobs=3, scoring='neg_log_loss'))\"\n        store_history = True\n        silent = False\n    209         finally:\n    210             self._restore_input()\n    211 \n    212         if res.error_before_exec is not None:\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=(\"%%time\\nprint(cross_val_score(pipeline, train.tex...         cv=3, n_jobs=3, scoring='neg_log_loss'))\",), **kwargs={'silent': False, 'store_history': True})\n    528             )\n    529         self.payload_manager.write_payload(payload)\n    530 \n    531     def run_cell(self, *args, **kwargs):\n    532         self._last_traceback = None\n--> 533         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = (\"%%time\\nprint(cross_val_score(pipeline, train.tex...         cv=3, n_jobs=3, scoring='neg_log_loss'))\",)\n        kwargs = {'silent': False, 'store_history': True}\n    534 \n    535     def _showtraceback(self, etype, evalue, stb):\n    536         # try to preserve ordering of tracebacks and print statements\n    537         sys.stdout.flush()\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=\"%%time\\nprint(cross_val_score(pipeline, train.tex...         cv=3, n_jobs=3, scoring='neg_log_loss'))\", store_history=True, silent=False, shell_futures=True)\n   2723                 self.displayhook.exec_result = result\n   2724 \n   2725                 # Execute the user code\n   2726                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2727                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2728                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2729                 \n   2730                 self.last_execution_succeeded = not has_raised\n   2731                 self.last_execution_result = result\n   2732 \n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Expr object>], cell_name='<ipython-input-118-3fa43c63eb93>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 7f602820e198, executi..._before_exec=None error_in_exec=None result=None>)\n   2851                     return True\n   2852 \n   2853             for i, node in enumerate(to_run_interactive):\n   2854                 mod = ast.Interactive([node])\n   2855                 code = compiler(mod, cell_name, \"single\")\n-> 2856                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7f6026d6aed0, file \"<ipython-input-118-3fa43c63eb93>\", line 1>\n        result = <ExecutionResult object at 7f602820e198, executi..._before_exec=None error_in_exec=None result=None>\n   2857                     return True\n   2858 \n   2859             # Flush softspace\n   2860             if softspace(sys.stdout, 0):\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7f6026d6aed0, file \"<ipython-input-118-3fa43c63eb93>\", line 1>, result=<ExecutionResult object at 7f602820e198, executi..._before_exec=None error_in_exec=None result=None>)\n   2905         outflag = True  # happens in more places, so it's easier as default\n   2906         try:\n   2907             try:\n   2908                 self.hooks.pre_run_code_hook()\n   2909                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2910                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7f6026d6aed0, file \"<ipython-input-118-3fa43c63eb93>\", line 1>\n        self.user_global_ns = {'Adam': <class 'keras.optimizers.Adam'>, 'CUSTOM_FILTERS': [<function strip_punctuation>, <function remove_stopwords>], 'Concatenate': <class 'keras.layers.merge.Concatenate'>, 'Conv2D': <class 'keras.layers.convolutional.Conv2D'>, 'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'Dense': <class 'keras.layers.core.Dense'>, 'Dropout': <class 'keras.layers.core.Dropout'>, 'FILENAME': 'processed_texts.pickle', 'HashingVectorizer': <class 'sklearn.feature_extraction.text.HashingVectorizer'>, 'In': ['', \"# corpus = [dictionary.doc2bow(text) for text in...k\\ncorpus = corpora.mmcorpus.MmCorpus('spooky.mm')\", \"import numpy as np\\nimport matplotlib.pyplot as p...lib', 'inline')\\n\\npd.options.display.max_rows = 10\", '# Dataset\\n\\n# use this if in fmi-hw... repo\\n# tra...pe)\\nprint(set(train.columns) - set(test.columns))', 'train.head(5)', 'from sklearn.pipeline import Pipeline\\nfrom sklea...om sklearn.model_selection import cross_val_score', \"pipeline = Pipeline([\\n    ('features', CountVect...peline, train.text, train.author, cv=3, n_jobs=3)\", 'from sklearn.model_selection import cross_val_pr...peline, train.text, train.author, cv=3, n_jobs=3)', \"import itertools\\nfrom sklearn.metrics import con...l('True label')\\n    plt.xlabel('Predicted label')\", 'pipeline.fit(train.text, train.author)\\nauthors =...matrix(train.author, prediction, classes=authors)', 'plot_confusion_matrix(train.author, prediction, classes=authors, normalize=True)', \"from sklearn.ensemble import RandomForestClassif...         cv=3, n_jobs=3, scoring='neg_log_loss'))\", \"from sklearn.linear_model import LogisticRegress...log_loss'))\\n\\n# Получихме малко по-добри резултати\", 'explore = train.copy()', \"# # бр. думи в текста\\n# explore['words'] = explo... sum([c in string.punctuation for c in str(s)]) )\", \"import nltk\\n# nltk.download('stopwords')\\nstopwords = nltk.corpus.stopwords.words('english')\", \"import nltk\\nnltk.download('stopwords')\\nstopwords = nltk.corpus.stopwords.words('english')\", \"import nltk\\n# nltk.download('stopwords')\\nstopwords = nltk.corpus.stopwords.words('english')\", \"# features_names = list(set(explore.columns) - {...or'})\\n# print('numeric features', features_names)\", \"from nltk.stem import WordNetLemmatizer\\nfrom nlt....split()])) \\n\\nexplore[['stemmed', 'text']].head()\", ...], ...}\n        self.user_ns = {'Adam': <class 'keras.optimizers.Adam'>, 'CUSTOM_FILTERS': [<function strip_punctuation>, <function remove_stopwords>], 'Concatenate': <class 'keras.layers.merge.Concatenate'>, 'Conv2D': <class 'keras.layers.convolutional.Conv2D'>, 'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'Dense': <class 'keras.layers.core.Dense'>, 'Dropout': <class 'keras.layers.core.Dropout'>, 'FILENAME': 'processed_texts.pickle', 'HashingVectorizer': <class 'sklearn.feature_extraction.text.HashingVectorizer'>, 'In': ['', \"# corpus = [dictionary.doc2bow(text) for text in...k\\ncorpus = corpora.mmcorpus.MmCorpus('spooky.mm')\", \"import numpy as np\\nimport matplotlib.pyplot as p...lib', 'inline')\\n\\npd.options.display.max_rows = 10\", '# Dataset\\n\\n# use this if in fmi-hw... repo\\n# tra...pe)\\nprint(set(train.columns) - set(test.columns))', 'train.head(5)', 'from sklearn.pipeline import Pipeline\\nfrom sklea...om sklearn.model_selection import cross_val_score', \"pipeline = Pipeline([\\n    ('features', CountVect...peline, train.text, train.author, cv=3, n_jobs=3)\", 'from sklearn.model_selection import cross_val_pr...peline, train.text, train.author, cv=3, n_jobs=3)', \"import itertools\\nfrom sklearn.metrics import con...l('True label')\\n    plt.xlabel('Predicted label')\", 'pipeline.fit(train.text, train.author)\\nauthors =...matrix(train.author, prediction, classes=authors)', 'plot_confusion_matrix(train.author, prediction, classes=authors, normalize=True)', \"from sklearn.ensemble import RandomForestClassif...         cv=3, n_jobs=3, scoring='neg_log_loss'))\", \"from sklearn.linear_model import LogisticRegress...log_loss'))\\n\\n# Получихме малко по-добри резултати\", 'explore = train.copy()', \"# # бр. думи в текста\\n# explore['words'] = explo... sum([c in string.punctuation for c in str(s)]) )\", \"import nltk\\n# nltk.download('stopwords')\\nstopwords = nltk.corpus.stopwords.words('english')\", \"import nltk\\nnltk.download('stopwords')\\nstopwords = nltk.corpus.stopwords.words('english')\", \"import nltk\\n# nltk.download('stopwords')\\nstopwords = nltk.corpus.stopwords.words('english')\", \"# features_names = list(set(explore.columns) - {...or'})\\n# print('numeric features', features_names)\", \"from nltk.stem import WordNetLemmatizer\\nfrom nlt....split()])) \\n\\nexplore[['stemmed', 'text']].head()\", ...], ...}\n   2911             finally:\n   2912                 # Reset our crash handler in place\n   2913                 sys.excepthook = old_excepthook\n   2914         except SystemExit as e:\n\n...........................................................................\n/home/bob/gits/spooky/<ipython-input-118-3fa43c63eb93> in <module>()\n----> 1 get_ipython().run_cell_magic('time', '', \"print(cross_val_score(pipeline, train.text, train.author, \\n                cv=3, n_jobs=3, scoring='neg_log_loss'))\")\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_cell_magic(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, magic_name='time', line='', cell=\"print(cross_val_score(pipeline, train.text, trai...         cv=3, n_jobs=3, scoring='neg_log_loss'))\")\n   2126             # This will need to be updated if the internal calling logic gets\n   2127             # refactored, or else we'll be expanding the wrong variables.\n   2128             stack_depth = 2\n   2129             magic_arg_s = self.var_expand(line, stack_depth)\n   2130             with self.builtin_trap:\n-> 2131                 result = fn(magic_arg_s, cell)\n        result = undefined\n        fn = <bound method ExecutionMagics.time of <IPython.core.magics.execution.ExecutionMagics object>>\n        magic_arg_s = ''\n        cell = \"print(cross_val_score(pipeline, train.text, trai...         cv=3, n_jobs=3, scoring='neg_log_loss'))\"\n   2132             return result\n   2133 \n   2134     def find_line_magic(self, magic_name):\n   2135         \"\"\"Find and return a line magic by name.\n\n...........................................................................\n/home/bob/gits/spooky/<decorator-gen-62> in time(self=<IPython.core.magics.execution.ExecutionMagics object>, line='', cell=\"print(cross_val_score(pipeline, train.text, trai...         cv=3, n_jobs=3, scoring='neg_log_loss'))\", local_ns=None)\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/IPython/core/magic.py in <lambda>(f=<function ExecutionMagics.time>, *a=(<IPython.core.magics.execution.ExecutionMagics object>, '', \"print(cross_val_score(pipeline, train.text, trai...         cv=3, n_jobs=3, scoring='neg_log_loss'))\", None), **k={})\n    182     validate_type(magic_kind)\n    183 \n    184     # This is a closure to capture the magic_kind.  We could also use a class,\n    185     # but it's overkill for just that one bit of state.\n    186     def magic_deco(arg):\n--> 187         call = lambda f, *a, **k: f(*a, **k)\n        f = <function ExecutionMagics.time>\n        a = (<IPython.core.magics.execution.ExecutionMagics object>, '', \"print(cross_val_score(pipeline, train.text, trai...         cv=3, n_jobs=3, scoring='neg_log_loss'))\", None)\n        k = {}\n    188 \n    189         if callable(arg):\n    190             # \"Naked\" decorator call (just @foo, no args)\n    191             func = arg\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/IPython/core/magics/execution.py in time(self=<IPython.core.magics.execution.ExecutionMagics object>, line='', cell=\"print(cross_val_score(pipeline, train.text, trai...         cv=3, n_jobs=3, scoring='neg_log_loss'))\", local_ns=None)\n   1225         # time execution\n   1226         wall_st = wtime()\n   1227         if mode=='eval':\n   1228             st = clock2()\n   1229             try:\n-> 1230                 out = eval(code, glob, local_ns)\n        out = undefined\n        code = <code object <module> at 0x7f6025716ed0, file \"<timed eval>\", line 1>\n        glob = {'Adam': <class 'keras.optimizers.Adam'>, 'CUSTOM_FILTERS': [<function strip_punctuation>, <function remove_stopwords>], 'Concatenate': <class 'keras.layers.merge.Concatenate'>, 'Conv2D': <class 'keras.layers.convolutional.Conv2D'>, 'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'Dense': <class 'keras.layers.core.Dense'>, 'Dropout': <class 'keras.layers.core.Dropout'>, 'FILENAME': 'processed_texts.pickle', 'HashingVectorizer': <class 'sklearn.feature_extraction.text.HashingVectorizer'>, 'In': ['', \"# corpus = [dictionary.doc2bow(text) for text in...k\\ncorpus = corpora.mmcorpus.MmCorpus('spooky.mm')\", \"import numpy as np\\nimport matplotlib.pyplot as p...lib', 'inline')\\n\\npd.options.display.max_rows = 10\", '# Dataset\\n\\n# use this if in fmi-hw... repo\\n# tra...pe)\\nprint(set(train.columns) - set(test.columns))', 'train.head(5)', 'from sklearn.pipeline import Pipeline\\nfrom sklea...om sklearn.model_selection import cross_val_score', \"pipeline = Pipeline([\\n    ('features', CountVect...peline, train.text, train.author, cv=3, n_jobs=3)\", 'from sklearn.model_selection import cross_val_pr...peline, train.text, train.author, cv=3, n_jobs=3)', \"import itertools\\nfrom sklearn.metrics import con...l('True label')\\n    plt.xlabel('Predicted label')\", 'pipeline.fit(train.text, train.author)\\nauthors =...matrix(train.author, prediction, classes=authors)', 'plot_confusion_matrix(train.author, prediction, classes=authors, normalize=True)', \"from sklearn.ensemble import RandomForestClassif...         cv=3, n_jobs=3, scoring='neg_log_loss'))\", \"from sklearn.linear_model import LogisticRegress...log_loss'))\\n\\n# Получихме малко по-добри резултати\", 'explore = train.copy()', \"# # бр. думи в текста\\n# explore['words'] = explo... sum([c in string.punctuation for c in str(s)]) )\", \"import nltk\\n# nltk.download('stopwords')\\nstopwords = nltk.corpus.stopwords.words('english')\", \"import nltk\\nnltk.download('stopwords')\\nstopwords = nltk.corpus.stopwords.words('english')\", \"import nltk\\n# nltk.download('stopwords')\\nstopwords = nltk.corpus.stopwords.words('english')\", \"# features_names = list(set(explore.columns) - {...or'})\\n# print('numeric features', features_names)\", \"from nltk.stem import WordNetLemmatizer\\nfrom nlt....split()])) \\n\\nexplore[['stemmed', 'text']].head()\", ...], ...}\n        local_ns = None\n   1231             except:\n   1232                 self.shell.showtraceback()\n   1233                 return\n   1234             end = clock2()\n\n...........................................................................\n/home/bob/gits/spooky/<timed eval> in <module>()\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in cross_val_score(estimator=Pipeline(memory=None,\n     steps=[('bow', Text2B...B(alpha=1.0, class_prior=None, fit_prior=True))]), X=id\nid26305    This process, however, afforded me...d it ...\nName: text, Length: 19579, dtype: object, y=id\nid26305    EAP\nid17569    HPL\nid11008    EAP\n...   HPL\nName: author, Length: 19579, dtype: object, groups=None, scoring='neg_log_loss', cv=3, n_jobs=3, verbose=0, fit_params=None, pre_dispatch='2*n_jobs')\n    337     cv_results = cross_validate(estimator=estimator, X=X, y=y, groups=groups,\n    338                                 scoring={'score': scorer}, cv=cv,\n    339                                 return_train_score=False,\n    340                                 n_jobs=n_jobs, verbose=verbose,\n    341                                 fit_params=fit_params,\n--> 342                                 pre_dispatch=pre_dispatch)\n        pre_dispatch = '2*n_jobs'\n    343     return cv_results['test_score']\n    344 \n    345 \n    346 def _fit_and_score(estimator, X, y, scorer, train, test, verbose,\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in cross_validate(estimator=Pipeline(memory=None,\n     steps=[('bow', Text2B...B(alpha=1.0, class_prior=None, fit_prior=True))]), X=id\nid26305    This process, however, afforded me...d it ...\nName: text, Length: 19579, dtype: object, y=id\nid26305    EAP\nid17569    HPL\nid11008    EAP\n...   HPL\nName: author, Length: 19579, dtype: object, groups=None, scoring={'score': make_scorer(log_loss, greater_is_better=False, needs_proba=True)}, cv=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=3, verbose=0, fit_params=None, pre_dispatch='2*n_jobs', return_train_score=False)\n    201     scores = parallel(\n    202         delayed(_fit_and_score)(\n    203             clone(estimator), X, y, scorers, train, test, verbose, None,\n    204             fit_params, return_train_score=return_train_score,\n    205             return_times=True)\n--> 206         for train, test in cv.split(X, y, groups))\n        cv.split = <bound method StratifiedKFold.split of Stratifie...ld(n_splits=3, random_state=None, shuffle=False)>\n        X = id\nid26305    This process, however, afforded me...d it ...\nName: text, Length: 19579, dtype: object\n        y = id\nid26305    EAP\nid17569    HPL\nid11008    EAP\n...   HPL\nName: author, Length: 19579, dtype: object\n        groups = None\n    207 \n    208     if return_train_score:\n    209         train_scores, test_scores, fit_times, score_times = zip(*scores)\n    210         train_scores = _aggregate_score_dicts(train_scores)\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=3), iterable=<generator object cross_validate.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=3)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nValueError                                         Wed Jan  3 06:40:41 2018\nPID: 18886                                   Python 3.6.3: /usr/bin/python3\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(memory=None,\n     steps=[('bow', Text2B...B(alpha=1.0, class_prior=None, fit_prior=True))]), id\nid26305    This process, however, afforded me...d it ...\nName: text, Length: 19579, dtype: object, id\nid26305    EAP\nid17569    HPL\nid11008    EAP\n...   HPL\nName: author, Length: 19579, dtype: object, {'score': make_scorer(log_loss, greater_is_better=False, needs_proba=True)}, array([ 6475,  6476,  6478, ..., 19576, 19577, 19578]), array([   0,    1,    2, ..., 6604, 6605, 6606]), 0, None, None), {'return_times': True, 'return_train_score': False})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(memory=None,\n     steps=[('bow', Text2B...B(alpha=1.0, class_prior=None, fit_prior=True))]), id\nid26305    This process, however, afforded me...d it ...\nName: text, Length: 19579, dtype: object, id\nid26305    EAP\nid17569    HPL\nid11008    EAP\n...   HPL\nName: author, Length: 19579, dtype: object, {'score': make_scorer(log_loss, greater_is_better=False, needs_proba=True)}, array([ 6475,  6476,  6478, ..., 19576, 19577, 19578]), array([   0,    1,    2, ..., 6604, 6605, 6606]), 0, None, None)\n        kwargs = {'return_times': True, 'return_train_score': False}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(memory=None,\n     steps=[('bow', Text2B...B(alpha=1.0, class_prior=None, fit_prior=True))]), X=id\nid26305    This process, however, afforded me...d it ...\nName: text, Length: 19579, dtype: object, y=id\nid26305    EAP\nid17569    HPL\nid11008    EAP\n...   HPL\nName: author, Length: 19579, dtype: object, scorer={'score': make_scorer(log_loss, greater_is_better=False, needs_proba=True)}, train=array([ 6475,  6476,  6478, ..., 19576, 19577, 19578]), test=array([   0,    1,    2, ..., 6604, 6605, 6606]), verbose=0, parameters=None, fit_params={}, return_train_score=False, return_parameters=False, return_n_test_samples=False, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(memory=No...(alpha=1.0, class_prior=None, fit_prior=True))])>\n        X_train = id\nid15739    Its close resemblance to the medic...d it ...\nName: text, Length: 13051, dtype: object\n        y_train = id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   HPL\nName: author, Length: 13051, dtype: object\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/sklearn/pipeline.py in fit(self=Pipeline(memory=None,\n     steps=[('bow', Text2B...B(alpha=1.0, class_prior=None, fit_prior=True))]), X=id\nid15739    Its close resemblance to the medic...d it ...\nName: text, Length: 13051, dtype: object, y=id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   HPL\nName: author, Length: 13051, dtype: object, **fit_params={})\n    245         self : Pipeline\n    246             This estimator\n    247         \"\"\"\n    248         Xt, fit_params = self._fit(X, y, **fit_params)\n    249         if self._final_estimator is not None:\n--> 250             self._final_estimator.fit(Xt, y, **fit_params)\n        self._final_estimator.fit = <bound method BaseDiscreteNB.fit of MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)>\n        Xt = [[(0, 0.2664714100608566), (1, 0.10480362539462479), (2, 0.2604873336049293), (3, 0.2319432706761858), (4, 0.24213524135011738), (5, 0.41116785474457307), (6, 0.39218741029198784), (7, 0.32007681200061067), (8, 0.35974012618542445), (9, 0.3057814279583449), (10, 0.033103432446463785), (11, 0.07685359149299217), (12, 0.19308643986019788), (13, 0.2126248353265933)], [(10, 0.024133907128526488), (11, 0.056029761946444906), (14, 0.2713402113709543), (15, 0.3420085240826053), (16, 0.5718449017195636), (17, 0.2614985439879363), (18, 0.1808821162765368), (19, 0.036987739317106506), (20, 0.42712530611518384), (21, 0.3189304655793621), (22, 0.09341436285112015), (23, 0.27476798475726655), (24, 0.088498637497393)], [(1, 0.07932158066320133), (10, 0.06263658764531839), (14, 0.14084602912179758), (18, 0.09389145710463107), (19, 0.01919942563133236), (22, 0.048489097889497096), (25, 0.2722725986774012), (26, 0.2620800981425273), (27, 0.24487868329638712), (28, 0.2722725986774012), (29, 0.03505679663903942), (30, 0.08624781176709262), (31, 0.07149215764800944), (32, 0.31119614453633054), (33, 0.2729933988114348), (34, 0.1733099699406774), (35, 0.17712329620393766), (36, 0.06842298368233549), (37, 0.1420511395070622), (38, 0.14386314798381059), ...], [(10, 0.015973364734577944), (19, 0.0489617074991789), (23, 0.18185904232327854), (30, 0.1099730849576353), (47, 0.046070957177769906), (48, 0.2975393193706599), (49, 0.11089059498724518), (50, 0.3968007922798119), (51, 0.19827784646150795), (52, 0.07722160044388235), (53, 0.04857698480295758), (54, 0.42811430060066746), (55, 0.3236593651896635), (56, 0.07556909941755309), (57, 0.24915443219781466), (58, 0.42811430060066746), (59, 0.3240930775601281)], [(10, 0.03745602951273969), (36, 0.10229058576225049), (47, 0.10803203710707177), (60, 0.23619581984220353), (61, 0.03133470722492), (62, 0.385564388719468), (63, 0.3015083410363086), (64, 0.11103716027435812), (65, 0.1905968735808948), (66, 0.24536394387695537), (67, 0.5308001436909793), (68, 0.3703268648059433), (69, 0.23994513095105036), (70, 0.3078975881412962)], [(47, 0.07100101906724518), (53, 0.07486311627769447), (61, 0.04118771069619043), (71, 0.15202575486829917), (72, 0.14256352538466183), (73, 0.4667449719251434), (74, 0.2503651118140704), (75, 0.37022902619505615), (76, 0.5477253203876611), (77, 0.4760416328690717)], [(10, 0.06462580383426865), (11, 0.06430138284840814), (18, 0.06919519082403108), (22, 0.03573501237295617), (31, 0.10537515645034605), (37, 0.15703115077885102), (46, 0.07518496273136628), (64, 0.0547374449573064), (65, 0.04697880354149405), (78, 0.1734637042645781), (79, 0.21124370752304877), (80, 0.1237755155749664), (81, 0.15694812556791132), (82, 0.14377383733539228), (83, 0.21875527504189865), (84, 0.24744076215980707), (85, 0.10989353196818279), (86, 0.20541728310317606), (87, 0.1788334923026323), (88, 0.3009539216891794), ...], [(10, 0.01715833256303342), (19, 0.026296941004117013), (22, 0.06641422358292039), (24, 0.06291932116373902), (29, 0.0480163589636599), (30, 0.11813132649510569), (47, 0.049488684312218235), (51, 0.21298688698342833), (53, 0.052180619049802826), (56, 0.08117511625389999), (61, 0.028708399385876993), (102, 0.23403001292583833), (113, 0.20423929132245422), (115, 0.4262370557703321), (116, 0.2690731650842324), (117, 0.3589641175494357), (118, 0.17450791755009787), (119, 0.2001630105211242), (120, 0.303670599600059), (121, 0.4262370557703321), ...], [(10, 0.021322122227474257), (11, 0.04950186582869004), (30, 0.14679809784366432), (31, 0.12168323506929181), (47, 0.12299607457840946), (52, 0.20615924454556003), (61, 0.035675028352090576), (91, 0.14487860845275446), (98, 0.27840822828588596), (103, 0.23291869434274284), (124, 0.13839751535552838), (125, 0.3007913272538198), (126, 0.39081820673094264), (127, 0.2462174723129556), (128, 0.4123268436706392), (129, 0.22158417220596172), (130, 0.3435567610008015), (131, 0.18912652719692835), (132, 0.2306072899995085)], [(10, 0.060204206982548865), (19, 0.055361550083157234), (22, 0.09321221974181244), (49, 0.08359015703908927), (53, 0.036617693219445066), (61, 0.04029217669993592), (104, 0.0754428098308278), (133, 0.26790813350704007), (134, 0.18687533762152778), (135, 0.24430377753540683), (136, 0.23809501978731895), (137, 0.2755070388560312), (138, 0.21903792681162104), (139, 0.12747319308243932), (140, 0.12362178362577428), (141, 0.14439454632496657), (142, 0.22244615763028036), (143, 0.21903792681162104), (144, 0.23809501978731895), (145, 0.07431384829899074), ...], [(10, 0.017868951261395715), (11, 0.08296982996385277), (61, 0.02989736837972952), (72, 0.10348412582038423), (155, 0.31426513521849647), (156, 0.1699525813091037), (157, 0.1701766922652076), (158, 0.27640100939491724), (159, 0.2978192836150871), (160, 0.24276889717607655), (161, 0.33880121349699616), (162, 0.23450847942150704), (163, 0.3625537576651779), (164, 0.3183102485560416), (165, 0.24137147345766863), (166, 0.3738307494342447)], [(10, 0.04520429241093692), (11, 0.02623679756093562), (19, 0.017320077669206606), (22, 0.08748542354155527), (29, 0.06325047970506475), (53, 0.034367966016593844), (57, 0.08813773738110729), (61, 0.018908347820537255), (167, 0.10458059386108247), (168, 0.19113441661345087), (169, 0.3028886579507455), (170, 0.25858036692787534), (171, 0.13969408373222703), (172, 0.16532352454213142), (173, 0.16532352454213142), (174, 0.3028886579507455), (175, 0.19313267199821985), (176, 0.17097852648678477), (177, 0.2807345124393104), (178, 0.09429094471794976), ...], [(10, 0.009856156567640691), (15, 0.13967442332789326), (19, 0.04531677542111343), (24, 0.10842708839707535), (29, 0.11032697962119518), (41, 0.07260064341795866), (49, 0.06842359667072355), (56, 0.04662892808818644), (57, 0.1537374957667718), (61, 0.016490791171816524), (165, 0.13313568315005253), (190, 0.16755438923335436), (191, 0.13904086957076212), (192, 0.2448407583411407), (193, 0.12430484550933028), (194, 0.18687598151030094), (195, 0.15050032010750405), (196, 0.09143544591163232), (197, 0.12269104128755341), (198, 0.2641623506180873), ...], [(10, 0.05128666371555671), (11, 0.02381362904652451), (19, 0.047161308888427914), (22, 0.07940547692065152), (24, 0.037613469186254155), (30, 0.07061946834251158), (47, 0.05916914130722203), (61, 0.03432403515216796), (65, 0.05219494905222182), (66, 0.13438581975978053), (71, 0.06334580468218413), (72, 0.05940310075515709), (103, 0.05602454869351825), (111, 0.08056360687311669), (124, 0.0665782397585325), (131, 0.09098220614776903), (217, 0.13014327126494443), (218, 0.14707666165679775), (219, 0.1425038820600952), (220, 0.27491457850702283), ...], [(10, 0.03373914624704608), (11, 0.03916473868965497), (19, 0.05170877386415411), (22, 0.03264826570583768), (24, 0.06186044508432571), (29, 0.047208286451325736), (36, 0.046069979621768835), (45, 0.08087235171979418), (47, 0.024327916938460356), (53, 0.07695369862930257), (56, 0.07980901141908836), (61, 0.028225262619209712), (69, 0.1080672987732272), (112, 0.04557089762967087), (127, 0.09740080300544345), (241, 0.1796469603779444), (242, 0.08992259466649655), (243, 0.11166235358548782), (244, 0.15848012162533082), (245, 0.26548969682262125), ...], [(10, 0.014878436612043862), (11, 0.0690841526047578), (19, 0.022802761770881638), (24, 0.05455898049349969), (30, 0.10243474689030971), (45, 0.071326888352145), (47, 0.08582585164937677), (53, 0.045247172477722745), (56, 0.07038905541794807), (69, 0.09531198228070385), (104, 0.09322197900786375), (112, 0.08038421681121939), (131, 0.13197124641419022), (182, 0.17819993890438107), (185, 0.13080027223004068), (270, 0.18048741343046848), (271, 0.12834700317270165), (277, 0.21653463756000216), (278, 0.15765046010508932), (279, 0.3696012414193431), ...], [(10, 0.03791716652108541), (11, 0.05868616859386632), (22, 0.04892159860155609), (29, 0.1061084619891196), (36, 0.13806657118911214), (53, 0.03843693658103161), (64, 0.07493612378605678), (65, 0.06431446407942601), (91, 0.08587929261825251), (105, 0.3574063109864729), (141, 0.15156836850641137), (145, 0.07800591525639525), (213, 0.12076867033953673), (219, 0.17559286809079885), (222, 0.09905080713055688), (290, 0.17634543810717943), (291, 0.2354304202988956), (292, 0.23747359098140175), (293, 0.21838240905413028), (294, 0.2068871594723075), ...], [(10, 0.01568731468793941), (29, 0.08779964258937602), (37, 0.0889416637703053), (53, 0.04770707110604152), (61, 0.02624717137862027), (64, 0.09300905076906602), (237, 0.1379113333775812), (305, 0.30664896585612444), (306, 0.2833078792717857), (307, 0.20730843748050645), (308, 0.2233412719391391), (309, 0.38969490776368737), (310, 0.2695472861513626), (311, 0.21271630313917306), (312, 0.4204476556771751), (313, 0.20658700960294096), (314, 0.4204476556771751), (315, 0.14264507531499204)], [(10, 0.027389551203659726), (316, 0.7340881994597976), (317, 0.37735752159971553), (318, 0.5638844108421996)], [(18, 0.35789169848754704), (36, 0.13040599539512807), (52, 0.11542397475108049), (92, 0.1830990480525225), (130, 0.19234978763494665), (319, 0.37310009228736857), (320, 0.5931025045589059), (321, 0.2808020052524273), (322, 0.44859490237601146)], ...]\n        y = id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   HPL\nName: author, Length: 13051, dtype: object\n        fit_params = {}\n    251         return self\n    252 \n    253     def fit_transform(self, X, y=None, **fit_params):\n    254         \"\"\"Fit the model and transform with the final estimator\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/sklearn/naive_bayes.py in fit(self=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), X=[[(0, 0.2664714100608566), (1, 0.10480362539462479), (2, 0.2604873336049293), (3, 0.2319432706761858), (4, 0.24213524135011738), (5, 0.41116785474457307), (6, 0.39218741029198784), (7, 0.32007681200061067), (8, 0.35974012618542445), (9, 0.3057814279583449), (10, 0.033103432446463785), (11, 0.07685359149299217), (12, 0.19308643986019788), (13, 0.2126248353265933)], [(10, 0.024133907128526488), (11, 0.056029761946444906), (14, 0.2713402113709543), (15, 0.3420085240826053), (16, 0.5718449017195636), (17, 0.2614985439879363), (18, 0.1808821162765368), (19, 0.036987739317106506), (20, 0.42712530611518384), (21, 0.3189304655793621), (22, 0.09341436285112015), (23, 0.27476798475726655), (24, 0.088498637497393)], [(1, 0.07932158066320133), (10, 0.06263658764531839), (14, 0.14084602912179758), (18, 0.09389145710463107), (19, 0.01919942563133236), (22, 0.048489097889497096), (25, 0.2722725986774012), (26, 0.2620800981425273), (27, 0.24487868329638712), (28, 0.2722725986774012), (29, 0.03505679663903942), (30, 0.08624781176709262), (31, 0.07149215764800944), (32, 0.31119614453633054), (33, 0.2729933988114348), (34, 0.1733099699406774), (35, 0.17712329620393766), (36, 0.06842298368233549), (37, 0.1420511395070622), (38, 0.14386314798381059), ...], [(10, 0.015973364734577944), (19, 0.0489617074991789), (23, 0.18185904232327854), (30, 0.1099730849576353), (47, 0.046070957177769906), (48, 0.2975393193706599), (49, 0.11089059498724518), (50, 0.3968007922798119), (51, 0.19827784646150795), (52, 0.07722160044388235), (53, 0.04857698480295758), (54, 0.42811430060066746), (55, 0.3236593651896635), (56, 0.07556909941755309), (57, 0.24915443219781466), (58, 0.42811430060066746), (59, 0.3240930775601281)], [(10, 0.03745602951273969), (36, 0.10229058576225049), (47, 0.10803203710707177), (60, 0.23619581984220353), (61, 0.03133470722492), (62, 0.385564388719468), (63, 0.3015083410363086), (64, 0.11103716027435812), (65, 0.1905968735808948), (66, 0.24536394387695537), (67, 0.5308001436909793), (68, 0.3703268648059433), (69, 0.23994513095105036), (70, 0.3078975881412962)], [(47, 0.07100101906724518), (53, 0.07486311627769447), (61, 0.04118771069619043), (71, 0.15202575486829917), (72, 0.14256352538466183), (73, 0.4667449719251434), (74, 0.2503651118140704), (75, 0.37022902619505615), (76, 0.5477253203876611), (77, 0.4760416328690717)], [(10, 0.06462580383426865), (11, 0.06430138284840814), (18, 0.06919519082403108), (22, 0.03573501237295617), (31, 0.10537515645034605), (37, 0.15703115077885102), (46, 0.07518496273136628), (64, 0.0547374449573064), (65, 0.04697880354149405), (78, 0.1734637042645781), (79, 0.21124370752304877), (80, 0.1237755155749664), (81, 0.15694812556791132), (82, 0.14377383733539228), (83, 0.21875527504189865), (84, 0.24744076215980707), (85, 0.10989353196818279), (86, 0.20541728310317606), (87, 0.1788334923026323), (88, 0.3009539216891794), ...], [(10, 0.01715833256303342), (19, 0.026296941004117013), (22, 0.06641422358292039), (24, 0.06291932116373902), (29, 0.0480163589636599), (30, 0.11813132649510569), (47, 0.049488684312218235), (51, 0.21298688698342833), (53, 0.052180619049802826), (56, 0.08117511625389999), (61, 0.028708399385876993), (102, 0.23403001292583833), (113, 0.20423929132245422), (115, 0.4262370557703321), (116, 0.2690731650842324), (117, 0.3589641175494357), (118, 0.17450791755009787), (119, 0.2001630105211242), (120, 0.303670599600059), (121, 0.4262370557703321), ...], [(10, 0.021322122227474257), (11, 0.04950186582869004), (30, 0.14679809784366432), (31, 0.12168323506929181), (47, 0.12299607457840946), (52, 0.20615924454556003), (61, 0.035675028352090576), (91, 0.14487860845275446), (98, 0.27840822828588596), (103, 0.23291869434274284), (124, 0.13839751535552838), (125, 0.3007913272538198), (126, 0.39081820673094264), (127, 0.2462174723129556), (128, 0.4123268436706392), (129, 0.22158417220596172), (130, 0.3435567610008015), (131, 0.18912652719692835), (132, 0.2306072899995085)], [(10, 0.060204206982548865), (19, 0.055361550083157234), (22, 0.09321221974181244), (49, 0.08359015703908927), (53, 0.036617693219445066), (61, 0.04029217669993592), (104, 0.0754428098308278), (133, 0.26790813350704007), (134, 0.18687533762152778), (135, 0.24430377753540683), (136, 0.23809501978731895), (137, 0.2755070388560312), (138, 0.21903792681162104), (139, 0.12747319308243932), (140, 0.12362178362577428), (141, 0.14439454632496657), (142, 0.22244615763028036), (143, 0.21903792681162104), (144, 0.23809501978731895), (145, 0.07431384829899074), ...], [(10, 0.017868951261395715), (11, 0.08296982996385277), (61, 0.02989736837972952), (72, 0.10348412582038423), (155, 0.31426513521849647), (156, 0.1699525813091037), (157, 0.1701766922652076), (158, 0.27640100939491724), (159, 0.2978192836150871), (160, 0.24276889717607655), (161, 0.33880121349699616), (162, 0.23450847942150704), (163, 0.3625537576651779), (164, 0.3183102485560416), (165, 0.24137147345766863), (166, 0.3738307494342447)], [(10, 0.04520429241093692), (11, 0.02623679756093562), (19, 0.017320077669206606), (22, 0.08748542354155527), (29, 0.06325047970506475), (53, 0.034367966016593844), (57, 0.08813773738110729), (61, 0.018908347820537255), (167, 0.10458059386108247), (168, 0.19113441661345087), (169, 0.3028886579507455), (170, 0.25858036692787534), (171, 0.13969408373222703), (172, 0.16532352454213142), (173, 0.16532352454213142), (174, 0.3028886579507455), (175, 0.19313267199821985), (176, 0.17097852648678477), (177, 0.2807345124393104), (178, 0.09429094471794976), ...], [(10, 0.009856156567640691), (15, 0.13967442332789326), (19, 0.04531677542111343), (24, 0.10842708839707535), (29, 0.11032697962119518), (41, 0.07260064341795866), (49, 0.06842359667072355), (56, 0.04662892808818644), (57, 0.1537374957667718), (61, 0.016490791171816524), (165, 0.13313568315005253), (190, 0.16755438923335436), (191, 0.13904086957076212), (192, 0.2448407583411407), (193, 0.12430484550933028), (194, 0.18687598151030094), (195, 0.15050032010750405), (196, 0.09143544591163232), (197, 0.12269104128755341), (198, 0.2641623506180873), ...], [(10, 0.05128666371555671), (11, 0.02381362904652451), (19, 0.047161308888427914), (22, 0.07940547692065152), (24, 0.037613469186254155), (30, 0.07061946834251158), (47, 0.05916914130722203), (61, 0.03432403515216796), (65, 0.05219494905222182), (66, 0.13438581975978053), (71, 0.06334580468218413), (72, 0.05940310075515709), (103, 0.05602454869351825), (111, 0.08056360687311669), (124, 0.0665782397585325), (131, 0.09098220614776903), (217, 0.13014327126494443), (218, 0.14707666165679775), (219, 0.1425038820600952), (220, 0.27491457850702283), ...], [(10, 0.03373914624704608), (11, 0.03916473868965497), (19, 0.05170877386415411), (22, 0.03264826570583768), (24, 0.06186044508432571), (29, 0.047208286451325736), (36, 0.046069979621768835), (45, 0.08087235171979418), (47, 0.024327916938460356), (53, 0.07695369862930257), (56, 0.07980901141908836), (61, 0.028225262619209712), (69, 0.1080672987732272), (112, 0.04557089762967087), (127, 0.09740080300544345), (241, 0.1796469603779444), (242, 0.08992259466649655), (243, 0.11166235358548782), (244, 0.15848012162533082), (245, 0.26548969682262125), ...], [(10, 0.014878436612043862), (11, 0.0690841526047578), (19, 0.022802761770881638), (24, 0.05455898049349969), (30, 0.10243474689030971), (45, 0.071326888352145), (47, 0.08582585164937677), (53, 0.045247172477722745), (56, 0.07038905541794807), (69, 0.09531198228070385), (104, 0.09322197900786375), (112, 0.08038421681121939), (131, 0.13197124641419022), (182, 0.17819993890438107), (185, 0.13080027223004068), (270, 0.18048741343046848), (271, 0.12834700317270165), (277, 0.21653463756000216), (278, 0.15765046010508932), (279, 0.3696012414193431), ...], [(10, 0.03791716652108541), (11, 0.05868616859386632), (22, 0.04892159860155609), (29, 0.1061084619891196), (36, 0.13806657118911214), (53, 0.03843693658103161), (64, 0.07493612378605678), (65, 0.06431446407942601), (91, 0.08587929261825251), (105, 0.3574063109864729), (141, 0.15156836850641137), (145, 0.07800591525639525), (213, 0.12076867033953673), (219, 0.17559286809079885), (222, 0.09905080713055688), (290, 0.17634543810717943), (291, 0.2354304202988956), (292, 0.23747359098140175), (293, 0.21838240905413028), (294, 0.2068871594723075), ...], [(10, 0.01568731468793941), (29, 0.08779964258937602), (37, 0.0889416637703053), (53, 0.04770707110604152), (61, 0.02624717137862027), (64, 0.09300905076906602), (237, 0.1379113333775812), (305, 0.30664896585612444), (306, 0.2833078792717857), (307, 0.20730843748050645), (308, 0.2233412719391391), (309, 0.38969490776368737), (310, 0.2695472861513626), (311, 0.21271630313917306), (312, 0.4204476556771751), (313, 0.20658700960294096), (314, 0.4204476556771751), (315, 0.14264507531499204)], [(10, 0.027389551203659726), (316, 0.7340881994597976), (317, 0.37735752159971553), (318, 0.5638844108421996)], [(18, 0.35789169848754704), (36, 0.13040599539512807), (52, 0.11542397475108049), (92, 0.1830990480525225), (130, 0.19234978763494665), (319, 0.37310009228736857), (320, 0.5931025045589059), (321, 0.2808020052524273), (322, 0.44859490237601146)], ...], y=id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   HPL\nName: author, Length: 13051, dtype: object, sample_weight=None)\n    574         Returns\n    575         -------\n    576         self : object\n    577             Returns self.\n    578         \"\"\"\n--> 579         X, y = check_X_y(X, y, 'csr')\n        X = [[(0, 0.2664714100608566), (1, 0.10480362539462479), (2, 0.2604873336049293), (3, 0.2319432706761858), (4, 0.24213524135011738), (5, 0.41116785474457307), (6, 0.39218741029198784), (7, 0.32007681200061067), (8, 0.35974012618542445), (9, 0.3057814279583449), (10, 0.033103432446463785), (11, 0.07685359149299217), (12, 0.19308643986019788), (13, 0.2126248353265933)], [(10, 0.024133907128526488), (11, 0.056029761946444906), (14, 0.2713402113709543), (15, 0.3420085240826053), (16, 0.5718449017195636), (17, 0.2614985439879363), (18, 0.1808821162765368), (19, 0.036987739317106506), (20, 0.42712530611518384), (21, 0.3189304655793621), (22, 0.09341436285112015), (23, 0.27476798475726655), (24, 0.088498637497393)], [(1, 0.07932158066320133), (10, 0.06263658764531839), (14, 0.14084602912179758), (18, 0.09389145710463107), (19, 0.01919942563133236), (22, 0.048489097889497096), (25, 0.2722725986774012), (26, 0.2620800981425273), (27, 0.24487868329638712), (28, 0.2722725986774012), (29, 0.03505679663903942), (30, 0.08624781176709262), (31, 0.07149215764800944), (32, 0.31119614453633054), (33, 0.2729933988114348), (34, 0.1733099699406774), (35, 0.17712329620393766), (36, 0.06842298368233549), (37, 0.1420511395070622), (38, 0.14386314798381059), ...], [(10, 0.015973364734577944), (19, 0.0489617074991789), (23, 0.18185904232327854), (30, 0.1099730849576353), (47, 0.046070957177769906), (48, 0.2975393193706599), (49, 0.11089059498724518), (50, 0.3968007922798119), (51, 0.19827784646150795), (52, 0.07722160044388235), (53, 0.04857698480295758), (54, 0.42811430060066746), (55, 0.3236593651896635), (56, 0.07556909941755309), (57, 0.24915443219781466), (58, 0.42811430060066746), (59, 0.3240930775601281)], [(10, 0.03745602951273969), (36, 0.10229058576225049), (47, 0.10803203710707177), (60, 0.23619581984220353), (61, 0.03133470722492), (62, 0.385564388719468), (63, 0.3015083410363086), (64, 0.11103716027435812), (65, 0.1905968735808948), (66, 0.24536394387695537), (67, 0.5308001436909793), (68, 0.3703268648059433), (69, 0.23994513095105036), (70, 0.3078975881412962)], [(47, 0.07100101906724518), (53, 0.07486311627769447), (61, 0.04118771069619043), (71, 0.15202575486829917), (72, 0.14256352538466183), (73, 0.4667449719251434), (74, 0.2503651118140704), (75, 0.37022902619505615), (76, 0.5477253203876611), (77, 0.4760416328690717)], [(10, 0.06462580383426865), (11, 0.06430138284840814), (18, 0.06919519082403108), (22, 0.03573501237295617), (31, 0.10537515645034605), (37, 0.15703115077885102), (46, 0.07518496273136628), (64, 0.0547374449573064), (65, 0.04697880354149405), (78, 0.1734637042645781), (79, 0.21124370752304877), (80, 0.1237755155749664), (81, 0.15694812556791132), (82, 0.14377383733539228), (83, 0.21875527504189865), (84, 0.24744076215980707), (85, 0.10989353196818279), (86, 0.20541728310317606), (87, 0.1788334923026323), (88, 0.3009539216891794), ...], [(10, 0.01715833256303342), (19, 0.026296941004117013), (22, 0.06641422358292039), (24, 0.06291932116373902), (29, 0.0480163589636599), (30, 0.11813132649510569), (47, 0.049488684312218235), (51, 0.21298688698342833), (53, 0.052180619049802826), (56, 0.08117511625389999), (61, 0.028708399385876993), (102, 0.23403001292583833), (113, 0.20423929132245422), (115, 0.4262370557703321), (116, 0.2690731650842324), (117, 0.3589641175494357), (118, 0.17450791755009787), (119, 0.2001630105211242), (120, 0.303670599600059), (121, 0.4262370557703321), ...], [(10, 0.021322122227474257), (11, 0.04950186582869004), (30, 0.14679809784366432), (31, 0.12168323506929181), (47, 0.12299607457840946), (52, 0.20615924454556003), (61, 0.035675028352090576), (91, 0.14487860845275446), (98, 0.27840822828588596), (103, 0.23291869434274284), (124, 0.13839751535552838), (125, 0.3007913272538198), (126, 0.39081820673094264), (127, 0.2462174723129556), (128, 0.4123268436706392), (129, 0.22158417220596172), (130, 0.3435567610008015), (131, 0.18912652719692835), (132, 0.2306072899995085)], [(10, 0.060204206982548865), (19, 0.055361550083157234), (22, 0.09321221974181244), (49, 0.08359015703908927), (53, 0.036617693219445066), (61, 0.04029217669993592), (104, 0.0754428098308278), (133, 0.26790813350704007), (134, 0.18687533762152778), (135, 0.24430377753540683), (136, 0.23809501978731895), (137, 0.2755070388560312), (138, 0.21903792681162104), (139, 0.12747319308243932), (140, 0.12362178362577428), (141, 0.14439454632496657), (142, 0.22244615763028036), (143, 0.21903792681162104), (144, 0.23809501978731895), (145, 0.07431384829899074), ...], [(10, 0.017868951261395715), (11, 0.08296982996385277), (61, 0.02989736837972952), (72, 0.10348412582038423), (155, 0.31426513521849647), (156, 0.1699525813091037), (157, 0.1701766922652076), (158, 0.27640100939491724), (159, 0.2978192836150871), (160, 0.24276889717607655), (161, 0.33880121349699616), (162, 0.23450847942150704), (163, 0.3625537576651779), (164, 0.3183102485560416), (165, 0.24137147345766863), (166, 0.3738307494342447)], [(10, 0.04520429241093692), (11, 0.02623679756093562), (19, 0.017320077669206606), (22, 0.08748542354155527), (29, 0.06325047970506475), (53, 0.034367966016593844), (57, 0.08813773738110729), (61, 0.018908347820537255), (167, 0.10458059386108247), (168, 0.19113441661345087), (169, 0.3028886579507455), (170, 0.25858036692787534), (171, 0.13969408373222703), (172, 0.16532352454213142), (173, 0.16532352454213142), (174, 0.3028886579507455), (175, 0.19313267199821985), (176, 0.17097852648678477), (177, 0.2807345124393104), (178, 0.09429094471794976), ...], [(10, 0.009856156567640691), (15, 0.13967442332789326), (19, 0.04531677542111343), (24, 0.10842708839707535), (29, 0.11032697962119518), (41, 0.07260064341795866), (49, 0.06842359667072355), (56, 0.04662892808818644), (57, 0.1537374957667718), (61, 0.016490791171816524), (165, 0.13313568315005253), (190, 0.16755438923335436), (191, 0.13904086957076212), (192, 0.2448407583411407), (193, 0.12430484550933028), (194, 0.18687598151030094), (195, 0.15050032010750405), (196, 0.09143544591163232), (197, 0.12269104128755341), (198, 0.2641623506180873), ...], [(10, 0.05128666371555671), (11, 0.02381362904652451), (19, 0.047161308888427914), (22, 0.07940547692065152), (24, 0.037613469186254155), (30, 0.07061946834251158), (47, 0.05916914130722203), (61, 0.03432403515216796), (65, 0.05219494905222182), (66, 0.13438581975978053), (71, 0.06334580468218413), (72, 0.05940310075515709), (103, 0.05602454869351825), (111, 0.08056360687311669), (124, 0.0665782397585325), (131, 0.09098220614776903), (217, 0.13014327126494443), (218, 0.14707666165679775), (219, 0.1425038820600952), (220, 0.27491457850702283), ...], [(10, 0.03373914624704608), (11, 0.03916473868965497), (19, 0.05170877386415411), (22, 0.03264826570583768), (24, 0.06186044508432571), (29, 0.047208286451325736), (36, 0.046069979621768835), (45, 0.08087235171979418), (47, 0.024327916938460356), (53, 0.07695369862930257), (56, 0.07980901141908836), (61, 0.028225262619209712), (69, 0.1080672987732272), (112, 0.04557089762967087), (127, 0.09740080300544345), (241, 0.1796469603779444), (242, 0.08992259466649655), (243, 0.11166235358548782), (244, 0.15848012162533082), (245, 0.26548969682262125), ...], [(10, 0.014878436612043862), (11, 0.0690841526047578), (19, 0.022802761770881638), (24, 0.05455898049349969), (30, 0.10243474689030971), (45, 0.071326888352145), (47, 0.08582585164937677), (53, 0.045247172477722745), (56, 0.07038905541794807), (69, 0.09531198228070385), (104, 0.09322197900786375), (112, 0.08038421681121939), (131, 0.13197124641419022), (182, 0.17819993890438107), (185, 0.13080027223004068), (270, 0.18048741343046848), (271, 0.12834700317270165), (277, 0.21653463756000216), (278, 0.15765046010508932), (279, 0.3696012414193431), ...], [(10, 0.03791716652108541), (11, 0.05868616859386632), (22, 0.04892159860155609), (29, 0.1061084619891196), (36, 0.13806657118911214), (53, 0.03843693658103161), (64, 0.07493612378605678), (65, 0.06431446407942601), (91, 0.08587929261825251), (105, 0.3574063109864729), (141, 0.15156836850641137), (145, 0.07800591525639525), (213, 0.12076867033953673), (219, 0.17559286809079885), (222, 0.09905080713055688), (290, 0.17634543810717943), (291, 0.2354304202988956), (292, 0.23747359098140175), (293, 0.21838240905413028), (294, 0.2068871594723075), ...], [(10, 0.01568731468793941), (29, 0.08779964258937602), (37, 0.0889416637703053), (53, 0.04770707110604152), (61, 0.02624717137862027), (64, 0.09300905076906602), (237, 0.1379113333775812), (305, 0.30664896585612444), (306, 0.2833078792717857), (307, 0.20730843748050645), (308, 0.2233412719391391), (309, 0.38969490776368737), (310, 0.2695472861513626), (311, 0.21271630313917306), (312, 0.4204476556771751), (313, 0.20658700960294096), (314, 0.4204476556771751), (315, 0.14264507531499204)], [(10, 0.027389551203659726), (316, 0.7340881994597976), (317, 0.37735752159971553), (318, 0.5638844108421996)], [(18, 0.35789169848754704), (36, 0.13040599539512807), (52, 0.11542397475108049), (92, 0.1830990480525225), (130, 0.19234978763494665), (319, 0.37310009228736857), (320, 0.5931025045589059), (321, 0.2808020052524273), (322, 0.44859490237601146)], ...]\n        y = id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   HPL\nName: author, Length: 13051, dtype: object\n    580         _, n_features = X.shape\n    581 \n    582         labelbin = LabelBinarizer()\n    583         Y = labelbin.fit_transform(y)\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/sklearn/utils/validation.py in check_X_y(X=[[(0, 0.2664714100608566), (1, 0.10480362539462479), (2, 0.2604873336049293), (3, 0.2319432706761858), (4, 0.24213524135011738), (5, 0.41116785474457307), (6, 0.39218741029198784), (7, 0.32007681200061067), (8, 0.35974012618542445), (9, 0.3057814279583449), (10, 0.033103432446463785), (11, 0.07685359149299217), (12, 0.19308643986019788), (13, 0.2126248353265933)], [(10, 0.024133907128526488), (11, 0.056029761946444906), (14, 0.2713402113709543), (15, 0.3420085240826053), (16, 0.5718449017195636), (17, 0.2614985439879363), (18, 0.1808821162765368), (19, 0.036987739317106506), (20, 0.42712530611518384), (21, 0.3189304655793621), (22, 0.09341436285112015), (23, 0.27476798475726655), (24, 0.088498637497393)], [(1, 0.07932158066320133), (10, 0.06263658764531839), (14, 0.14084602912179758), (18, 0.09389145710463107), (19, 0.01919942563133236), (22, 0.048489097889497096), (25, 0.2722725986774012), (26, 0.2620800981425273), (27, 0.24487868329638712), (28, 0.2722725986774012), (29, 0.03505679663903942), (30, 0.08624781176709262), (31, 0.07149215764800944), (32, 0.31119614453633054), (33, 0.2729933988114348), (34, 0.1733099699406774), (35, 0.17712329620393766), (36, 0.06842298368233549), (37, 0.1420511395070622), (38, 0.14386314798381059), ...], [(10, 0.015973364734577944), (19, 0.0489617074991789), (23, 0.18185904232327854), (30, 0.1099730849576353), (47, 0.046070957177769906), (48, 0.2975393193706599), (49, 0.11089059498724518), (50, 0.3968007922798119), (51, 0.19827784646150795), (52, 0.07722160044388235), (53, 0.04857698480295758), (54, 0.42811430060066746), (55, 0.3236593651896635), (56, 0.07556909941755309), (57, 0.24915443219781466), (58, 0.42811430060066746), (59, 0.3240930775601281)], [(10, 0.03745602951273969), (36, 0.10229058576225049), (47, 0.10803203710707177), (60, 0.23619581984220353), (61, 0.03133470722492), (62, 0.385564388719468), (63, 0.3015083410363086), (64, 0.11103716027435812), (65, 0.1905968735808948), (66, 0.24536394387695537), (67, 0.5308001436909793), (68, 0.3703268648059433), (69, 0.23994513095105036), (70, 0.3078975881412962)], [(47, 0.07100101906724518), (53, 0.07486311627769447), (61, 0.04118771069619043), (71, 0.15202575486829917), (72, 0.14256352538466183), (73, 0.4667449719251434), (74, 0.2503651118140704), (75, 0.37022902619505615), (76, 0.5477253203876611), (77, 0.4760416328690717)], [(10, 0.06462580383426865), (11, 0.06430138284840814), (18, 0.06919519082403108), (22, 0.03573501237295617), (31, 0.10537515645034605), (37, 0.15703115077885102), (46, 0.07518496273136628), (64, 0.0547374449573064), (65, 0.04697880354149405), (78, 0.1734637042645781), (79, 0.21124370752304877), (80, 0.1237755155749664), (81, 0.15694812556791132), (82, 0.14377383733539228), (83, 0.21875527504189865), (84, 0.24744076215980707), (85, 0.10989353196818279), (86, 0.20541728310317606), (87, 0.1788334923026323), (88, 0.3009539216891794), ...], [(10, 0.01715833256303342), (19, 0.026296941004117013), (22, 0.06641422358292039), (24, 0.06291932116373902), (29, 0.0480163589636599), (30, 0.11813132649510569), (47, 0.049488684312218235), (51, 0.21298688698342833), (53, 0.052180619049802826), (56, 0.08117511625389999), (61, 0.028708399385876993), (102, 0.23403001292583833), (113, 0.20423929132245422), (115, 0.4262370557703321), (116, 0.2690731650842324), (117, 0.3589641175494357), (118, 0.17450791755009787), (119, 0.2001630105211242), (120, 0.303670599600059), (121, 0.4262370557703321), ...], [(10, 0.021322122227474257), (11, 0.04950186582869004), (30, 0.14679809784366432), (31, 0.12168323506929181), (47, 0.12299607457840946), (52, 0.20615924454556003), (61, 0.035675028352090576), (91, 0.14487860845275446), (98, 0.27840822828588596), (103, 0.23291869434274284), (124, 0.13839751535552838), (125, 0.3007913272538198), (126, 0.39081820673094264), (127, 0.2462174723129556), (128, 0.4123268436706392), (129, 0.22158417220596172), (130, 0.3435567610008015), (131, 0.18912652719692835), (132, 0.2306072899995085)], [(10, 0.060204206982548865), (19, 0.055361550083157234), (22, 0.09321221974181244), (49, 0.08359015703908927), (53, 0.036617693219445066), (61, 0.04029217669993592), (104, 0.0754428098308278), (133, 0.26790813350704007), (134, 0.18687533762152778), (135, 0.24430377753540683), (136, 0.23809501978731895), (137, 0.2755070388560312), (138, 0.21903792681162104), (139, 0.12747319308243932), (140, 0.12362178362577428), (141, 0.14439454632496657), (142, 0.22244615763028036), (143, 0.21903792681162104), (144, 0.23809501978731895), (145, 0.07431384829899074), ...], [(10, 0.017868951261395715), (11, 0.08296982996385277), (61, 0.02989736837972952), (72, 0.10348412582038423), (155, 0.31426513521849647), (156, 0.1699525813091037), (157, 0.1701766922652076), (158, 0.27640100939491724), (159, 0.2978192836150871), (160, 0.24276889717607655), (161, 0.33880121349699616), (162, 0.23450847942150704), (163, 0.3625537576651779), (164, 0.3183102485560416), (165, 0.24137147345766863), (166, 0.3738307494342447)], [(10, 0.04520429241093692), (11, 0.02623679756093562), (19, 0.017320077669206606), (22, 0.08748542354155527), (29, 0.06325047970506475), (53, 0.034367966016593844), (57, 0.08813773738110729), (61, 0.018908347820537255), (167, 0.10458059386108247), (168, 0.19113441661345087), (169, 0.3028886579507455), (170, 0.25858036692787534), (171, 0.13969408373222703), (172, 0.16532352454213142), (173, 0.16532352454213142), (174, 0.3028886579507455), (175, 0.19313267199821985), (176, 0.17097852648678477), (177, 0.2807345124393104), (178, 0.09429094471794976), ...], [(10, 0.009856156567640691), (15, 0.13967442332789326), (19, 0.04531677542111343), (24, 0.10842708839707535), (29, 0.11032697962119518), (41, 0.07260064341795866), (49, 0.06842359667072355), (56, 0.04662892808818644), (57, 0.1537374957667718), (61, 0.016490791171816524), (165, 0.13313568315005253), (190, 0.16755438923335436), (191, 0.13904086957076212), (192, 0.2448407583411407), (193, 0.12430484550933028), (194, 0.18687598151030094), (195, 0.15050032010750405), (196, 0.09143544591163232), (197, 0.12269104128755341), (198, 0.2641623506180873), ...], [(10, 0.05128666371555671), (11, 0.02381362904652451), (19, 0.047161308888427914), (22, 0.07940547692065152), (24, 0.037613469186254155), (30, 0.07061946834251158), (47, 0.05916914130722203), (61, 0.03432403515216796), (65, 0.05219494905222182), (66, 0.13438581975978053), (71, 0.06334580468218413), (72, 0.05940310075515709), (103, 0.05602454869351825), (111, 0.08056360687311669), (124, 0.0665782397585325), (131, 0.09098220614776903), (217, 0.13014327126494443), (218, 0.14707666165679775), (219, 0.1425038820600952), (220, 0.27491457850702283), ...], [(10, 0.03373914624704608), (11, 0.03916473868965497), (19, 0.05170877386415411), (22, 0.03264826570583768), (24, 0.06186044508432571), (29, 0.047208286451325736), (36, 0.046069979621768835), (45, 0.08087235171979418), (47, 0.024327916938460356), (53, 0.07695369862930257), (56, 0.07980901141908836), (61, 0.028225262619209712), (69, 0.1080672987732272), (112, 0.04557089762967087), (127, 0.09740080300544345), (241, 0.1796469603779444), (242, 0.08992259466649655), (243, 0.11166235358548782), (244, 0.15848012162533082), (245, 0.26548969682262125), ...], [(10, 0.014878436612043862), (11, 0.0690841526047578), (19, 0.022802761770881638), (24, 0.05455898049349969), (30, 0.10243474689030971), (45, 0.071326888352145), (47, 0.08582585164937677), (53, 0.045247172477722745), (56, 0.07038905541794807), (69, 0.09531198228070385), (104, 0.09322197900786375), (112, 0.08038421681121939), (131, 0.13197124641419022), (182, 0.17819993890438107), (185, 0.13080027223004068), (270, 0.18048741343046848), (271, 0.12834700317270165), (277, 0.21653463756000216), (278, 0.15765046010508932), (279, 0.3696012414193431), ...], [(10, 0.03791716652108541), (11, 0.05868616859386632), (22, 0.04892159860155609), (29, 0.1061084619891196), (36, 0.13806657118911214), (53, 0.03843693658103161), (64, 0.07493612378605678), (65, 0.06431446407942601), (91, 0.08587929261825251), (105, 0.3574063109864729), (141, 0.15156836850641137), (145, 0.07800591525639525), (213, 0.12076867033953673), (219, 0.17559286809079885), (222, 0.09905080713055688), (290, 0.17634543810717943), (291, 0.2354304202988956), (292, 0.23747359098140175), (293, 0.21838240905413028), (294, 0.2068871594723075), ...], [(10, 0.01568731468793941), (29, 0.08779964258937602), (37, 0.0889416637703053), (53, 0.04770707110604152), (61, 0.02624717137862027), (64, 0.09300905076906602), (237, 0.1379113333775812), (305, 0.30664896585612444), (306, 0.2833078792717857), (307, 0.20730843748050645), (308, 0.2233412719391391), (309, 0.38969490776368737), (310, 0.2695472861513626), (311, 0.21271630313917306), (312, 0.4204476556771751), (313, 0.20658700960294096), (314, 0.4204476556771751), (315, 0.14264507531499204)], [(10, 0.027389551203659726), (316, 0.7340881994597976), (317, 0.37735752159971553), (318, 0.5638844108421996)], [(18, 0.35789169848754704), (36, 0.13040599539512807), (52, 0.11542397475108049), (92, 0.1830990480525225), (130, 0.19234978763494665), (319, 0.37310009228736857), (320, 0.5931025045589059), (321, 0.2808020052524273), (322, 0.44859490237601146)], ...], y=id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   HPL\nName: author, Length: 13051, dtype: object, accept_sparse='csr', dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, multi_output=False, ensure_min_samples=1, ensure_min_features=1, y_numeric=False, warn_on_dtype=False, estimator=None)\n    568     y_converted : object\n    569         The converted and validated y.\n    570     \"\"\"\n    571     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n    572                     ensure_2d, allow_nd, ensure_min_samples,\n--> 573                     ensure_min_features, warn_on_dtype, estimator)\n        ensure_min_features = 1\n        warn_on_dtype = False\n        estimator = None\n    574     if multi_output:\n    575         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n    576                         dtype=None)\n    577     else:\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/sklearn/utils/validation.py in check_array(array=array([ list([(0, 0.2664714100608566), (1, 0.104...), (11075, 0.41154013028457553)])], dtype=object), accept_sparse='csr', dtype=None, order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=False, estimator=None)\n    436             if array.ndim == 1:\n    437                 raise ValueError(\n    438                     \"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\n    439                     \"Reshape your data either using array.reshape(-1, 1) if \"\n    440                     \"your data has a single feature or array.reshape(1, -1) \"\n--> 441                     \"if it contains a single sample.\".format(array))\n        array = array([ list([(0, 0.2664714100608566), (1, 0.104...), (11075, 0.41154013028457553)])], dtype=object)\n    442             array = np.atleast_2d(array)\n    443             # To ensure that array flags are maintained\n    444             array = np.array(array, dtype=dtype, order=order, copy=copy)\n    445 \n\nValueError: Expected 2D array, got 1D array instead:\narray=[ list([(0, 0.2664714100608566), (1, 0.10480362539462479), (2, 0.2604873336049293), (3, 0.2319432706761858), (4, 0.24213524135011738), (5, 0.41116785474457307), (6, 0.39218741029198784), (7, 0.32007681200061067), (8, 0.35974012618542445), (9, 0.3057814279583449), (10, 0.033103432446463785), (11, 0.07685359149299217), (12, 0.19308643986019788), (13, 0.2126248353265933)])\n list([(10, 0.024133907128526488), (11, 0.056029761946444906), (14, 0.2713402113709543), (15, 0.3420085240826053), (16, 0.5718449017195636), (17, 0.2614985439879363), (18, 0.1808821162765368), (19, 0.036987739317106506), (20, 0.42712530611518384), (21, 0.3189304655793621), (22, 0.09341436285112015), (23, 0.27476798475726655), (24, 0.088498637497393)])\n list([(1, 0.07932158066320133), (10, 0.06263658764531839), (14, 0.14084602912179758), (18, 0.09389145710463107), (19, 0.01919942563133236), (22, 0.048489097889497096), (25, 0.2722725986774012), (26, 0.2620800981425273), (27, 0.24487868329638712), (28, 0.2722725986774012), (29, 0.03505679663903942), (30, 0.08624781176709262), (31, 0.07149215764800944), (32, 0.31119614453633054), (33, 0.2729933988114348), (34, 0.1733099699406774), (35, 0.17712329620393766), (36, 0.06842298368233549), (37, 0.1420511395070622), (38, 0.14386314798381059), (39, 0.2050581341267459), (40, 0.23334905281847185), (41, 0.09227646767474229), (42, 0.21187381993571314), (43, 0.3357541677332322), (44, 0.3357541677332322), (45, 0.06005567668474601), (46, 0.10201902211900157)])\n ...,\n list([(11, 0.03483379824389391), (22, 0.05807586817427632), (29, 0.041987869208477496), (145, 0.09260247785729721), (208, 0.1530751157324031), (1037, 0.16584589431194527), (6507, 0.28722118181246425), (8178, 0.3338402944907435), (16710, 0.37272267484996136), (20408, 0.37272267484996136), (20543, 0.37272267484996136), (23339, 0.4021360601237881), (23340, 0.4021360601237881)])\n list([(19, 0.028810064274408482), (24, 0.06893234032603549), (49, 0.13050056185493572), (60, 0.2370797936612917), (65, 0.19131019122706705), (69, 0.12042156837049621), (154, 0.19044775173097933), (237, 0.16525911588786907), (1154, 0.18801448431613588), (3699, 0.28355351691441877), (8329, 0.4085638403294151), (8330, 0.35641827923263075), (11702, 0.44541485037215933), (15323, 0.44541485037215933)])\n list([(11, 0.04032298873683362), (19, 0.02661899933307242), (22, 0.13445519560495678), (24, 0.06368989335423944), (29, 0.048604414750374345), (56, 0.08216926695338246), (61, 0.029059990817433712), (65, 0.08838032786353116), (103, 0.09486488773052526), (104, 0.10882347594433836), (178, 0.14491451149923437), (230, 0.10170684012562291), (360, 0.15235494384947212), (395, 0.363360346899261), (988, 0.17705848089319678), (1512, 0.2984337453903016), (3656, 0.39740876145355286), (3947, 0.3699196063748875), (8512, 0.25681322148061364), (10438, 0.3159541460973265), (11075, 0.41154013028457553)])].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n___________________________________________________________________________",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/bob/.local/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 350, in __call__\n    return self.func(*args, **kwargs)\n  File \"/home/bob/.local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 131, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/home/bob/.local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 131, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/home/bob/.local/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 458, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/bob/.local/lib/python3.6/site-packages/sklearn/pipeline.py\", line 250, in fit\n    self._final_estimator.fit(Xt, y, **fit_params)\n  File \"/home/bob/.local/lib/python3.6/site-packages/sklearn/naive_bayes.py\", line 579, in fit\n    X, y = check_X_y(X, y, 'csr')\n  File \"/home/bob/.local/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 573, in check_X_y\n    ensure_min_features, warn_on_dtype, estimator)\n  File \"/home/bob/.local/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 441, in check_array\n    \"if it contains a single sample.\".format(array))\nValueError: Expected 2D array, got 1D array instead:\narray=[ list([(0, 0.2664714100608566), (1, 0.10480362539462479), (2, 0.2604873336049293), (3, 0.2319432706761858), (4, 0.24213524135011738), (5, 0.41116785474457307), (6, 0.39218741029198784), (7, 0.32007681200061067), (8, 0.35974012618542445), (9, 0.3057814279583449), (10, 0.033103432446463785), (11, 0.07685359149299217), (12, 0.19308643986019788), (13, 0.2126248353265933)])\n list([(10, 0.024133907128526488), (11, 0.056029761946444906), (14, 0.2713402113709543), (15, 0.3420085240826053), (16, 0.5718449017195636), (17, 0.2614985439879363), (18, 0.1808821162765368), (19, 0.036987739317106506), (20, 0.42712530611518384), (21, 0.3189304655793621), (22, 0.09341436285112015), (23, 0.27476798475726655), (24, 0.088498637497393)])\n list([(1, 0.07932158066320133), (10, 0.06263658764531839), (14, 0.14084602912179758), (18, 0.09389145710463107), (19, 0.01919942563133236), (22, 0.048489097889497096), (25, 0.2722725986774012), (26, 0.2620800981425273), (27, 0.24487868329638712), (28, 0.2722725986774012), (29, 0.03505679663903942), (30, 0.08624781176709262), (31, 0.07149215764800944), (32, 0.31119614453633054), (33, 0.2729933988114348), (34, 0.1733099699406774), (35, 0.17712329620393766), (36, 0.06842298368233549), (37, 0.1420511395070622), (38, 0.14386314798381059), (39, 0.2050581341267459), (40, 0.23334905281847185), (41, 0.09227646767474229), (42, 0.21187381993571314), (43, 0.3357541677332322), (44, 0.3357541677332322), (45, 0.06005567668474601), (46, 0.10201902211900157)])\n ...,\n list([(11, 0.03483379824389391), (22, 0.05807586817427632), (29, 0.041987869208477496), (145, 0.09260247785729721), (208, 0.1530751157324031), (1037, 0.16584589431194527), (6507, 0.28722118181246425), (8178, 0.3338402944907435), (16710, 0.37272267484996136), (20408, 0.37272267484996136), (20543, 0.37272267484996136), (23339, 0.4021360601237881), (23340, 0.4021360601237881)])\n list([(19, 0.028810064274408482), (24, 0.06893234032603549), (49, 0.13050056185493572), (60, 0.2370797936612917), (65, 0.19131019122706705), (69, 0.12042156837049621), (154, 0.19044775173097933), (237, 0.16525911588786907), (1154, 0.18801448431613588), (3699, 0.28355351691441877), (8329, 0.4085638403294151), (8330, 0.35641827923263075), (11702, 0.44541485037215933), (15323, 0.44541485037215933)])\n list([(11, 0.04032298873683362), (19, 0.02661899933307242), (22, 0.13445519560495678), (24, 0.06368989335423944), (29, 0.048604414750374345), (56, 0.08216926695338246), (61, 0.029059990817433712), (65, 0.08838032786353116), (103, 0.09486488773052526), (104, 0.10882347594433836), (178, 0.14491451149923437), (230, 0.10170684012562291), (360, 0.15235494384947212), (395, 0.363360346899261), (988, 0.17705848089319678), (1512, 0.2984337453903016), (3656, 0.39740876145355286), (3947, 0.3699196063748875), (8512, 0.25681322148061364), (10438, 0.3159541460973265), (11075, 0.41154013028457553)])].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/bob/.local/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 359, in __call__\n    raise TransportableException(text, e_type)\nsklearn.externals.joblib.my_exceptions.TransportableException: TransportableException\n___________________________________________________________________________\nValueError                                         Wed Jan  3 06:40:41 2018\nPID: 18886                                   Python 3.6.3: /usr/bin/python3\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(memory=None,\n     steps=[('bow', Text2B...B(alpha=1.0, class_prior=None, fit_prior=True))]), id\nid26305    This process, however, afforded me...d it ...\nName: text, Length: 19579, dtype: object, id\nid26305    EAP\nid17569    HPL\nid11008    EAP\n...   HPL\nName: author, Length: 19579, dtype: object, {'score': make_scorer(log_loss, greater_is_better=False, needs_proba=True)}, array([ 6475,  6476,  6478, ..., 19576, 19577, 19578]), array([   0,    1,    2, ..., 6604, 6605, 6606]), 0, None, None), {'return_times': True, 'return_train_score': False})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(memory=None,\n     steps=[('bow', Text2B...B(alpha=1.0, class_prior=None, fit_prior=True))]), id\nid26305    This process, however, afforded me...d it ...\nName: text, Length: 19579, dtype: object, id\nid26305    EAP\nid17569    HPL\nid11008    EAP\n...   HPL\nName: author, Length: 19579, dtype: object, {'score': make_scorer(log_loss, greater_is_better=False, needs_proba=True)}, array([ 6475,  6476,  6478, ..., 19576, 19577, 19578]), array([   0,    1,    2, ..., 6604, 6605, 6606]), 0, None, None)\n        kwargs = {'return_times': True, 'return_train_score': False}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(memory=None,\n     steps=[('bow', Text2B...B(alpha=1.0, class_prior=None, fit_prior=True))]), X=id\nid26305    This process, however, afforded me...d it ...\nName: text, Length: 19579, dtype: object, y=id\nid26305    EAP\nid17569    HPL\nid11008    EAP\n...   HPL\nName: author, Length: 19579, dtype: object, scorer={'score': make_scorer(log_loss, greater_is_better=False, needs_proba=True)}, train=array([ 6475,  6476,  6478, ..., 19576, 19577, 19578]), test=array([   0,    1,    2, ..., 6604, 6605, 6606]), verbose=0, parameters=None, fit_params={}, return_train_score=False, return_parameters=False, return_n_test_samples=False, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(memory=No...(alpha=1.0, class_prior=None, fit_prior=True))])>\n        X_train = id\nid15739    Its close resemblance to the medic...d it ...\nName: text, Length: 13051, dtype: object\n        y_train = id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   HPL\nName: author, Length: 13051, dtype: object\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/sklearn/pipeline.py in fit(self=Pipeline(memory=None,\n     steps=[('bow', Text2B...B(alpha=1.0, class_prior=None, fit_prior=True))]), X=id\nid15739    Its close resemblance to the medic...d it ...\nName: text, Length: 13051, dtype: object, y=id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   HPL\nName: author, Length: 13051, dtype: object, **fit_params={})\n    245         self : Pipeline\n    246             This estimator\n    247         \"\"\"\n    248         Xt, fit_params = self._fit(X, y, **fit_params)\n    249         if self._final_estimator is not None:\n--> 250             self._final_estimator.fit(Xt, y, **fit_params)\n        self._final_estimator.fit = <bound method BaseDiscreteNB.fit of MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)>\n        Xt = [[(0, 0.2664714100608566), (1, 0.10480362539462479), (2, 0.2604873336049293), (3, 0.2319432706761858), (4, 0.24213524135011738), (5, 0.41116785474457307), (6, 0.39218741029198784), (7, 0.32007681200061067), (8, 0.35974012618542445), (9, 0.3057814279583449), (10, 0.033103432446463785), (11, 0.07685359149299217), (12, 0.19308643986019788), (13, 0.2126248353265933)], [(10, 0.024133907128526488), (11, 0.056029761946444906), (14, 0.2713402113709543), (15, 0.3420085240826053), (16, 0.5718449017195636), (17, 0.2614985439879363), (18, 0.1808821162765368), (19, 0.036987739317106506), (20, 0.42712530611518384), (21, 0.3189304655793621), (22, 0.09341436285112015), (23, 0.27476798475726655), (24, 0.088498637497393)], [(1, 0.07932158066320133), (10, 0.06263658764531839), (14, 0.14084602912179758), (18, 0.09389145710463107), (19, 0.01919942563133236), (22, 0.048489097889497096), (25, 0.2722725986774012), (26, 0.2620800981425273), (27, 0.24487868329638712), (28, 0.2722725986774012), (29, 0.03505679663903942), (30, 0.08624781176709262), (31, 0.07149215764800944), (32, 0.31119614453633054), (33, 0.2729933988114348), (34, 0.1733099699406774), (35, 0.17712329620393766), (36, 0.06842298368233549), (37, 0.1420511395070622), (38, 0.14386314798381059), ...], [(10, 0.015973364734577944), (19, 0.0489617074991789), (23, 0.18185904232327854), (30, 0.1099730849576353), (47, 0.046070957177769906), (48, 0.2975393193706599), (49, 0.11089059498724518), (50, 0.3968007922798119), (51, 0.19827784646150795), (52, 0.07722160044388235), (53, 0.04857698480295758), (54, 0.42811430060066746), (55, 0.3236593651896635), (56, 0.07556909941755309), (57, 0.24915443219781466), (58, 0.42811430060066746), (59, 0.3240930775601281)], [(10, 0.03745602951273969), (36, 0.10229058576225049), (47, 0.10803203710707177), (60, 0.23619581984220353), (61, 0.03133470722492), (62, 0.385564388719468), (63, 0.3015083410363086), (64, 0.11103716027435812), (65, 0.1905968735808948), (66, 0.24536394387695537), (67, 0.5308001436909793), (68, 0.3703268648059433), (69, 0.23994513095105036), (70, 0.3078975881412962)], [(47, 0.07100101906724518), (53, 0.07486311627769447), (61, 0.04118771069619043), (71, 0.15202575486829917), (72, 0.14256352538466183), (73, 0.4667449719251434), (74, 0.2503651118140704), (75, 0.37022902619505615), (76, 0.5477253203876611), (77, 0.4760416328690717)], [(10, 0.06462580383426865), (11, 0.06430138284840814), (18, 0.06919519082403108), (22, 0.03573501237295617), (31, 0.10537515645034605), (37, 0.15703115077885102), (46, 0.07518496273136628), (64, 0.0547374449573064), (65, 0.04697880354149405), (78, 0.1734637042645781), (79, 0.21124370752304877), (80, 0.1237755155749664), (81, 0.15694812556791132), (82, 0.14377383733539228), (83, 0.21875527504189865), (84, 0.24744076215980707), (85, 0.10989353196818279), (86, 0.20541728310317606), (87, 0.1788334923026323), (88, 0.3009539216891794), ...], [(10, 0.01715833256303342), (19, 0.026296941004117013), (22, 0.06641422358292039), (24, 0.06291932116373902), (29, 0.0480163589636599), (30, 0.11813132649510569), (47, 0.049488684312218235), (51, 0.21298688698342833), (53, 0.052180619049802826), (56, 0.08117511625389999), (61, 0.028708399385876993), (102, 0.23403001292583833), (113, 0.20423929132245422), (115, 0.4262370557703321), (116, 0.2690731650842324), (117, 0.3589641175494357), (118, 0.17450791755009787), (119, 0.2001630105211242), (120, 0.303670599600059), (121, 0.4262370557703321), ...], [(10, 0.021322122227474257), (11, 0.04950186582869004), (30, 0.14679809784366432), (31, 0.12168323506929181), (47, 0.12299607457840946), (52, 0.20615924454556003), (61, 0.035675028352090576), (91, 0.14487860845275446), (98, 0.27840822828588596), (103, 0.23291869434274284), (124, 0.13839751535552838), (125, 0.3007913272538198), (126, 0.39081820673094264), (127, 0.2462174723129556), (128, 0.4123268436706392), (129, 0.22158417220596172), (130, 0.3435567610008015), (131, 0.18912652719692835), (132, 0.2306072899995085)], [(10, 0.060204206982548865), (19, 0.055361550083157234), (22, 0.09321221974181244), (49, 0.08359015703908927), (53, 0.036617693219445066), (61, 0.04029217669993592), (104, 0.0754428098308278), (133, 0.26790813350704007), (134, 0.18687533762152778), (135, 0.24430377753540683), (136, 0.23809501978731895), (137, 0.2755070388560312), (138, 0.21903792681162104), (139, 0.12747319308243932), (140, 0.12362178362577428), (141, 0.14439454632496657), (142, 0.22244615763028036), (143, 0.21903792681162104), (144, 0.23809501978731895), (145, 0.07431384829899074), ...], [(10, 0.017868951261395715), (11, 0.08296982996385277), (61, 0.02989736837972952), (72, 0.10348412582038423), (155, 0.31426513521849647), (156, 0.1699525813091037), (157, 0.1701766922652076), (158, 0.27640100939491724), (159, 0.2978192836150871), (160, 0.24276889717607655), (161, 0.33880121349699616), (162, 0.23450847942150704), (163, 0.3625537576651779), (164, 0.3183102485560416), (165, 0.24137147345766863), (166, 0.3738307494342447)], [(10, 0.04520429241093692), (11, 0.02623679756093562), (19, 0.017320077669206606), (22, 0.08748542354155527), (29, 0.06325047970506475), (53, 0.034367966016593844), (57, 0.08813773738110729), (61, 0.018908347820537255), (167, 0.10458059386108247), (168, 0.19113441661345087), (169, 0.3028886579507455), (170, 0.25858036692787534), (171, 0.13969408373222703), (172, 0.16532352454213142), (173, 0.16532352454213142), (174, 0.3028886579507455), (175, 0.19313267199821985), (176, 0.17097852648678477), (177, 0.2807345124393104), (178, 0.09429094471794976), ...], [(10, 0.009856156567640691), (15, 0.13967442332789326), (19, 0.04531677542111343), (24, 0.10842708839707535), (29, 0.11032697962119518), (41, 0.07260064341795866), (49, 0.06842359667072355), (56, 0.04662892808818644), (57, 0.1537374957667718), (61, 0.016490791171816524), (165, 0.13313568315005253), (190, 0.16755438923335436), (191, 0.13904086957076212), (192, 0.2448407583411407), (193, 0.12430484550933028), (194, 0.18687598151030094), (195, 0.15050032010750405), (196, 0.09143544591163232), (197, 0.12269104128755341), (198, 0.2641623506180873), ...], [(10, 0.05128666371555671), (11, 0.02381362904652451), (19, 0.047161308888427914), (22, 0.07940547692065152), (24, 0.037613469186254155), (30, 0.07061946834251158), (47, 0.05916914130722203), (61, 0.03432403515216796), (65, 0.05219494905222182), (66, 0.13438581975978053), (71, 0.06334580468218413), (72, 0.05940310075515709), (103, 0.05602454869351825), (111, 0.08056360687311669), (124, 0.0665782397585325), (131, 0.09098220614776903), (217, 0.13014327126494443), (218, 0.14707666165679775), (219, 0.1425038820600952), (220, 0.27491457850702283), ...], [(10, 0.03373914624704608), (11, 0.03916473868965497), (19, 0.05170877386415411), (22, 0.03264826570583768), (24, 0.06186044508432571), (29, 0.047208286451325736), (36, 0.046069979621768835), (45, 0.08087235171979418), (47, 0.024327916938460356), (53, 0.07695369862930257), (56, 0.07980901141908836), (61, 0.028225262619209712), (69, 0.1080672987732272), (112, 0.04557089762967087), (127, 0.09740080300544345), (241, 0.1796469603779444), (242, 0.08992259466649655), (243, 0.11166235358548782), (244, 0.15848012162533082), (245, 0.26548969682262125), ...], [(10, 0.014878436612043862), (11, 0.0690841526047578), (19, 0.022802761770881638), (24, 0.05455898049349969), (30, 0.10243474689030971), (45, 0.071326888352145), (47, 0.08582585164937677), (53, 0.045247172477722745), (56, 0.07038905541794807), (69, 0.09531198228070385), (104, 0.09322197900786375), (112, 0.08038421681121939), (131, 0.13197124641419022), (182, 0.17819993890438107), (185, 0.13080027223004068), (270, 0.18048741343046848), (271, 0.12834700317270165), (277, 0.21653463756000216), (278, 0.15765046010508932), (279, 0.3696012414193431), ...], [(10, 0.03791716652108541), (11, 0.05868616859386632), (22, 0.04892159860155609), (29, 0.1061084619891196), (36, 0.13806657118911214), (53, 0.03843693658103161), (64, 0.07493612378605678), (65, 0.06431446407942601), (91, 0.08587929261825251), (105, 0.3574063109864729), (141, 0.15156836850641137), (145, 0.07800591525639525), (213, 0.12076867033953673), (219, 0.17559286809079885), (222, 0.09905080713055688), (290, 0.17634543810717943), (291, 0.2354304202988956), (292, 0.23747359098140175), (293, 0.21838240905413028), (294, 0.2068871594723075), ...], [(10, 0.01568731468793941), (29, 0.08779964258937602), (37, 0.0889416637703053), (53, 0.04770707110604152), (61, 0.02624717137862027), (64, 0.09300905076906602), (237, 0.1379113333775812), (305, 0.30664896585612444), (306, 0.2833078792717857), (307, 0.20730843748050645), (308, 0.2233412719391391), (309, 0.38969490776368737), (310, 0.2695472861513626), (311, 0.21271630313917306), (312, 0.4204476556771751), (313, 0.20658700960294096), (314, 0.4204476556771751), (315, 0.14264507531499204)], [(10, 0.027389551203659726), (316, 0.7340881994597976), (317, 0.37735752159971553), (318, 0.5638844108421996)], [(18, 0.35789169848754704), (36, 0.13040599539512807), (52, 0.11542397475108049), (92, 0.1830990480525225), (130, 0.19234978763494665), (319, 0.37310009228736857), (320, 0.5931025045589059), (321, 0.2808020052524273), (322, 0.44859490237601146)], ...]\n        y = id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   HPL\nName: author, Length: 13051, dtype: object\n        fit_params = {}\n    251         return self\n    252 \n    253     def fit_transform(self, X, y=None, **fit_params):\n    254         \"\"\"Fit the model and transform with the final estimator\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/sklearn/naive_bayes.py in fit(self=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), X=[[(0, 0.2664714100608566), (1, 0.10480362539462479), (2, 0.2604873336049293), (3, 0.2319432706761858), (4, 0.24213524135011738), (5, 0.41116785474457307), (6, 0.39218741029198784), (7, 0.32007681200061067), (8, 0.35974012618542445), (9, 0.3057814279583449), (10, 0.033103432446463785), (11, 0.07685359149299217), (12, 0.19308643986019788), (13, 0.2126248353265933)], [(10, 0.024133907128526488), (11, 0.056029761946444906), (14, 0.2713402113709543), (15, 0.3420085240826053), (16, 0.5718449017195636), (17, 0.2614985439879363), (18, 0.1808821162765368), (19, 0.036987739317106506), (20, 0.42712530611518384), (21, 0.3189304655793621), (22, 0.09341436285112015), (23, 0.27476798475726655), (24, 0.088498637497393)], [(1, 0.07932158066320133), (10, 0.06263658764531839), (14, 0.14084602912179758), (18, 0.09389145710463107), (19, 0.01919942563133236), (22, 0.048489097889497096), (25, 0.2722725986774012), (26, 0.2620800981425273), (27, 0.24487868329638712), (28, 0.2722725986774012), (29, 0.03505679663903942), (30, 0.08624781176709262), (31, 0.07149215764800944), (32, 0.31119614453633054), (33, 0.2729933988114348), (34, 0.1733099699406774), (35, 0.17712329620393766), (36, 0.06842298368233549), (37, 0.1420511395070622), (38, 0.14386314798381059), ...], [(10, 0.015973364734577944), (19, 0.0489617074991789), (23, 0.18185904232327854), (30, 0.1099730849576353), (47, 0.046070957177769906), (48, 0.2975393193706599), (49, 0.11089059498724518), (50, 0.3968007922798119), (51, 0.19827784646150795), (52, 0.07722160044388235), (53, 0.04857698480295758), (54, 0.42811430060066746), (55, 0.3236593651896635), (56, 0.07556909941755309), (57, 0.24915443219781466), (58, 0.42811430060066746), (59, 0.3240930775601281)], [(10, 0.03745602951273969), (36, 0.10229058576225049), (47, 0.10803203710707177), (60, 0.23619581984220353), (61, 0.03133470722492), (62, 0.385564388719468), (63, 0.3015083410363086), (64, 0.11103716027435812), (65, 0.1905968735808948), (66, 0.24536394387695537), (67, 0.5308001436909793), (68, 0.3703268648059433), (69, 0.23994513095105036), (70, 0.3078975881412962)], [(47, 0.07100101906724518), (53, 0.07486311627769447), (61, 0.04118771069619043), (71, 0.15202575486829917), (72, 0.14256352538466183), (73, 0.4667449719251434), (74, 0.2503651118140704), (75, 0.37022902619505615), (76, 0.5477253203876611), (77, 0.4760416328690717)], [(10, 0.06462580383426865), (11, 0.06430138284840814), (18, 0.06919519082403108), (22, 0.03573501237295617), (31, 0.10537515645034605), (37, 0.15703115077885102), (46, 0.07518496273136628), (64, 0.0547374449573064), (65, 0.04697880354149405), (78, 0.1734637042645781), (79, 0.21124370752304877), (80, 0.1237755155749664), (81, 0.15694812556791132), (82, 0.14377383733539228), (83, 0.21875527504189865), (84, 0.24744076215980707), (85, 0.10989353196818279), (86, 0.20541728310317606), (87, 0.1788334923026323), (88, 0.3009539216891794), ...], [(10, 0.01715833256303342), (19, 0.026296941004117013), (22, 0.06641422358292039), (24, 0.06291932116373902), (29, 0.0480163589636599), (30, 0.11813132649510569), (47, 0.049488684312218235), (51, 0.21298688698342833), (53, 0.052180619049802826), (56, 0.08117511625389999), (61, 0.028708399385876993), (102, 0.23403001292583833), (113, 0.20423929132245422), (115, 0.4262370557703321), (116, 0.2690731650842324), (117, 0.3589641175494357), (118, 0.17450791755009787), (119, 0.2001630105211242), (120, 0.303670599600059), (121, 0.4262370557703321), ...], [(10, 0.021322122227474257), (11, 0.04950186582869004), (30, 0.14679809784366432), (31, 0.12168323506929181), (47, 0.12299607457840946), (52, 0.20615924454556003), (61, 0.035675028352090576), (91, 0.14487860845275446), (98, 0.27840822828588596), (103, 0.23291869434274284), (124, 0.13839751535552838), (125, 0.3007913272538198), (126, 0.39081820673094264), (127, 0.2462174723129556), (128, 0.4123268436706392), (129, 0.22158417220596172), (130, 0.3435567610008015), (131, 0.18912652719692835), (132, 0.2306072899995085)], [(10, 0.060204206982548865), (19, 0.055361550083157234), (22, 0.09321221974181244), (49, 0.08359015703908927), (53, 0.036617693219445066), (61, 0.04029217669993592), (104, 0.0754428098308278), (133, 0.26790813350704007), (134, 0.18687533762152778), (135, 0.24430377753540683), (136, 0.23809501978731895), (137, 0.2755070388560312), (138, 0.21903792681162104), (139, 0.12747319308243932), (140, 0.12362178362577428), (141, 0.14439454632496657), (142, 0.22244615763028036), (143, 0.21903792681162104), (144, 0.23809501978731895), (145, 0.07431384829899074), ...], [(10, 0.017868951261395715), (11, 0.08296982996385277), (61, 0.02989736837972952), (72, 0.10348412582038423), (155, 0.31426513521849647), (156, 0.1699525813091037), (157, 0.1701766922652076), (158, 0.27640100939491724), (159, 0.2978192836150871), (160, 0.24276889717607655), (161, 0.33880121349699616), (162, 0.23450847942150704), (163, 0.3625537576651779), (164, 0.3183102485560416), (165, 0.24137147345766863), (166, 0.3738307494342447)], [(10, 0.04520429241093692), (11, 0.02623679756093562), (19, 0.017320077669206606), (22, 0.08748542354155527), (29, 0.06325047970506475), (53, 0.034367966016593844), (57, 0.08813773738110729), (61, 0.018908347820537255), (167, 0.10458059386108247), (168, 0.19113441661345087), (169, 0.3028886579507455), (170, 0.25858036692787534), (171, 0.13969408373222703), (172, 0.16532352454213142), (173, 0.16532352454213142), (174, 0.3028886579507455), (175, 0.19313267199821985), (176, 0.17097852648678477), (177, 0.2807345124393104), (178, 0.09429094471794976), ...], [(10, 0.009856156567640691), (15, 0.13967442332789326), (19, 0.04531677542111343), (24, 0.10842708839707535), (29, 0.11032697962119518), (41, 0.07260064341795866), (49, 0.06842359667072355), (56, 0.04662892808818644), (57, 0.1537374957667718), (61, 0.016490791171816524), (165, 0.13313568315005253), (190, 0.16755438923335436), (191, 0.13904086957076212), (192, 0.2448407583411407), (193, 0.12430484550933028), (194, 0.18687598151030094), (195, 0.15050032010750405), (196, 0.09143544591163232), (197, 0.12269104128755341), (198, 0.2641623506180873), ...], [(10, 0.05128666371555671), (11, 0.02381362904652451), (19, 0.047161308888427914), (22, 0.07940547692065152), (24, 0.037613469186254155), (30, 0.07061946834251158), (47, 0.05916914130722203), (61, 0.03432403515216796), (65, 0.05219494905222182), (66, 0.13438581975978053), (71, 0.06334580468218413), (72, 0.05940310075515709), (103, 0.05602454869351825), (111, 0.08056360687311669), (124, 0.0665782397585325), (131, 0.09098220614776903), (217, 0.13014327126494443), (218, 0.14707666165679775), (219, 0.1425038820600952), (220, 0.27491457850702283), ...], [(10, 0.03373914624704608), (11, 0.03916473868965497), (19, 0.05170877386415411), (22, 0.03264826570583768), (24, 0.06186044508432571), (29, 0.047208286451325736), (36, 0.046069979621768835), (45, 0.08087235171979418), (47, 0.024327916938460356), (53, 0.07695369862930257), (56, 0.07980901141908836), (61, 0.028225262619209712), (69, 0.1080672987732272), (112, 0.04557089762967087), (127, 0.09740080300544345), (241, 0.1796469603779444), (242, 0.08992259466649655), (243, 0.11166235358548782), (244, 0.15848012162533082), (245, 0.26548969682262125), ...], [(10, 0.014878436612043862), (11, 0.0690841526047578), (19, 0.022802761770881638), (24, 0.05455898049349969), (30, 0.10243474689030971), (45, 0.071326888352145), (47, 0.08582585164937677), (53, 0.045247172477722745), (56, 0.07038905541794807), (69, 0.09531198228070385), (104, 0.09322197900786375), (112, 0.08038421681121939), (131, 0.13197124641419022), (182, 0.17819993890438107), (185, 0.13080027223004068), (270, 0.18048741343046848), (271, 0.12834700317270165), (277, 0.21653463756000216), (278, 0.15765046010508932), (279, 0.3696012414193431), ...], [(10, 0.03791716652108541), (11, 0.05868616859386632), (22, 0.04892159860155609), (29, 0.1061084619891196), (36, 0.13806657118911214), (53, 0.03843693658103161), (64, 0.07493612378605678), (65, 0.06431446407942601), (91, 0.08587929261825251), (105, 0.3574063109864729), (141, 0.15156836850641137), (145, 0.07800591525639525), (213, 0.12076867033953673), (219, 0.17559286809079885), (222, 0.09905080713055688), (290, 0.17634543810717943), (291, 0.2354304202988956), (292, 0.23747359098140175), (293, 0.21838240905413028), (294, 0.2068871594723075), ...], [(10, 0.01568731468793941), (29, 0.08779964258937602), (37, 0.0889416637703053), (53, 0.04770707110604152), (61, 0.02624717137862027), (64, 0.09300905076906602), (237, 0.1379113333775812), (305, 0.30664896585612444), (306, 0.2833078792717857), (307, 0.20730843748050645), (308, 0.2233412719391391), (309, 0.38969490776368737), (310, 0.2695472861513626), (311, 0.21271630313917306), (312, 0.4204476556771751), (313, 0.20658700960294096), (314, 0.4204476556771751), (315, 0.14264507531499204)], [(10, 0.027389551203659726), (316, 0.7340881994597976), (317, 0.37735752159971553), (318, 0.5638844108421996)], [(18, 0.35789169848754704), (36, 0.13040599539512807), (52, 0.11542397475108049), (92, 0.1830990480525225), (130, 0.19234978763494665), (319, 0.37310009228736857), (320, 0.5931025045589059), (321, 0.2808020052524273), (322, 0.44859490237601146)], ...], y=id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   HPL\nName: author, Length: 13051, dtype: object, sample_weight=None)\n    574         Returns\n    575         -------\n    576         self : object\n    577             Returns self.\n    578         \"\"\"\n--> 579         X, y = check_X_y(X, y, 'csr')\n        X = [[(0, 0.2664714100608566), (1, 0.10480362539462479), (2, 0.2604873336049293), (3, 0.2319432706761858), (4, 0.24213524135011738), (5, 0.41116785474457307), (6, 0.39218741029198784), (7, 0.32007681200061067), (8, 0.35974012618542445), (9, 0.3057814279583449), (10, 0.033103432446463785), (11, 0.07685359149299217), (12, 0.19308643986019788), (13, 0.2126248353265933)], [(10, 0.024133907128526488), (11, 0.056029761946444906), (14, 0.2713402113709543), (15, 0.3420085240826053), (16, 0.5718449017195636), (17, 0.2614985439879363), (18, 0.1808821162765368), (19, 0.036987739317106506), (20, 0.42712530611518384), (21, 0.3189304655793621), (22, 0.09341436285112015), (23, 0.27476798475726655), (24, 0.088498637497393)], [(1, 0.07932158066320133), (10, 0.06263658764531839), (14, 0.14084602912179758), (18, 0.09389145710463107), (19, 0.01919942563133236), (22, 0.048489097889497096), (25, 0.2722725986774012), (26, 0.2620800981425273), (27, 0.24487868329638712), (28, 0.2722725986774012), (29, 0.03505679663903942), (30, 0.08624781176709262), (31, 0.07149215764800944), (32, 0.31119614453633054), (33, 0.2729933988114348), (34, 0.1733099699406774), (35, 0.17712329620393766), (36, 0.06842298368233549), (37, 0.1420511395070622), (38, 0.14386314798381059), ...], [(10, 0.015973364734577944), (19, 0.0489617074991789), (23, 0.18185904232327854), (30, 0.1099730849576353), (47, 0.046070957177769906), (48, 0.2975393193706599), (49, 0.11089059498724518), (50, 0.3968007922798119), (51, 0.19827784646150795), (52, 0.07722160044388235), (53, 0.04857698480295758), (54, 0.42811430060066746), (55, 0.3236593651896635), (56, 0.07556909941755309), (57, 0.24915443219781466), (58, 0.42811430060066746), (59, 0.3240930775601281)], [(10, 0.03745602951273969), (36, 0.10229058576225049), (47, 0.10803203710707177), (60, 0.23619581984220353), (61, 0.03133470722492), (62, 0.385564388719468), (63, 0.3015083410363086), (64, 0.11103716027435812), (65, 0.1905968735808948), (66, 0.24536394387695537), (67, 0.5308001436909793), (68, 0.3703268648059433), (69, 0.23994513095105036), (70, 0.3078975881412962)], [(47, 0.07100101906724518), (53, 0.07486311627769447), (61, 0.04118771069619043), (71, 0.15202575486829917), (72, 0.14256352538466183), (73, 0.4667449719251434), (74, 0.2503651118140704), (75, 0.37022902619505615), (76, 0.5477253203876611), (77, 0.4760416328690717)], [(10, 0.06462580383426865), (11, 0.06430138284840814), (18, 0.06919519082403108), (22, 0.03573501237295617), (31, 0.10537515645034605), (37, 0.15703115077885102), (46, 0.07518496273136628), (64, 0.0547374449573064), (65, 0.04697880354149405), (78, 0.1734637042645781), (79, 0.21124370752304877), (80, 0.1237755155749664), (81, 0.15694812556791132), (82, 0.14377383733539228), (83, 0.21875527504189865), (84, 0.24744076215980707), (85, 0.10989353196818279), (86, 0.20541728310317606), (87, 0.1788334923026323), (88, 0.3009539216891794), ...], [(10, 0.01715833256303342), (19, 0.026296941004117013), (22, 0.06641422358292039), (24, 0.06291932116373902), (29, 0.0480163589636599), (30, 0.11813132649510569), (47, 0.049488684312218235), (51, 0.21298688698342833), (53, 0.052180619049802826), (56, 0.08117511625389999), (61, 0.028708399385876993), (102, 0.23403001292583833), (113, 0.20423929132245422), (115, 0.4262370557703321), (116, 0.2690731650842324), (117, 0.3589641175494357), (118, 0.17450791755009787), (119, 0.2001630105211242), (120, 0.303670599600059), (121, 0.4262370557703321), ...], [(10, 0.021322122227474257), (11, 0.04950186582869004), (30, 0.14679809784366432), (31, 0.12168323506929181), (47, 0.12299607457840946), (52, 0.20615924454556003), (61, 0.035675028352090576), (91, 0.14487860845275446), (98, 0.27840822828588596), (103, 0.23291869434274284), (124, 0.13839751535552838), (125, 0.3007913272538198), (126, 0.39081820673094264), (127, 0.2462174723129556), (128, 0.4123268436706392), (129, 0.22158417220596172), (130, 0.3435567610008015), (131, 0.18912652719692835), (132, 0.2306072899995085)], [(10, 0.060204206982548865), (19, 0.055361550083157234), (22, 0.09321221974181244), (49, 0.08359015703908927), (53, 0.036617693219445066), (61, 0.04029217669993592), (104, 0.0754428098308278), (133, 0.26790813350704007), (134, 0.18687533762152778), (135, 0.24430377753540683), (136, 0.23809501978731895), (137, 0.2755070388560312), (138, 0.21903792681162104), (139, 0.12747319308243932), (140, 0.12362178362577428), (141, 0.14439454632496657), (142, 0.22244615763028036), (143, 0.21903792681162104), (144, 0.23809501978731895), (145, 0.07431384829899074), ...], [(10, 0.017868951261395715), (11, 0.08296982996385277), (61, 0.02989736837972952), (72, 0.10348412582038423), (155, 0.31426513521849647), (156, 0.1699525813091037), (157, 0.1701766922652076), (158, 0.27640100939491724), (159, 0.2978192836150871), (160, 0.24276889717607655), (161, 0.33880121349699616), (162, 0.23450847942150704), (163, 0.3625537576651779), (164, 0.3183102485560416), (165, 0.24137147345766863), (166, 0.3738307494342447)], [(10, 0.04520429241093692), (11, 0.02623679756093562), (19, 0.017320077669206606), (22, 0.08748542354155527), (29, 0.06325047970506475), (53, 0.034367966016593844), (57, 0.08813773738110729), (61, 0.018908347820537255), (167, 0.10458059386108247), (168, 0.19113441661345087), (169, 0.3028886579507455), (170, 0.25858036692787534), (171, 0.13969408373222703), (172, 0.16532352454213142), (173, 0.16532352454213142), (174, 0.3028886579507455), (175, 0.19313267199821985), (176, 0.17097852648678477), (177, 0.2807345124393104), (178, 0.09429094471794976), ...], [(10, 0.009856156567640691), (15, 0.13967442332789326), (19, 0.04531677542111343), (24, 0.10842708839707535), (29, 0.11032697962119518), (41, 0.07260064341795866), (49, 0.06842359667072355), (56, 0.04662892808818644), (57, 0.1537374957667718), (61, 0.016490791171816524), (165, 0.13313568315005253), (190, 0.16755438923335436), (191, 0.13904086957076212), (192, 0.2448407583411407), (193, 0.12430484550933028), (194, 0.18687598151030094), (195, 0.15050032010750405), (196, 0.09143544591163232), (197, 0.12269104128755341), (198, 0.2641623506180873), ...], [(10, 0.05128666371555671), (11, 0.02381362904652451), (19, 0.047161308888427914), (22, 0.07940547692065152), (24, 0.037613469186254155), (30, 0.07061946834251158), (47, 0.05916914130722203), (61, 0.03432403515216796), (65, 0.05219494905222182), (66, 0.13438581975978053), (71, 0.06334580468218413), (72, 0.05940310075515709), (103, 0.05602454869351825), (111, 0.08056360687311669), (124, 0.0665782397585325), (131, 0.09098220614776903), (217, 0.13014327126494443), (218, 0.14707666165679775), (219, 0.1425038820600952), (220, 0.27491457850702283), ...], [(10, 0.03373914624704608), (11, 0.03916473868965497), (19, 0.05170877386415411), (22, 0.03264826570583768), (24, 0.06186044508432571), (29, 0.047208286451325736), (36, 0.046069979621768835), (45, 0.08087235171979418), (47, 0.024327916938460356), (53, 0.07695369862930257), (56, 0.07980901141908836), (61, 0.028225262619209712), (69, 0.1080672987732272), (112, 0.04557089762967087), (127, 0.09740080300544345), (241, 0.1796469603779444), (242, 0.08992259466649655), (243, 0.11166235358548782), (244, 0.15848012162533082), (245, 0.26548969682262125), ...], [(10, 0.014878436612043862), (11, 0.0690841526047578), (19, 0.022802761770881638), (24, 0.05455898049349969), (30, 0.10243474689030971), (45, 0.071326888352145), (47, 0.08582585164937677), (53, 0.045247172477722745), (56, 0.07038905541794807), (69, 0.09531198228070385), (104, 0.09322197900786375), (112, 0.08038421681121939), (131, 0.13197124641419022), (182, 0.17819993890438107), (185, 0.13080027223004068), (270, 0.18048741343046848), (271, 0.12834700317270165), (277, 0.21653463756000216), (278, 0.15765046010508932), (279, 0.3696012414193431), ...], [(10, 0.03791716652108541), (11, 0.05868616859386632), (22, 0.04892159860155609), (29, 0.1061084619891196), (36, 0.13806657118911214), (53, 0.03843693658103161), (64, 0.07493612378605678), (65, 0.06431446407942601), (91, 0.08587929261825251), (105, 0.3574063109864729), (141, 0.15156836850641137), (145, 0.07800591525639525), (213, 0.12076867033953673), (219, 0.17559286809079885), (222, 0.09905080713055688), (290, 0.17634543810717943), (291, 0.2354304202988956), (292, 0.23747359098140175), (293, 0.21838240905413028), (294, 0.2068871594723075), ...], [(10, 0.01568731468793941), (29, 0.08779964258937602), (37, 0.0889416637703053), (53, 0.04770707110604152), (61, 0.02624717137862027), (64, 0.09300905076906602), (237, 0.1379113333775812), (305, 0.30664896585612444), (306, 0.2833078792717857), (307, 0.20730843748050645), (308, 0.2233412719391391), (309, 0.38969490776368737), (310, 0.2695472861513626), (311, 0.21271630313917306), (312, 0.4204476556771751), (313, 0.20658700960294096), (314, 0.4204476556771751), (315, 0.14264507531499204)], [(10, 0.027389551203659726), (316, 0.7340881994597976), (317, 0.37735752159971553), (318, 0.5638844108421996)], [(18, 0.35789169848754704), (36, 0.13040599539512807), (52, 0.11542397475108049), (92, 0.1830990480525225), (130, 0.19234978763494665), (319, 0.37310009228736857), (320, 0.5931025045589059), (321, 0.2808020052524273), (322, 0.44859490237601146)], ...]\n        y = id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   HPL\nName: author, Length: 13051, dtype: object\n    580         _, n_features = X.shape\n    581 \n    582         labelbin = LabelBinarizer()\n    583         Y = labelbin.fit_transform(y)\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/sklearn/utils/validation.py in check_X_y(X=[[(0, 0.2664714100608566), (1, 0.10480362539462479), (2, 0.2604873336049293), (3, 0.2319432706761858), (4, 0.24213524135011738), (5, 0.41116785474457307), (6, 0.39218741029198784), (7, 0.32007681200061067), (8, 0.35974012618542445), (9, 0.3057814279583449), (10, 0.033103432446463785), (11, 0.07685359149299217), (12, 0.19308643986019788), (13, 0.2126248353265933)], [(10, 0.024133907128526488), (11, 0.056029761946444906), (14, 0.2713402113709543), (15, 0.3420085240826053), (16, 0.5718449017195636), (17, 0.2614985439879363), (18, 0.1808821162765368), (19, 0.036987739317106506), (20, 0.42712530611518384), (21, 0.3189304655793621), (22, 0.09341436285112015), (23, 0.27476798475726655), (24, 0.088498637497393)], [(1, 0.07932158066320133), (10, 0.06263658764531839), (14, 0.14084602912179758), (18, 0.09389145710463107), (19, 0.01919942563133236), (22, 0.048489097889497096), (25, 0.2722725986774012), (26, 0.2620800981425273), (27, 0.24487868329638712), (28, 0.2722725986774012), (29, 0.03505679663903942), (30, 0.08624781176709262), (31, 0.07149215764800944), (32, 0.31119614453633054), (33, 0.2729933988114348), (34, 0.1733099699406774), (35, 0.17712329620393766), (36, 0.06842298368233549), (37, 0.1420511395070622), (38, 0.14386314798381059), ...], [(10, 0.015973364734577944), (19, 0.0489617074991789), (23, 0.18185904232327854), (30, 0.1099730849576353), (47, 0.046070957177769906), (48, 0.2975393193706599), (49, 0.11089059498724518), (50, 0.3968007922798119), (51, 0.19827784646150795), (52, 0.07722160044388235), (53, 0.04857698480295758), (54, 0.42811430060066746), (55, 0.3236593651896635), (56, 0.07556909941755309), (57, 0.24915443219781466), (58, 0.42811430060066746), (59, 0.3240930775601281)], [(10, 0.03745602951273969), (36, 0.10229058576225049), (47, 0.10803203710707177), (60, 0.23619581984220353), (61, 0.03133470722492), (62, 0.385564388719468), (63, 0.3015083410363086), (64, 0.11103716027435812), (65, 0.1905968735808948), (66, 0.24536394387695537), (67, 0.5308001436909793), (68, 0.3703268648059433), (69, 0.23994513095105036), (70, 0.3078975881412962)], [(47, 0.07100101906724518), (53, 0.07486311627769447), (61, 0.04118771069619043), (71, 0.15202575486829917), (72, 0.14256352538466183), (73, 0.4667449719251434), (74, 0.2503651118140704), (75, 0.37022902619505615), (76, 0.5477253203876611), (77, 0.4760416328690717)], [(10, 0.06462580383426865), (11, 0.06430138284840814), (18, 0.06919519082403108), (22, 0.03573501237295617), (31, 0.10537515645034605), (37, 0.15703115077885102), (46, 0.07518496273136628), (64, 0.0547374449573064), (65, 0.04697880354149405), (78, 0.1734637042645781), (79, 0.21124370752304877), (80, 0.1237755155749664), (81, 0.15694812556791132), (82, 0.14377383733539228), (83, 0.21875527504189865), (84, 0.24744076215980707), (85, 0.10989353196818279), (86, 0.20541728310317606), (87, 0.1788334923026323), (88, 0.3009539216891794), ...], [(10, 0.01715833256303342), (19, 0.026296941004117013), (22, 0.06641422358292039), (24, 0.06291932116373902), (29, 0.0480163589636599), (30, 0.11813132649510569), (47, 0.049488684312218235), (51, 0.21298688698342833), (53, 0.052180619049802826), (56, 0.08117511625389999), (61, 0.028708399385876993), (102, 0.23403001292583833), (113, 0.20423929132245422), (115, 0.4262370557703321), (116, 0.2690731650842324), (117, 0.3589641175494357), (118, 0.17450791755009787), (119, 0.2001630105211242), (120, 0.303670599600059), (121, 0.4262370557703321), ...], [(10, 0.021322122227474257), (11, 0.04950186582869004), (30, 0.14679809784366432), (31, 0.12168323506929181), (47, 0.12299607457840946), (52, 0.20615924454556003), (61, 0.035675028352090576), (91, 0.14487860845275446), (98, 0.27840822828588596), (103, 0.23291869434274284), (124, 0.13839751535552838), (125, 0.3007913272538198), (126, 0.39081820673094264), (127, 0.2462174723129556), (128, 0.4123268436706392), (129, 0.22158417220596172), (130, 0.3435567610008015), (131, 0.18912652719692835), (132, 0.2306072899995085)], [(10, 0.060204206982548865), (19, 0.055361550083157234), (22, 0.09321221974181244), (49, 0.08359015703908927), (53, 0.036617693219445066), (61, 0.04029217669993592), (104, 0.0754428098308278), (133, 0.26790813350704007), (134, 0.18687533762152778), (135, 0.24430377753540683), (136, 0.23809501978731895), (137, 0.2755070388560312), (138, 0.21903792681162104), (139, 0.12747319308243932), (140, 0.12362178362577428), (141, 0.14439454632496657), (142, 0.22244615763028036), (143, 0.21903792681162104), (144, 0.23809501978731895), (145, 0.07431384829899074), ...], [(10, 0.017868951261395715), (11, 0.08296982996385277), (61, 0.02989736837972952), (72, 0.10348412582038423), (155, 0.31426513521849647), (156, 0.1699525813091037), (157, 0.1701766922652076), (158, 0.27640100939491724), (159, 0.2978192836150871), (160, 0.24276889717607655), (161, 0.33880121349699616), (162, 0.23450847942150704), (163, 0.3625537576651779), (164, 0.3183102485560416), (165, 0.24137147345766863), (166, 0.3738307494342447)], [(10, 0.04520429241093692), (11, 0.02623679756093562), (19, 0.017320077669206606), (22, 0.08748542354155527), (29, 0.06325047970506475), (53, 0.034367966016593844), (57, 0.08813773738110729), (61, 0.018908347820537255), (167, 0.10458059386108247), (168, 0.19113441661345087), (169, 0.3028886579507455), (170, 0.25858036692787534), (171, 0.13969408373222703), (172, 0.16532352454213142), (173, 0.16532352454213142), (174, 0.3028886579507455), (175, 0.19313267199821985), (176, 0.17097852648678477), (177, 0.2807345124393104), (178, 0.09429094471794976), ...], [(10, 0.009856156567640691), (15, 0.13967442332789326), (19, 0.04531677542111343), (24, 0.10842708839707535), (29, 0.11032697962119518), (41, 0.07260064341795866), (49, 0.06842359667072355), (56, 0.04662892808818644), (57, 0.1537374957667718), (61, 0.016490791171816524), (165, 0.13313568315005253), (190, 0.16755438923335436), (191, 0.13904086957076212), (192, 0.2448407583411407), (193, 0.12430484550933028), (194, 0.18687598151030094), (195, 0.15050032010750405), (196, 0.09143544591163232), (197, 0.12269104128755341), (198, 0.2641623506180873), ...], [(10, 0.05128666371555671), (11, 0.02381362904652451), (19, 0.047161308888427914), (22, 0.07940547692065152), (24, 0.037613469186254155), (30, 0.07061946834251158), (47, 0.05916914130722203), (61, 0.03432403515216796), (65, 0.05219494905222182), (66, 0.13438581975978053), (71, 0.06334580468218413), (72, 0.05940310075515709), (103, 0.05602454869351825), (111, 0.08056360687311669), (124, 0.0665782397585325), (131, 0.09098220614776903), (217, 0.13014327126494443), (218, 0.14707666165679775), (219, 0.1425038820600952), (220, 0.27491457850702283), ...], [(10, 0.03373914624704608), (11, 0.03916473868965497), (19, 0.05170877386415411), (22, 0.03264826570583768), (24, 0.06186044508432571), (29, 0.047208286451325736), (36, 0.046069979621768835), (45, 0.08087235171979418), (47, 0.024327916938460356), (53, 0.07695369862930257), (56, 0.07980901141908836), (61, 0.028225262619209712), (69, 0.1080672987732272), (112, 0.04557089762967087), (127, 0.09740080300544345), (241, 0.1796469603779444), (242, 0.08992259466649655), (243, 0.11166235358548782), (244, 0.15848012162533082), (245, 0.26548969682262125), ...], [(10, 0.014878436612043862), (11, 0.0690841526047578), (19, 0.022802761770881638), (24, 0.05455898049349969), (30, 0.10243474689030971), (45, 0.071326888352145), (47, 0.08582585164937677), (53, 0.045247172477722745), (56, 0.07038905541794807), (69, 0.09531198228070385), (104, 0.09322197900786375), (112, 0.08038421681121939), (131, 0.13197124641419022), (182, 0.17819993890438107), (185, 0.13080027223004068), (270, 0.18048741343046848), (271, 0.12834700317270165), (277, 0.21653463756000216), (278, 0.15765046010508932), (279, 0.3696012414193431), ...], [(10, 0.03791716652108541), (11, 0.05868616859386632), (22, 0.04892159860155609), (29, 0.1061084619891196), (36, 0.13806657118911214), (53, 0.03843693658103161), (64, 0.07493612378605678), (65, 0.06431446407942601), (91, 0.08587929261825251), (105, 0.3574063109864729), (141, 0.15156836850641137), (145, 0.07800591525639525), (213, 0.12076867033953673), (219, 0.17559286809079885), (222, 0.09905080713055688), (290, 0.17634543810717943), (291, 0.2354304202988956), (292, 0.23747359098140175), (293, 0.21838240905413028), (294, 0.2068871594723075), ...], [(10, 0.01568731468793941), (29, 0.08779964258937602), (37, 0.0889416637703053), (53, 0.04770707110604152), (61, 0.02624717137862027), (64, 0.09300905076906602), (237, 0.1379113333775812), (305, 0.30664896585612444), (306, 0.2833078792717857), (307, 0.20730843748050645), (308, 0.2233412719391391), (309, 0.38969490776368737), (310, 0.2695472861513626), (311, 0.21271630313917306), (312, 0.4204476556771751), (313, 0.20658700960294096), (314, 0.4204476556771751), (315, 0.14264507531499204)], [(10, 0.027389551203659726), (316, 0.7340881994597976), (317, 0.37735752159971553), (318, 0.5638844108421996)], [(18, 0.35789169848754704), (36, 0.13040599539512807), (52, 0.11542397475108049), (92, 0.1830990480525225), (130, 0.19234978763494665), (319, 0.37310009228736857), (320, 0.5931025045589059), (321, 0.2808020052524273), (322, 0.44859490237601146)], ...], y=id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   HPL\nName: author, Length: 13051, dtype: object, accept_sparse='csr', dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, multi_output=False, ensure_min_samples=1, ensure_min_features=1, y_numeric=False, warn_on_dtype=False, estimator=None)\n    568     y_converted : object\n    569         The converted and validated y.\n    570     \"\"\"\n    571     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n    572                     ensure_2d, allow_nd, ensure_min_samples,\n--> 573                     ensure_min_features, warn_on_dtype, estimator)\n        ensure_min_features = 1\n        warn_on_dtype = False\n        estimator = None\n    574     if multi_output:\n    575         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n    576                         dtype=None)\n    577     else:\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/sklearn/utils/validation.py in check_array(array=array([ list([(0, 0.2664714100608566), (1, 0.104...), (11075, 0.41154013028457553)])], dtype=object), accept_sparse='csr', dtype=None, order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=False, estimator=None)\n    436             if array.ndim == 1:\n    437                 raise ValueError(\n    438                     \"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\n    439                     \"Reshape your data either using array.reshape(-1, 1) if \"\n    440                     \"your data has a single feature or array.reshape(1, -1) \"\n--> 441                     \"if it contains a single sample.\".format(array))\n        array = array([ list([(0, 0.2664714100608566), (1, 0.104...), (11075, 0.41154013028457553)])], dtype=object)\n    442             array = np.atleast_2d(array)\n    443             # To ensure that array flags are maintained\n    444             array = np.array(array, dtype=dtype, order=order, copy=copy)\n    445 \n\nValueError: Expected 2D array, got 1D array instead:\narray=[ list([(0, 0.2664714100608566), (1, 0.10480362539462479), (2, 0.2604873336049293), (3, 0.2319432706761858), (4, 0.24213524135011738), (5, 0.41116785474457307), (6, 0.39218741029198784), (7, 0.32007681200061067), (8, 0.35974012618542445), (9, 0.3057814279583449), (10, 0.033103432446463785), (11, 0.07685359149299217), (12, 0.19308643986019788), (13, 0.2126248353265933)])\n list([(10, 0.024133907128526488), (11, 0.056029761946444906), (14, 0.2713402113709543), (15, 0.3420085240826053), (16, 0.5718449017195636), (17, 0.2614985439879363), (18, 0.1808821162765368), (19, 0.036987739317106506), (20, 0.42712530611518384), (21, 0.3189304655793621), (22, 0.09341436285112015), (23, 0.27476798475726655), (24, 0.088498637497393)])\n list([(1, 0.07932158066320133), (10, 0.06263658764531839), (14, 0.14084602912179758), (18, 0.09389145710463107), (19, 0.01919942563133236), (22, 0.048489097889497096), (25, 0.2722725986774012), (26, 0.2620800981425273), (27, 0.24487868329638712), (28, 0.2722725986774012), (29, 0.03505679663903942), (30, 0.08624781176709262), (31, 0.07149215764800944), (32, 0.31119614453633054), (33, 0.2729933988114348), (34, 0.1733099699406774), (35, 0.17712329620393766), (36, 0.06842298368233549), (37, 0.1420511395070622), (38, 0.14386314798381059), (39, 0.2050581341267459), (40, 0.23334905281847185), (41, 0.09227646767474229), (42, 0.21187381993571314), (43, 0.3357541677332322), (44, 0.3357541677332322), (45, 0.06005567668474601), (46, 0.10201902211900157)])\n ...,\n list([(11, 0.03483379824389391), (22, 0.05807586817427632), (29, 0.041987869208477496), (145, 0.09260247785729721), (208, 0.1530751157324031), (1037, 0.16584589431194527), (6507, 0.28722118181246425), (8178, 0.3338402944907435), (16710, 0.37272267484996136), (20408, 0.37272267484996136), (20543, 0.37272267484996136), (23339, 0.4021360601237881), (23340, 0.4021360601237881)])\n list([(19, 0.028810064274408482), (24, 0.06893234032603549), (49, 0.13050056185493572), (60, 0.2370797936612917), (65, 0.19131019122706705), (69, 0.12042156837049621), (154, 0.19044775173097933), (237, 0.16525911588786907), (1154, 0.18801448431613588), (3699, 0.28355351691441877), (8329, 0.4085638403294151), (8330, 0.35641827923263075), (11702, 0.44541485037215933), (15323, 0.44541485037215933)])\n list([(11, 0.04032298873683362), (19, 0.02661899933307242), (22, 0.13445519560495678), (24, 0.06368989335423944), (29, 0.048604414750374345), (56, 0.08216926695338246), (61, 0.029059990817433712), (65, 0.08838032786353116), (103, 0.09486488773052526), (104, 0.10882347594433836), (178, 0.14491451149923437), (230, 0.10170684012562291), (360, 0.15235494384947212), (395, 0.363360346899261), (988, 0.17705848089319678), (1512, 0.2984337453903016), (3656, 0.39740876145355286), (3947, 0.3699196063748875), (8512, 0.25681322148061364), (10438, 0.3159541460973265), (11075, 0.41154013028457553)])].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n___________________________________________________________________________\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTransportableException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTransportableException\u001b[0m: TransportableException\n___________________________________________________________________________\nValueError                                         Wed Jan  3 06:40:41 2018\nPID: 18886                                   Python 3.6.3: /usr/bin/python3\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(memory=None,\n     steps=[('bow', Text2B...B(alpha=1.0, class_prior=None, fit_prior=True))]), id\nid26305    This process, however, afforded me...d it ...\nName: text, Length: 19579, dtype: object, id\nid26305    EAP\nid17569    HPL\nid11008    EAP\n...   HPL\nName: author, Length: 19579, dtype: object, {'score': make_scorer(log_loss, greater_is_better=False, needs_proba=True)}, array([ 6475,  6476,  6478, ..., 19576, 19577, 19578]), array([   0,    1,    2, ..., 6604, 6605, 6606]), 0, None, None), {'return_times': True, 'return_train_score': False})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(memory=None,\n     steps=[('bow', Text2B...B(alpha=1.0, class_prior=None, fit_prior=True))]), id\nid26305    This process, however, afforded me...d it ...\nName: text, Length: 19579, dtype: object, id\nid26305    EAP\nid17569    HPL\nid11008    EAP\n...   HPL\nName: author, Length: 19579, dtype: object, {'score': make_scorer(log_loss, greater_is_better=False, needs_proba=True)}, array([ 6475,  6476,  6478, ..., 19576, 19577, 19578]), array([   0,    1,    2, ..., 6604, 6605, 6606]), 0, None, None)\n        kwargs = {'return_times': True, 'return_train_score': False}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(memory=None,\n     steps=[('bow', Text2B...B(alpha=1.0, class_prior=None, fit_prior=True))]), X=id\nid26305    This process, however, afforded me...d it ...\nName: text, Length: 19579, dtype: object, y=id\nid26305    EAP\nid17569    HPL\nid11008    EAP\n...   HPL\nName: author, Length: 19579, dtype: object, scorer={'score': make_scorer(log_loss, greater_is_better=False, needs_proba=True)}, train=array([ 6475,  6476,  6478, ..., 19576, 19577, 19578]), test=array([   0,    1,    2, ..., 6604, 6605, 6606]), verbose=0, parameters=None, fit_params={}, return_train_score=False, return_parameters=False, return_n_test_samples=False, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(memory=No...(alpha=1.0, class_prior=None, fit_prior=True))])>\n        X_train = id\nid15739    Its close resemblance to the medic...d it ...\nName: text, Length: 13051, dtype: object\n        y_train = id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   HPL\nName: author, Length: 13051, dtype: object\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/sklearn/pipeline.py in fit(self=Pipeline(memory=None,\n     steps=[('bow', Text2B...B(alpha=1.0, class_prior=None, fit_prior=True))]), X=id\nid15739    Its close resemblance to the medic...d it ...\nName: text, Length: 13051, dtype: object, y=id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   HPL\nName: author, Length: 13051, dtype: object, **fit_params={})\n    245         self : Pipeline\n    246             This estimator\n    247         \"\"\"\n    248         Xt, fit_params = self._fit(X, y, **fit_params)\n    249         if self._final_estimator is not None:\n--> 250             self._final_estimator.fit(Xt, y, **fit_params)\n        self._final_estimator.fit = <bound method BaseDiscreteNB.fit of MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)>\n        Xt = [[(0, 0.2664714100608566), (1, 0.10480362539462479), (2, 0.2604873336049293), (3, 0.2319432706761858), (4, 0.24213524135011738), (5, 0.41116785474457307), (6, 0.39218741029198784), (7, 0.32007681200061067), (8, 0.35974012618542445), (9, 0.3057814279583449), (10, 0.033103432446463785), (11, 0.07685359149299217), (12, 0.19308643986019788), (13, 0.2126248353265933)], [(10, 0.024133907128526488), (11, 0.056029761946444906), (14, 0.2713402113709543), (15, 0.3420085240826053), (16, 0.5718449017195636), (17, 0.2614985439879363), (18, 0.1808821162765368), (19, 0.036987739317106506), (20, 0.42712530611518384), (21, 0.3189304655793621), (22, 0.09341436285112015), (23, 0.27476798475726655), (24, 0.088498637497393)], [(1, 0.07932158066320133), (10, 0.06263658764531839), (14, 0.14084602912179758), (18, 0.09389145710463107), (19, 0.01919942563133236), (22, 0.048489097889497096), (25, 0.2722725986774012), (26, 0.2620800981425273), (27, 0.24487868329638712), (28, 0.2722725986774012), (29, 0.03505679663903942), (30, 0.08624781176709262), (31, 0.07149215764800944), (32, 0.31119614453633054), (33, 0.2729933988114348), (34, 0.1733099699406774), (35, 0.17712329620393766), (36, 0.06842298368233549), (37, 0.1420511395070622), (38, 0.14386314798381059), ...], [(10, 0.015973364734577944), (19, 0.0489617074991789), (23, 0.18185904232327854), (30, 0.1099730849576353), (47, 0.046070957177769906), (48, 0.2975393193706599), (49, 0.11089059498724518), (50, 0.3968007922798119), (51, 0.19827784646150795), (52, 0.07722160044388235), (53, 0.04857698480295758), (54, 0.42811430060066746), (55, 0.3236593651896635), (56, 0.07556909941755309), (57, 0.24915443219781466), (58, 0.42811430060066746), (59, 0.3240930775601281)], [(10, 0.03745602951273969), (36, 0.10229058576225049), (47, 0.10803203710707177), (60, 0.23619581984220353), (61, 0.03133470722492), (62, 0.385564388719468), (63, 0.3015083410363086), (64, 0.11103716027435812), (65, 0.1905968735808948), (66, 0.24536394387695537), (67, 0.5308001436909793), (68, 0.3703268648059433), (69, 0.23994513095105036), (70, 0.3078975881412962)], [(47, 0.07100101906724518), (53, 0.07486311627769447), (61, 0.04118771069619043), (71, 0.15202575486829917), (72, 0.14256352538466183), (73, 0.4667449719251434), (74, 0.2503651118140704), (75, 0.37022902619505615), (76, 0.5477253203876611), (77, 0.4760416328690717)], [(10, 0.06462580383426865), (11, 0.06430138284840814), (18, 0.06919519082403108), (22, 0.03573501237295617), (31, 0.10537515645034605), (37, 0.15703115077885102), (46, 0.07518496273136628), (64, 0.0547374449573064), (65, 0.04697880354149405), (78, 0.1734637042645781), (79, 0.21124370752304877), (80, 0.1237755155749664), (81, 0.15694812556791132), (82, 0.14377383733539228), (83, 0.21875527504189865), (84, 0.24744076215980707), (85, 0.10989353196818279), (86, 0.20541728310317606), (87, 0.1788334923026323), (88, 0.3009539216891794), ...], [(10, 0.01715833256303342), (19, 0.026296941004117013), (22, 0.06641422358292039), (24, 0.06291932116373902), (29, 0.0480163589636599), (30, 0.11813132649510569), (47, 0.049488684312218235), (51, 0.21298688698342833), (53, 0.052180619049802826), (56, 0.08117511625389999), (61, 0.028708399385876993), (102, 0.23403001292583833), (113, 0.20423929132245422), (115, 0.4262370557703321), (116, 0.2690731650842324), (117, 0.3589641175494357), (118, 0.17450791755009787), (119, 0.2001630105211242), (120, 0.303670599600059), (121, 0.4262370557703321), ...], [(10, 0.021322122227474257), (11, 0.04950186582869004), (30, 0.14679809784366432), (31, 0.12168323506929181), (47, 0.12299607457840946), (52, 0.20615924454556003), (61, 0.035675028352090576), (91, 0.14487860845275446), (98, 0.27840822828588596), (103, 0.23291869434274284), (124, 0.13839751535552838), (125, 0.3007913272538198), (126, 0.39081820673094264), (127, 0.2462174723129556), (128, 0.4123268436706392), (129, 0.22158417220596172), (130, 0.3435567610008015), (131, 0.18912652719692835), (132, 0.2306072899995085)], [(10, 0.060204206982548865), (19, 0.055361550083157234), (22, 0.09321221974181244), (49, 0.08359015703908927), (53, 0.036617693219445066), (61, 0.04029217669993592), (104, 0.0754428098308278), (133, 0.26790813350704007), (134, 0.18687533762152778), (135, 0.24430377753540683), (136, 0.23809501978731895), (137, 0.2755070388560312), (138, 0.21903792681162104), (139, 0.12747319308243932), (140, 0.12362178362577428), (141, 0.14439454632496657), (142, 0.22244615763028036), (143, 0.21903792681162104), (144, 0.23809501978731895), (145, 0.07431384829899074), ...], [(10, 0.017868951261395715), (11, 0.08296982996385277), (61, 0.02989736837972952), (72, 0.10348412582038423), (155, 0.31426513521849647), (156, 0.1699525813091037), (157, 0.1701766922652076), (158, 0.27640100939491724), (159, 0.2978192836150871), (160, 0.24276889717607655), (161, 0.33880121349699616), (162, 0.23450847942150704), (163, 0.3625537576651779), (164, 0.3183102485560416), (165, 0.24137147345766863), (166, 0.3738307494342447)], [(10, 0.04520429241093692), (11, 0.02623679756093562), (19, 0.017320077669206606), (22, 0.08748542354155527), (29, 0.06325047970506475), (53, 0.034367966016593844), (57, 0.08813773738110729), (61, 0.018908347820537255), (167, 0.10458059386108247), (168, 0.19113441661345087), (169, 0.3028886579507455), (170, 0.25858036692787534), (171, 0.13969408373222703), (172, 0.16532352454213142), (173, 0.16532352454213142), (174, 0.3028886579507455), (175, 0.19313267199821985), (176, 0.17097852648678477), (177, 0.2807345124393104), (178, 0.09429094471794976), ...], [(10, 0.009856156567640691), (15, 0.13967442332789326), (19, 0.04531677542111343), (24, 0.10842708839707535), (29, 0.11032697962119518), (41, 0.07260064341795866), (49, 0.06842359667072355), (56, 0.04662892808818644), (57, 0.1537374957667718), (61, 0.016490791171816524), (165, 0.13313568315005253), (190, 0.16755438923335436), (191, 0.13904086957076212), (192, 0.2448407583411407), (193, 0.12430484550933028), (194, 0.18687598151030094), (195, 0.15050032010750405), (196, 0.09143544591163232), (197, 0.12269104128755341), (198, 0.2641623506180873), ...], [(10, 0.05128666371555671), (11, 0.02381362904652451), (19, 0.047161308888427914), (22, 0.07940547692065152), (24, 0.037613469186254155), (30, 0.07061946834251158), (47, 0.05916914130722203), (61, 0.03432403515216796), (65, 0.05219494905222182), (66, 0.13438581975978053), (71, 0.06334580468218413), (72, 0.05940310075515709), (103, 0.05602454869351825), (111, 0.08056360687311669), (124, 0.0665782397585325), (131, 0.09098220614776903), (217, 0.13014327126494443), (218, 0.14707666165679775), (219, 0.1425038820600952), (220, 0.27491457850702283), ...], [(10, 0.03373914624704608), (11, 0.03916473868965497), (19, 0.05170877386415411), (22, 0.03264826570583768), (24, 0.06186044508432571), (29, 0.047208286451325736), (36, 0.046069979621768835), (45, 0.08087235171979418), (47, 0.024327916938460356), (53, 0.07695369862930257), (56, 0.07980901141908836), (61, 0.028225262619209712), (69, 0.1080672987732272), (112, 0.04557089762967087), (127, 0.09740080300544345), (241, 0.1796469603779444), (242, 0.08992259466649655), (243, 0.11166235358548782), (244, 0.15848012162533082), (245, 0.26548969682262125), ...], [(10, 0.014878436612043862), (11, 0.0690841526047578), (19, 0.022802761770881638), (24, 0.05455898049349969), (30, 0.10243474689030971), (45, 0.071326888352145), (47, 0.08582585164937677), (53, 0.045247172477722745), (56, 0.07038905541794807), (69, 0.09531198228070385), (104, 0.09322197900786375), (112, 0.08038421681121939), (131, 0.13197124641419022), (182, 0.17819993890438107), (185, 0.13080027223004068), (270, 0.18048741343046848), (271, 0.12834700317270165), (277, 0.21653463756000216), (278, 0.15765046010508932), (279, 0.3696012414193431), ...], [(10, 0.03791716652108541), (11, 0.05868616859386632), (22, 0.04892159860155609), (29, 0.1061084619891196), (36, 0.13806657118911214), (53, 0.03843693658103161), (64, 0.07493612378605678), (65, 0.06431446407942601), (91, 0.08587929261825251), (105, 0.3574063109864729), (141, 0.15156836850641137), (145, 0.07800591525639525), (213, 0.12076867033953673), (219, 0.17559286809079885), (222, 0.09905080713055688), (290, 0.17634543810717943), (291, 0.2354304202988956), (292, 0.23747359098140175), (293, 0.21838240905413028), (294, 0.2068871594723075), ...], [(10, 0.01568731468793941), (29, 0.08779964258937602), (37, 0.0889416637703053), (53, 0.04770707110604152), (61, 0.02624717137862027), (64, 0.09300905076906602), (237, 0.1379113333775812), (305, 0.30664896585612444), (306, 0.2833078792717857), (307, 0.20730843748050645), (308, 0.2233412719391391), (309, 0.38969490776368737), (310, 0.2695472861513626), (311, 0.21271630313917306), (312, 0.4204476556771751), (313, 0.20658700960294096), (314, 0.4204476556771751), (315, 0.14264507531499204)], [(10, 0.027389551203659726), (316, 0.7340881994597976), (317, 0.37735752159971553), (318, 0.5638844108421996)], [(18, 0.35789169848754704), (36, 0.13040599539512807), (52, 0.11542397475108049), (92, 0.1830990480525225), (130, 0.19234978763494665), (319, 0.37310009228736857), (320, 0.5931025045589059), (321, 0.2808020052524273), (322, 0.44859490237601146)], ...]\n        y = id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   HPL\nName: author, Length: 13051, dtype: object\n        fit_params = {}\n    251         return self\n    252 \n    253     def fit_transform(self, X, y=None, **fit_params):\n    254         \"\"\"Fit the model and transform with the final estimator\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/sklearn/naive_bayes.py in fit(self=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), X=[[(0, 0.2664714100608566), (1, 0.10480362539462479), (2, 0.2604873336049293), (3, 0.2319432706761858), (4, 0.24213524135011738), (5, 0.41116785474457307), (6, 0.39218741029198784), (7, 0.32007681200061067), (8, 0.35974012618542445), (9, 0.3057814279583449), (10, 0.033103432446463785), (11, 0.07685359149299217), (12, 0.19308643986019788), (13, 0.2126248353265933)], [(10, 0.024133907128526488), (11, 0.056029761946444906), (14, 0.2713402113709543), (15, 0.3420085240826053), (16, 0.5718449017195636), (17, 0.2614985439879363), (18, 0.1808821162765368), (19, 0.036987739317106506), (20, 0.42712530611518384), (21, 0.3189304655793621), (22, 0.09341436285112015), (23, 0.27476798475726655), (24, 0.088498637497393)], [(1, 0.07932158066320133), (10, 0.06263658764531839), (14, 0.14084602912179758), (18, 0.09389145710463107), (19, 0.01919942563133236), (22, 0.048489097889497096), (25, 0.2722725986774012), (26, 0.2620800981425273), (27, 0.24487868329638712), (28, 0.2722725986774012), (29, 0.03505679663903942), (30, 0.08624781176709262), (31, 0.07149215764800944), (32, 0.31119614453633054), (33, 0.2729933988114348), (34, 0.1733099699406774), (35, 0.17712329620393766), (36, 0.06842298368233549), (37, 0.1420511395070622), (38, 0.14386314798381059), ...], [(10, 0.015973364734577944), (19, 0.0489617074991789), (23, 0.18185904232327854), (30, 0.1099730849576353), (47, 0.046070957177769906), (48, 0.2975393193706599), (49, 0.11089059498724518), (50, 0.3968007922798119), (51, 0.19827784646150795), (52, 0.07722160044388235), (53, 0.04857698480295758), (54, 0.42811430060066746), (55, 0.3236593651896635), (56, 0.07556909941755309), (57, 0.24915443219781466), (58, 0.42811430060066746), (59, 0.3240930775601281)], [(10, 0.03745602951273969), (36, 0.10229058576225049), (47, 0.10803203710707177), (60, 0.23619581984220353), (61, 0.03133470722492), (62, 0.385564388719468), (63, 0.3015083410363086), (64, 0.11103716027435812), (65, 0.1905968735808948), (66, 0.24536394387695537), (67, 0.5308001436909793), (68, 0.3703268648059433), (69, 0.23994513095105036), (70, 0.3078975881412962)], [(47, 0.07100101906724518), (53, 0.07486311627769447), (61, 0.04118771069619043), (71, 0.15202575486829917), (72, 0.14256352538466183), (73, 0.4667449719251434), (74, 0.2503651118140704), (75, 0.37022902619505615), (76, 0.5477253203876611), (77, 0.4760416328690717)], [(10, 0.06462580383426865), (11, 0.06430138284840814), (18, 0.06919519082403108), (22, 0.03573501237295617), (31, 0.10537515645034605), (37, 0.15703115077885102), (46, 0.07518496273136628), (64, 0.0547374449573064), (65, 0.04697880354149405), (78, 0.1734637042645781), (79, 0.21124370752304877), (80, 0.1237755155749664), (81, 0.15694812556791132), (82, 0.14377383733539228), (83, 0.21875527504189865), (84, 0.24744076215980707), (85, 0.10989353196818279), (86, 0.20541728310317606), (87, 0.1788334923026323), (88, 0.3009539216891794), ...], [(10, 0.01715833256303342), (19, 0.026296941004117013), (22, 0.06641422358292039), (24, 0.06291932116373902), (29, 0.0480163589636599), (30, 0.11813132649510569), (47, 0.049488684312218235), (51, 0.21298688698342833), (53, 0.052180619049802826), (56, 0.08117511625389999), (61, 0.028708399385876993), (102, 0.23403001292583833), (113, 0.20423929132245422), (115, 0.4262370557703321), (116, 0.2690731650842324), (117, 0.3589641175494357), (118, 0.17450791755009787), (119, 0.2001630105211242), (120, 0.303670599600059), (121, 0.4262370557703321), ...], [(10, 0.021322122227474257), (11, 0.04950186582869004), (30, 0.14679809784366432), (31, 0.12168323506929181), (47, 0.12299607457840946), (52, 0.20615924454556003), (61, 0.035675028352090576), (91, 0.14487860845275446), (98, 0.27840822828588596), (103, 0.23291869434274284), (124, 0.13839751535552838), (125, 0.3007913272538198), (126, 0.39081820673094264), (127, 0.2462174723129556), (128, 0.4123268436706392), (129, 0.22158417220596172), (130, 0.3435567610008015), (131, 0.18912652719692835), (132, 0.2306072899995085)], [(10, 0.060204206982548865), (19, 0.055361550083157234), (22, 0.09321221974181244), (49, 0.08359015703908927), (53, 0.036617693219445066), (61, 0.04029217669993592), (104, 0.0754428098308278), (133, 0.26790813350704007), (134, 0.18687533762152778), (135, 0.24430377753540683), (136, 0.23809501978731895), (137, 0.2755070388560312), (138, 0.21903792681162104), (139, 0.12747319308243932), (140, 0.12362178362577428), (141, 0.14439454632496657), (142, 0.22244615763028036), (143, 0.21903792681162104), (144, 0.23809501978731895), (145, 0.07431384829899074), ...], [(10, 0.017868951261395715), (11, 0.08296982996385277), (61, 0.02989736837972952), (72, 0.10348412582038423), (155, 0.31426513521849647), (156, 0.1699525813091037), (157, 0.1701766922652076), (158, 0.27640100939491724), (159, 0.2978192836150871), (160, 0.24276889717607655), (161, 0.33880121349699616), (162, 0.23450847942150704), (163, 0.3625537576651779), (164, 0.3183102485560416), (165, 0.24137147345766863), (166, 0.3738307494342447)], [(10, 0.04520429241093692), (11, 0.02623679756093562), (19, 0.017320077669206606), (22, 0.08748542354155527), (29, 0.06325047970506475), (53, 0.034367966016593844), (57, 0.08813773738110729), (61, 0.018908347820537255), (167, 0.10458059386108247), (168, 0.19113441661345087), (169, 0.3028886579507455), (170, 0.25858036692787534), (171, 0.13969408373222703), (172, 0.16532352454213142), (173, 0.16532352454213142), (174, 0.3028886579507455), (175, 0.19313267199821985), (176, 0.17097852648678477), (177, 0.2807345124393104), (178, 0.09429094471794976), ...], [(10, 0.009856156567640691), (15, 0.13967442332789326), (19, 0.04531677542111343), (24, 0.10842708839707535), (29, 0.11032697962119518), (41, 0.07260064341795866), (49, 0.06842359667072355), (56, 0.04662892808818644), (57, 0.1537374957667718), (61, 0.016490791171816524), (165, 0.13313568315005253), (190, 0.16755438923335436), (191, 0.13904086957076212), (192, 0.2448407583411407), (193, 0.12430484550933028), (194, 0.18687598151030094), (195, 0.15050032010750405), (196, 0.09143544591163232), (197, 0.12269104128755341), (198, 0.2641623506180873), ...], [(10, 0.05128666371555671), (11, 0.02381362904652451), (19, 0.047161308888427914), (22, 0.07940547692065152), (24, 0.037613469186254155), (30, 0.07061946834251158), (47, 0.05916914130722203), (61, 0.03432403515216796), (65, 0.05219494905222182), (66, 0.13438581975978053), (71, 0.06334580468218413), (72, 0.05940310075515709), (103, 0.05602454869351825), (111, 0.08056360687311669), (124, 0.0665782397585325), (131, 0.09098220614776903), (217, 0.13014327126494443), (218, 0.14707666165679775), (219, 0.1425038820600952), (220, 0.27491457850702283), ...], [(10, 0.03373914624704608), (11, 0.03916473868965497), (19, 0.05170877386415411), (22, 0.03264826570583768), (24, 0.06186044508432571), (29, 0.047208286451325736), (36, 0.046069979621768835), (45, 0.08087235171979418), (47, 0.024327916938460356), (53, 0.07695369862930257), (56, 0.07980901141908836), (61, 0.028225262619209712), (69, 0.1080672987732272), (112, 0.04557089762967087), (127, 0.09740080300544345), (241, 0.1796469603779444), (242, 0.08992259466649655), (243, 0.11166235358548782), (244, 0.15848012162533082), (245, 0.26548969682262125), ...], [(10, 0.014878436612043862), (11, 0.0690841526047578), (19, 0.022802761770881638), (24, 0.05455898049349969), (30, 0.10243474689030971), (45, 0.071326888352145), (47, 0.08582585164937677), (53, 0.045247172477722745), (56, 0.07038905541794807), (69, 0.09531198228070385), (104, 0.09322197900786375), (112, 0.08038421681121939), (131, 0.13197124641419022), (182, 0.17819993890438107), (185, 0.13080027223004068), (270, 0.18048741343046848), (271, 0.12834700317270165), (277, 0.21653463756000216), (278, 0.15765046010508932), (279, 0.3696012414193431), ...], [(10, 0.03791716652108541), (11, 0.05868616859386632), (22, 0.04892159860155609), (29, 0.1061084619891196), (36, 0.13806657118911214), (53, 0.03843693658103161), (64, 0.07493612378605678), (65, 0.06431446407942601), (91, 0.08587929261825251), (105, 0.3574063109864729), (141, 0.15156836850641137), (145, 0.07800591525639525), (213, 0.12076867033953673), (219, 0.17559286809079885), (222, 0.09905080713055688), (290, 0.17634543810717943), (291, 0.2354304202988956), (292, 0.23747359098140175), (293, 0.21838240905413028), (294, 0.2068871594723075), ...], [(10, 0.01568731468793941), (29, 0.08779964258937602), (37, 0.0889416637703053), (53, 0.04770707110604152), (61, 0.02624717137862027), (64, 0.09300905076906602), (237, 0.1379113333775812), (305, 0.30664896585612444), (306, 0.2833078792717857), (307, 0.20730843748050645), (308, 0.2233412719391391), (309, 0.38969490776368737), (310, 0.2695472861513626), (311, 0.21271630313917306), (312, 0.4204476556771751), (313, 0.20658700960294096), (314, 0.4204476556771751), (315, 0.14264507531499204)], [(10, 0.027389551203659726), (316, 0.7340881994597976), (317, 0.37735752159971553), (318, 0.5638844108421996)], [(18, 0.35789169848754704), (36, 0.13040599539512807), (52, 0.11542397475108049), (92, 0.1830990480525225), (130, 0.19234978763494665), (319, 0.37310009228736857), (320, 0.5931025045589059), (321, 0.2808020052524273), (322, 0.44859490237601146)], ...], y=id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   HPL\nName: author, Length: 13051, dtype: object, sample_weight=None)\n    574         Returns\n    575         -------\n    576         self : object\n    577             Returns self.\n    578         \"\"\"\n--> 579         X, y = check_X_y(X, y, 'csr')\n        X = [[(0, 0.2664714100608566), (1, 0.10480362539462479), (2, 0.2604873336049293), (3, 0.2319432706761858), (4, 0.24213524135011738), (5, 0.41116785474457307), (6, 0.39218741029198784), (7, 0.32007681200061067), (8, 0.35974012618542445), (9, 0.3057814279583449), (10, 0.033103432446463785), (11, 0.07685359149299217), (12, 0.19308643986019788), (13, 0.2126248353265933)], [(10, 0.024133907128526488), (11, 0.056029761946444906), (14, 0.2713402113709543), (15, 0.3420085240826053), (16, 0.5718449017195636), (17, 0.2614985439879363), (18, 0.1808821162765368), (19, 0.036987739317106506), (20, 0.42712530611518384), (21, 0.3189304655793621), (22, 0.09341436285112015), (23, 0.27476798475726655), (24, 0.088498637497393)], [(1, 0.07932158066320133), (10, 0.06263658764531839), (14, 0.14084602912179758), (18, 0.09389145710463107), (19, 0.01919942563133236), (22, 0.048489097889497096), (25, 0.2722725986774012), (26, 0.2620800981425273), (27, 0.24487868329638712), (28, 0.2722725986774012), (29, 0.03505679663903942), (30, 0.08624781176709262), (31, 0.07149215764800944), (32, 0.31119614453633054), (33, 0.2729933988114348), (34, 0.1733099699406774), (35, 0.17712329620393766), (36, 0.06842298368233549), (37, 0.1420511395070622), (38, 0.14386314798381059), ...], [(10, 0.015973364734577944), (19, 0.0489617074991789), (23, 0.18185904232327854), (30, 0.1099730849576353), (47, 0.046070957177769906), (48, 0.2975393193706599), (49, 0.11089059498724518), (50, 0.3968007922798119), (51, 0.19827784646150795), (52, 0.07722160044388235), (53, 0.04857698480295758), (54, 0.42811430060066746), (55, 0.3236593651896635), (56, 0.07556909941755309), (57, 0.24915443219781466), (58, 0.42811430060066746), (59, 0.3240930775601281)], [(10, 0.03745602951273969), (36, 0.10229058576225049), (47, 0.10803203710707177), (60, 0.23619581984220353), (61, 0.03133470722492), (62, 0.385564388719468), (63, 0.3015083410363086), (64, 0.11103716027435812), (65, 0.1905968735808948), (66, 0.24536394387695537), (67, 0.5308001436909793), (68, 0.3703268648059433), (69, 0.23994513095105036), (70, 0.3078975881412962)], [(47, 0.07100101906724518), (53, 0.07486311627769447), (61, 0.04118771069619043), (71, 0.15202575486829917), (72, 0.14256352538466183), (73, 0.4667449719251434), (74, 0.2503651118140704), (75, 0.37022902619505615), (76, 0.5477253203876611), (77, 0.4760416328690717)], [(10, 0.06462580383426865), (11, 0.06430138284840814), (18, 0.06919519082403108), (22, 0.03573501237295617), (31, 0.10537515645034605), (37, 0.15703115077885102), (46, 0.07518496273136628), (64, 0.0547374449573064), (65, 0.04697880354149405), (78, 0.1734637042645781), (79, 0.21124370752304877), (80, 0.1237755155749664), (81, 0.15694812556791132), (82, 0.14377383733539228), (83, 0.21875527504189865), (84, 0.24744076215980707), (85, 0.10989353196818279), (86, 0.20541728310317606), (87, 0.1788334923026323), (88, 0.3009539216891794), ...], [(10, 0.01715833256303342), (19, 0.026296941004117013), (22, 0.06641422358292039), (24, 0.06291932116373902), (29, 0.0480163589636599), (30, 0.11813132649510569), (47, 0.049488684312218235), (51, 0.21298688698342833), (53, 0.052180619049802826), (56, 0.08117511625389999), (61, 0.028708399385876993), (102, 0.23403001292583833), (113, 0.20423929132245422), (115, 0.4262370557703321), (116, 0.2690731650842324), (117, 0.3589641175494357), (118, 0.17450791755009787), (119, 0.2001630105211242), (120, 0.303670599600059), (121, 0.4262370557703321), ...], [(10, 0.021322122227474257), (11, 0.04950186582869004), (30, 0.14679809784366432), (31, 0.12168323506929181), (47, 0.12299607457840946), (52, 0.20615924454556003), (61, 0.035675028352090576), (91, 0.14487860845275446), (98, 0.27840822828588596), (103, 0.23291869434274284), (124, 0.13839751535552838), (125, 0.3007913272538198), (126, 0.39081820673094264), (127, 0.2462174723129556), (128, 0.4123268436706392), (129, 0.22158417220596172), (130, 0.3435567610008015), (131, 0.18912652719692835), (132, 0.2306072899995085)], [(10, 0.060204206982548865), (19, 0.055361550083157234), (22, 0.09321221974181244), (49, 0.08359015703908927), (53, 0.036617693219445066), (61, 0.04029217669993592), (104, 0.0754428098308278), (133, 0.26790813350704007), (134, 0.18687533762152778), (135, 0.24430377753540683), (136, 0.23809501978731895), (137, 0.2755070388560312), (138, 0.21903792681162104), (139, 0.12747319308243932), (140, 0.12362178362577428), (141, 0.14439454632496657), (142, 0.22244615763028036), (143, 0.21903792681162104), (144, 0.23809501978731895), (145, 0.07431384829899074), ...], [(10, 0.017868951261395715), (11, 0.08296982996385277), (61, 0.02989736837972952), (72, 0.10348412582038423), (155, 0.31426513521849647), (156, 0.1699525813091037), (157, 0.1701766922652076), (158, 0.27640100939491724), (159, 0.2978192836150871), (160, 0.24276889717607655), (161, 0.33880121349699616), (162, 0.23450847942150704), (163, 0.3625537576651779), (164, 0.3183102485560416), (165, 0.24137147345766863), (166, 0.3738307494342447)], [(10, 0.04520429241093692), (11, 0.02623679756093562), (19, 0.017320077669206606), (22, 0.08748542354155527), (29, 0.06325047970506475), (53, 0.034367966016593844), (57, 0.08813773738110729), (61, 0.018908347820537255), (167, 0.10458059386108247), (168, 0.19113441661345087), (169, 0.3028886579507455), (170, 0.25858036692787534), (171, 0.13969408373222703), (172, 0.16532352454213142), (173, 0.16532352454213142), (174, 0.3028886579507455), (175, 0.19313267199821985), (176, 0.17097852648678477), (177, 0.2807345124393104), (178, 0.09429094471794976), ...], [(10, 0.009856156567640691), (15, 0.13967442332789326), (19, 0.04531677542111343), (24, 0.10842708839707535), (29, 0.11032697962119518), (41, 0.07260064341795866), (49, 0.06842359667072355), (56, 0.04662892808818644), (57, 0.1537374957667718), (61, 0.016490791171816524), (165, 0.13313568315005253), (190, 0.16755438923335436), (191, 0.13904086957076212), (192, 0.2448407583411407), (193, 0.12430484550933028), (194, 0.18687598151030094), (195, 0.15050032010750405), (196, 0.09143544591163232), (197, 0.12269104128755341), (198, 0.2641623506180873), ...], [(10, 0.05128666371555671), (11, 0.02381362904652451), (19, 0.047161308888427914), (22, 0.07940547692065152), (24, 0.037613469186254155), (30, 0.07061946834251158), (47, 0.05916914130722203), (61, 0.03432403515216796), (65, 0.05219494905222182), (66, 0.13438581975978053), (71, 0.06334580468218413), (72, 0.05940310075515709), (103, 0.05602454869351825), (111, 0.08056360687311669), (124, 0.0665782397585325), (131, 0.09098220614776903), (217, 0.13014327126494443), (218, 0.14707666165679775), (219, 0.1425038820600952), (220, 0.27491457850702283), ...], [(10, 0.03373914624704608), (11, 0.03916473868965497), (19, 0.05170877386415411), (22, 0.03264826570583768), (24, 0.06186044508432571), (29, 0.047208286451325736), (36, 0.046069979621768835), (45, 0.08087235171979418), (47, 0.024327916938460356), (53, 0.07695369862930257), (56, 0.07980901141908836), (61, 0.028225262619209712), (69, 0.1080672987732272), (112, 0.04557089762967087), (127, 0.09740080300544345), (241, 0.1796469603779444), (242, 0.08992259466649655), (243, 0.11166235358548782), (244, 0.15848012162533082), (245, 0.26548969682262125), ...], [(10, 0.014878436612043862), (11, 0.0690841526047578), (19, 0.022802761770881638), (24, 0.05455898049349969), (30, 0.10243474689030971), (45, 0.071326888352145), (47, 0.08582585164937677), (53, 0.045247172477722745), (56, 0.07038905541794807), (69, 0.09531198228070385), (104, 0.09322197900786375), (112, 0.08038421681121939), (131, 0.13197124641419022), (182, 0.17819993890438107), (185, 0.13080027223004068), (270, 0.18048741343046848), (271, 0.12834700317270165), (277, 0.21653463756000216), (278, 0.15765046010508932), (279, 0.3696012414193431), ...], [(10, 0.03791716652108541), (11, 0.05868616859386632), (22, 0.04892159860155609), (29, 0.1061084619891196), (36, 0.13806657118911214), (53, 0.03843693658103161), (64, 0.07493612378605678), (65, 0.06431446407942601), (91, 0.08587929261825251), (105, 0.3574063109864729), (141, 0.15156836850641137), (145, 0.07800591525639525), (213, 0.12076867033953673), (219, 0.17559286809079885), (222, 0.09905080713055688), (290, 0.17634543810717943), (291, 0.2354304202988956), (292, 0.23747359098140175), (293, 0.21838240905413028), (294, 0.2068871594723075), ...], [(10, 0.01568731468793941), (29, 0.08779964258937602), (37, 0.0889416637703053), (53, 0.04770707110604152), (61, 0.02624717137862027), (64, 0.09300905076906602), (237, 0.1379113333775812), (305, 0.30664896585612444), (306, 0.2833078792717857), (307, 0.20730843748050645), (308, 0.2233412719391391), (309, 0.38969490776368737), (310, 0.2695472861513626), (311, 0.21271630313917306), (312, 0.4204476556771751), (313, 0.20658700960294096), (314, 0.4204476556771751), (315, 0.14264507531499204)], [(10, 0.027389551203659726), (316, 0.7340881994597976), (317, 0.37735752159971553), (318, 0.5638844108421996)], [(18, 0.35789169848754704), (36, 0.13040599539512807), (52, 0.11542397475108049), (92, 0.1830990480525225), (130, 0.19234978763494665), (319, 0.37310009228736857), (320, 0.5931025045589059), (321, 0.2808020052524273), (322, 0.44859490237601146)], ...]\n        y = id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   HPL\nName: author, Length: 13051, dtype: object\n    580         _, n_features = X.shape\n    581 \n    582         labelbin = LabelBinarizer()\n    583         Y = labelbin.fit_transform(y)\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/sklearn/utils/validation.py in check_X_y(X=[[(0, 0.2664714100608566), (1, 0.10480362539462479), (2, 0.2604873336049293), (3, 0.2319432706761858), (4, 0.24213524135011738), (5, 0.41116785474457307), (6, 0.39218741029198784), (7, 0.32007681200061067), (8, 0.35974012618542445), (9, 0.3057814279583449), (10, 0.033103432446463785), (11, 0.07685359149299217), (12, 0.19308643986019788), (13, 0.2126248353265933)], [(10, 0.024133907128526488), (11, 0.056029761946444906), (14, 0.2713402113709543), (15, 0.3420085240826053), (16, 0.5718449017195636), (17, 0.2614985439879363), (18, 0.1808821162765368), (19, 0.036987739317106506), (20, 0.42712530611518384), (21, 0.3189304655793621), (22, 0.09341436285112015), (23, 0.27476798475726655), (24, 0.088498637497393)], [(1, 0.07932158066320133), (10, 0.06263658764531839), (14, 0.14084602912179758), (18, 0.09389145710463107), (19, 0.01919942563133236), (22, 0.048489097889497096), (25, 0.2722725986774012), (26, 0.2620800981425273), (27, 0.24487868329638712), (28, 0.2722725986774012), (29, 0.03505679663903942), (30, 0.08624781176709262), (31, 0.07149215764800944), (32, 0.31119614453633054), (33, 0.2729933988114348), (34, 0.1733099699406774), (35, 0.17712329620393766), (36, 0.06842298368233549), (37, 0.1420511395070622), (38, 0.14386314798381059), ...], [(10, 0.015973364734577944), (19, 0.0489617074991789), (23, 0.18185904232327854), (30, 0.1099730849576353), (47, 0.046070957177769906), (48, 0.2975393193706599), (49, 0.11089059498724518), (50, 0.3968007922798119), (51, 0.19827784646150795), (52, 0.07722160044388235), (53, 0.04857698480295758), (54, 0.42811430060066746), (55, 0.3236593651896635), (56, 0.07556909941755309), (57, 0.24915443219781466), (58, 0.42811430060066746), (59, 0.3240930775601281)], [(10, 0.03745602951273969), (36, 0.10229058576225049), (47, 0.10803203710707177), (60, 0.23619581984220353), (61, 0.03133470722492), (62, 0.385564388719468), (63, 0.3015083410363086), (64, 0.11103716027435812), (65, 0.1905968735808948), (66, 0.24536394387695537), (67, 0.5308001436909793), (68, 0.3703268648059433), (69, 0.23994513095105036), (70, 0.3078975881412962)], [(47, 0.07100101906724518), (53, 0.07486311627769447), (61, 0.04118771069619043), (71, 0.15202575486829917), (72, 0.14256352538466183), (73, 0.4667449719251434), (74, 0.2503651118140704), (75, 0.37022902619505615), (76, 0.5477253203876611), (77, 0.4760416328690717)], [(10, 0.06462580383426865), (11, 0.06430138284840814), (18, 0.06919519082403108), (22, 0.03573501237295617), (31, 0.10537515645034605), (37, 0.15703115077885102), (46, 0.07518496273136628), (64, 0.0547374449573064), (65, 0.04697880354149405), (78, 0.1734637042645781), (79, 0.21124370752304877), (80, 0.1237755155749664), (81, 0.15694812556791132), (82, 0.14377383733539228), (83, 0.21875527504189865), (84, 0.24744076215980707), (85, 0.10989353196818279), (86, 0.20541728310317606), (87, 0.1788334923026323), (88, 0.3009539216891794), ...], [(10, 0.01715833256303342), (19, 0.026296941004117013), (22, 0.06641422358292039), (24, 0.06291932116373902), (29, 0.0480163589636599), (30, 0.11813132649510569), (47, 0.049488684312218235), (51, 0.21298688698342833), (53, 0.052180619049802826), (56, 0.08117511625389999), (61, 0.028708399385876993), (102, 0.23403001292583833), (113, 0.20423929132245422), (115, 0.4262370557703321), (116, 0.2690731650842324), (117, 0.3589641175494357), (118, 0.17450791755009787), (119, 0.2001630105211242), (120, 0.303670599600059), (121, 0.4262370557703321), ...], [(10, 0.021322122227474257), (11, 0.04950186582869004), (30, 0.14679809784366432), (31, 0.12168323506929181), (47, 0.12299607457840946), (52, 0.20615924454556003), (61, 0.035675028352090576), (91, 0.14487860845275446), (98, 0.27840822828588596), (103, 0.23291869434274284), (124, 0.13839751535552838), (125, 0.3007913272538198), (126, 0.39081820673094264), (127, 0.2462174723129556), (128, 0.4123268436706392), (129, 0.22158417220596172), (130, 0.3435567610008015), (131, 0.18912652719692835), (132, 0.2306072899995085)], [(10, 0.060204206982548865), (19, 0.055361550083157234), (22, 0.09321221974181244), (49, 0.08359015703908927), (53, 0.036617693219445066), (61, 0.04029217669993592), (104, 0.0754428098308278), (133, 0.26790813350704007), (134, 0.18687533762152778), (135, 0.24430377753540683), (136, 0.23809501978731895), (137, 0.2755070388560312), (138, 0.21903792681162104), (139, 0.12747319308243932), (140, 0.12362178362577428), (141, 0.14439454632496657), (142, 0.22244615763028036), (143, 0.21903792681162104), (144, 0.23809501978731895), (145, 0.07431384829899074), ...], [(10, 0.017868951261395715), (11, 0.08296982996385277), (61, 0.02989736837972952), (72, 0.10348412582038423), (155, 0.31426513521849647), (156, 0.1699525813091037), (157, 0.1701766922652076), (158, 0.27640100939491724), (159, 0.2978192836150871), (160, 0.24276889717607655), (161, 0.33880121349699616), (162, 0.23450847942150704), (163, 0.3625537576651779), (164, 0.3183102485560416), (165, 0.24137147345766863), (166, 0.3738307494342447)], [(10, 0.04520429241093692), (11, 0.02623679756093562), (19, 0.017320077669206606), (22, 0.08748542354155527), (29, 0.06325047970506475), (53, 0.034367966016593844), (57, 0.08813773738110729), (61, 0.018908347820537255), (167, 0.10458059386108247), (168, 0.19113441661345087), (169, 0.3028886579507455), (170, 0.25858036692787534), (171, 0.13969408373222703), (172, 0.16532352454213142), (173, 0.16532352454213142), (174, 0.3028886579507455), (175, 0.19313267199821985), (176, 0.17097852648678477), (177, 0.2807345124393104), (178, 0.09429094471794976), ...], [(10, 0.009856156567640691), (15, 0.13967442332789326), (19, 0.04531677542111343), (24, 0.10842708839707535), (29, 0.11032697962119518), (41, 0.07260064341795866), (49, 0.06842359667072355), (56, 0.04662892808818644), (57, 0.1537374957667718), (61, 0.016490791171816524), (165, 0.13313568315005253), (190, 0.16755438923335436), (191, 0.13904086957076212), (192, 0.2448407583411407), (193, 0.12430484550933028), (194, 0.18687598151030094), (195, 0.15050032010750405), (196, 0.09143544591163232), (197, 0.12269104128755341), (198, 0.2641623506180873), ...], [(10, 0.05128666371555671), (11, 0.02381362904652451), (19, 0.047161308888427914), (22, 0.07940547692065152), (24, 0.037613469186254155), (30, 0.07061946834251158), (47, 0.05916914130722203), (61, 0.03432403515216796), (65, 0.05219494905222182), (66, 0.13438581975978053), (71, 0.06334580468218413), (72, 0.05940310075515709), (103, 0.05602454869351825), (111, 0.08056360687311669), (124, 0.0665782397585325), (131, 0.09098220614776903), (217, 0.13014327126494443), (218, 0.14707666165679775), (219, 0.1425038820600952), (220, 0.27491457850702283), ...], [(10, 0.03373914624704608), (11, 0.03916473868965497), (19, 0.05170877386415411), (22, 0.03264826570583768), (24, 0.06186044508432571), (29, 0.047208286451325736), (36, 0.046069979621768835), (45, 0.08087235171979418), (47, 0.024327916938460356), (53, 0.07695369862930257), (56, 0.07980901141908836), (61, 0.028225262619209712), (69, 0.1080672987732272), (112, 0.04557089762967087), (127, 0.09740080300544345), (241, 0.1796469603779444), (242, 0.08992259466649655), (243, 0.11166235358548782), (244, 0.15848012162533082), (245, 0.26548969682262125), ...], [(10, 0.014878436612043862), (11, 0.0690841526047578), (19, 0.022802761770881638), (24, 0.05455898049349969), (30, 0.10243474689030971), (45, 0.071326888352145), (47, 0.08582585164937677), (53, 0.045247172477722745), (56, 0.07038905541794807), (69, 0.09531198228070385), (104, 0.09322197900786375), (112, 0.08038421681121939), (131, 0.13197124641419022), (182, 0.17819993890438107), (185, 0.13080027223004068), (270, 0.18048741343046848), (271, 0.12834700317270165), (277, 0.21653463756000216), (278, 0.15765046010508932), (279, 0.3696012414193431), ...], [(10, 0.03791716652108541), (11, 0.05868616859386632), (22, 0.04892159860155609), (29, 0.1061084619891196), (36, 0.13806657118911214), (53, 0.03843693658103161), (64, 0.07493612378605678), (65, 0.06431446407942601), (91, 0.08587929261825251), (105, 0.3574063109864729), (141, 0.15156836850641137), (145, 0.07800591525639525), (213, 0.12076867033953673), (219, 0.17559286809079885), (222, 0.09905080713055688), (290, 0.17634543810717943), (291, 0.2354304202988956), (292, 0.23747359098140175), (293, 0.21838240905413028), (294, 0.2068871594723075), ...], [(10, 0.01568731468793941), (29, 0.08779964258937602), (37, 0.0889416637703053), (53, 0.04770707110604152), (61, 0.02624717137862027), (64, 0.09300905076906602), (237, 0.1379113333775812), (305, 0.30664896585612444), (306, 0.2833078792717857), (307, 0.20730843748050645), (308, 0.2233412719391391), (309, 0.38969490776368737), (310, 0.2695472861513626), (311, 0.21271630313917306), (312, 0.4204476556771751), (313, 0.20658700960294096), (314, 0.4204476556771751), (315, 0.14264507531499204)], [(10, 0.027389551203659726), (316, 0.7340881994597976), (317, 0.37735752159971553), (318, 0.5638844108421996)], [(18, 0.35789169848754704), (36, 0.13040599539512807), (52, 0.11542397475108049), (92, 0.1830990480525225), (130, 0.19234978763494665), (319, 0.37310009228736857), (320, 0.5931025045589059), (321, 0.2808020052524273), (322, 0.44859490237601146)], ...], y=id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   HPL\nName: author, Length: 13051, dtype: object, accept_sparse='csr', dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, multi_output=False, ensure_min_samples=1, ensure_min_features=1, y_numeric=False, warn_on_dtype=False, estimator=None)\n    568     y_converted : object\n    569         The converted and validated y.\n    570     \"\"\"\n    571     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n    572                     ensure_2d, allow_nd, ensure_min_samples,\n--> 573                     ensure_min_features, warn_on_dtype, estimator)\n        ensure_min_features = 1\n        warn_on_dtype = False\n        estimator = None\n    574     if multi_output:\n    575         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n    576                         dtype=None)\n    577     else:\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/sklearn/utils/validation.py in check_array(array=array([ list([(0, 0.2664714100608566), (1, 0.104...), (11075, 0.41154013028457553)])], dtype=object), accept_sparse='csr', dtype=None, order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=False, estimator=None)\n    436             if array.ndim == 1:\n    437                 raise ValueError(\n    438                     \"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\n    439                     \"Reshape your data either using array.reshape(-1, 1) if \"\n    440                     \"your data has a single feature or array.reshape(1, -1) \"\n--> 441                     \"if it contains a single sample.\".format(array))\n        array = array([ list([(0, 0.2664714100608566), (1, 0.104...), (11075, 0.41154013028457553)])], dtype=object)\n    442             array = np.atleast_2d(array)\n    443             # To ensure that array flags are maintained\n    444             array = np.array(array, dtype=dtype, order=order, copy=copy)\n    445 \n\nValueError: Expected 2D array, got 1D array instead:\narray=[ list([(0, 0.2664714100608566), (1, 0.10480362539462479), (2, 0.2604873336049293), (3, 0.2319432706761858), (4, 0.24213524135011738), (5, 0.41116785474457307), (6, 0.39218741029198784), (7, 0.32007681200061067), (8, 0.35974012618542445), (9, 0.3057814279583449), (10, 0.033103432446463785), (11, 0.07685359149299217), (12, 0.19308643986019788), (13, 0.2126248353265933)])\n list([(10, 0.024133907128526488), (11, 0.056029761946444906), (14, 0.2713402113709543), (15, 0.3420085240826053), (16, 0.5718449017195636), (17, 0.2614985439879363), (18, 0.1808821162765368), (19, 0.036987739317106506), (20, 0.42712530611518384), (21, 0.3189304655793621), (22, 0.09341436285112015), (23, 0.27476798475726655), (24, 0.088498637497393)])\n list([(1, 0.07932158066320133), (10, 0.06263658764531839), (14, 0.14084602912179758), (18, 0.09389145710463107), (19, 0.01919942563133236), (22, 0.048489097889497096), (25, 0.2722725986774012), (26, 0.2620800981425273), (27, 0.24487868329638712), (28, 0.2722725986774012), (29, 0.03505679663903942), (30, 0.08624781176709262), (31, 0.07149215764800944), (32, 0.31119614453633054), (33, 0.2729933988114348), (34, 0.1733099699406774), (35, 0.17712329620393766), (36, 0.06842298368233549), (37, 0.1420511395070622), (38, 0.14386314798381059), (39, 0.2050581341267459), (40, 0.23334905281847185), (41, 0.09227646767474229), (42, 0.21187381993571314), (43, 0.3357541677332322), (44, 0.3357541677332322), (45, 0.06005567668474601), (46, 0.10201902211900157)])\n ...,\n list([(11, 0.03483379824389391), (22, 0.05807586817427632), (29, 0.041987869208477496), (145, 0.09260247785729721), (208, 0.1530751157324031), (1037, 0.16584589431194527), (6507, 0.28722118181246425), (8178, 0.3338402944907435), (16710, 0.37272267484996136), (20408, 0.37272267484996136), (20543, 0.37272267484996136), (23339, 0.4021360601237881), (23340, 0.4021360601237881)])\n list([(19, 0.028810064274408482), (24, 0.06893234032603549), (49, 0.13050056185493572), (60, 0.2370797936612917), (65, 0.19131019122706705), (69, 0.12042156837049621), (154, 0.19044775173097933), (237, 0.16525911588786907), (1154, 0.18801448431613588), (3699, 0.28355351691441877), (8329, 0.4085638403294151), (8330, 0.35641827923263075), (11702, 0.44541485037215933), (15323, 0.44541485037215933)])\n list([(11, 0.04032298873683362), (19, 0.02661899933307242), (22, 0.13445519560495678), (24, 0.06368989335423944), (29, 0.048604414750374345), (56, 0.08216926695338246), (61, 0.029059990817433712), (65, 0.08838032786353116), (103, 0.09486488773052526), (104, 0.10882347594433836), (178, 0.14491451149923437), (230, 0.10170684012562291), (360, 0.15235494384947212), (395, 0.363360346899261), (988, 0.17705848089319678), (1512, 0.2984337453903016), (3656, 0.39740876145355286), (3947, 0.3699196063748875), (8512, 0.25681322148061364), (10438, 0.3159541460973265), (11075, 0.41154013028457553)])].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n___________________________________________________________________________",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJoblibValueError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    340\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                                 pre_dispatch=pre_dispatch)\n\u001b[0m\u001b[1;32m    343\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             return_times=True)\n\u001b[0;32m--> 206\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    738\u001b[0m                     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJoblibValueError\u001b[0m: JoblibValueError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/usr/lib/python3.6/runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/usr/lib/python3.6/runpy.py in _run_code(code=<code object <module> at 0x7f608565f1e0, file \"/...3.6/site-packages/ipykernel_launcher.py\", line 5>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/home/bob/.local/lib/python3.6/site-packages/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/home/bob/.local/lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/home/bob/.l.../python3.6/site-packages/ipykernel/kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x7f608565f1e0, file \"/...3.6/site-packages/ipykernel_launcher.py\", line 5>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/home/bob/.local/lib/python3.6/site-packages/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/home/bob/.local/lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/home/bob/.l.../python3.6/site-packages/ipykernel/kernelapp.py'>, ...}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    473         if self.poller is not None:\n    474             self.poller.start()\n    475         self.kernel.start()\n    476         self.io_loop = ioloop.IOLoop.current()\n    477         try:\n--> 478             self.io_loop.start()\n        self.io_loop.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    479         except KeyboardInterrupt:\n    480             pass\n    481 \n    482 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    172             )\n    173         return loop\n    174     \n    175     def start(self):\n    176         try:\n--> 177             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    178         except ZMQError as e:\n    179             if e.errno == ETERM:\n    180                 # quietly return on ETERM\n    181                 pass\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    883                 self._events.update(event_pairs)\n    884                 while self._events:\n    885                     fd, events = self._events.popitem()\n    886                     try:\n    887                         fd_obj, handler_func = self._handlers[fd]\n--> 888                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    889                     except (OSError, IOError) as e:\n    890                         if errno_from_exception(e) == errno.EPIPE:\n    891                             # Happens when the client closes the connection\n    892                             pass\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    276         if self.control_stream:\n    277             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    278 \n    279         def make_dispatcher(stream):\n    280             def dispatcher(msg):\n--> 281                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    282             return dispatcher\n    283 \n    284         for s in self.shell_streams:\n    285             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': \"%%time\\nprint(cross_val_score(pipeline, train.tex...         cv=3, n_jobs=3, scoring='neg_log_loss'))\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 1, 3, 14, 40, 37, 109981, tzinfo=tzutc()), 'msg_id': '8D23B63CFDE24DCC8A42E9FBCF10A1A6', 'msg_type': 'execute_request', 'session': '512E4502520F4C58B500E845BCAB949B', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '8D23B63CFDE24DCC8A42E9FBCF10A1A6', 'msg_type': 'execute_request', 'parent_header': {}})\n    227             self.log.warn(\"Unknown message type: %r\", msg_type)\n    228         else:\n    229             self.log.debug(\"%s: %s\", msg_type, msg)\n    230             self.pre_handler_hook()\n    231             try:\n--> 232                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'512E4502520F4C58B500E845BCAB949B']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': \"%%time\\nprint(cross_val_score(pipeline, train.tex...         cv=3, n_jobs=3, scoring='neg_log_loss'))\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 1, 3, 14, 40, 37, 109981, tzinfo=tzutc()), 'msg_id': '8D23B63CFDE24DCC8A42E9FBCF10A1A6', 'msg_type': 'execute_request', 'session': '512E4502520F4C58B500E845BCAB949B', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '8D23B63CFDE24DCC8A42E9FBCF10A1A6', 'msg_type': 'execute_request', 'parent_header': {}}\n    233             except Exception:\n    234                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    235             finally:\n    236                 self.post_handler_hook()\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'512E4502520F4C58B500E845BCAB949B'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': \"%%time\\nprint(cross_val_score(pipeline, train.tex...         cv=3, n_jobs=3, scoring='neg_log_loss'))\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 1, 3, 14, 40, 37, 109981, tzinfo=tzutc()), 'msg_id': '8D23B63CFDE24DCC8A42E9FBCF10A1A6', 'msg_type': 'execute_request', 'session': '512E4502520F4C58B500E845BCAB949B', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '8D23B63CFDE24DCC8A42E9FBCF10A1A6', 'msg_type': 'execute_request', 'parent_header': {}})\n    392         if not silent:\n    393             self.execution_count += 1\n    394             self._publish_execute_input(code, parent, self.execution_count)\n    395 \n    396         reply_content = self.do_execute(code, silent, store_history,\n--> 397                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    398 \n    399         # Flush output before sending the reply.\n    400         sys.stdout.flush()\n    401         sys.stderr.flush()\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code=\"%%time\\nprint(cross_val_score(pipeline, train.tex...         cv=3, n_jobs=3, scoring='neg_log_loss'))\", silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    203 \n    204         self._forward_input(allow_stdin)\n    205 \n    206         reply_content = {}\n    207         try:\n--> 208             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = \"%%time\\nprint(cross_val_score(pipeline, train.tex...         cv=3, n_jobs=3, scoring='neg_log_loss'))\"\n        store_history = True\n        silent = False\n    209         finally:\n    210             self._restore_input()\n    211 \n    212         if res.error_before_exec is not None:\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=(\"%%time\\nprint(cross_val_score(pipeline, train.tex...         cv=3, n_jobs=3, scoring='neg_log_loss'))\",), **kwargs={'silent': False, 'store_history': True})\n    528             )\n    529         self.payload_manager.write_payload(payload)\n    530 \n    531     def run_cell(self, *args, **kwargs):\n    532         self._last_traceback = None\n--> 533         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = (\"%%time\\nprint(cross_val_score(pipeline, train.tex...         cv=3, n_jobs=3, scoring='neg_log_loss'))\",)\n        kwargs = {'silent': False, 'store_history': True}\n    534 \n    535     def _showtraceback(self, etype, evalue, stb):\n    536         # try to preserve ordering of tracebacks and print statements\n    537         sys.stdout.flush()\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=\"%%time\\nprint(cross_val_score(pipeline, train.tex...         cv=3, n_jobs=3, scoring='neg_log_loss'))\", store_history=True, silent=False, shell_futures=True)\n   2723                 self.displayhook.exec_result = result\n   2724 \n   2725                 # Execute the user code\n   2726                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2727                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2728                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2729                 \n   2730                 self.last_execution_succeeded = not has_raised\n   2731                 self.last_execution_result = result\n   2732 \n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Expr object>], cell_name='<ipython-input-118-3fa43c63eb93>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 7f602820e198, executi..._before_exec=None error_in_exec=None result=None>)\n   2851                     return True\n   2852 \n   2853             for i, node in enumerate(to_run_interactive):\n   2854                 mod = ast.Interactive([node])\n   2855                 code = compiler(mod, cell_name, \"single\")\n-> 2856                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7f6026d6aed0, file \"<ipython-input-118-3fa43c63eb93>\", line 1>\n        result = <ExecutionResult object at 7f602820e198, executi..._before_exec=None error_in_exec=None result=None>\n   2857                     return True\n   2858 \n   2859             # Flush softspace\n   2860             if softspace(sys.stdout, 0):\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7f6026d6aed0, file \"<ipython-input-118-3fa43c63eb93>\", line 1>, result=<ExecutionResult object at 7f602820e198, executi..._before_exec=None error_in_exec=None result=None>)\n   2905         outflag = True  # happens in more places, so it's easier as default\n   2906         try:\n   2907             try:\n   2908                 self.hooks.pre_run_code_hook()\n   2909                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2910                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7f6026d6aed0, file \"<ipython-input-118-3fa43c63eb93>\", line 1>\n        self.user_global_ns = {'Adam': <class 'keras.optimizers.Adam'>, 'CUSTOM_FILTERS': [<function strip_punctuation>, <function remove_stopwords>], 'Concatenate': <class 'keras.layers.merge.Concatenate'>, 'Conv2D': <class 'keras.layers.convolutional.Conv2D'>, 'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'Dense': <class 'keras.layers.core.Dense'>, 'Dropout': <class 'keras.layers.core.Dropout'>, 'FILENAME': 'processed_texts.pickle', 'HashingVectorizer': <class 'sklearn.feature_extraction.text.HashingVectorizer'>, 'In': ['', \"# corpus = [dictionary.doc2bow(text) for text in...k\\ncorpus = corpora.mmcorpus.MmCorpus('spooky.mm')\", \"import numpy as np\\nimport matplotlib.pyplot as p...lib', 'inline')\\n\\npd.options.display.max_rows = 10\", '# Dataset\\n\\n# use this if in fmi-hw... repo\\n# tra...pe)\\nprint(set(train.columns) - set(test.columns))', 'train.head(5)', 'from sklearn.pipeline import Pipeline\\nfrom sklea...om sklearn.model_selection import cross_val_score', \"pipeline = Pipeline([\\n    ('features', CountVect...peline, train.text, train.author, cv=3, n_jobs=3)\", 'from sklearn.model_selection import cross_val_pr...peline, train.text, train.author, cv=3, n_jobs=3)', \"import itertools\\nfrom sklearn.metrics import con...l('True label')\\n    plt.xlabel('Predicted label')\", 'pipeline.fit(train.text, train.author)\\nauthors =...matrix(train.author, prediction, classes=authors)', 'plot_confusion_matrix(train.author, prediction, classes=authors, normalize=True)', \"from sklearn.ensemble import RandomForestClassif...         cv=3, n_jobs=3, scoring='neg_log_loss'))\", \"from sklearn.linear_model import LogisticRegress...log_loss'))\\n\\n# Получихме малко по-добри резултати\", 'explore = train.copy()', \"# # бр. думи в текста\\n# explore['words'] = explo... sum([c in string.punctuation for c in str(s)]) )\", \"import nltk\\n# nltk.download('stopwords')\\nstopwords = nltk.corpus.stopwords.words('english')\", \"import nltk\\nnltk.download('stopwords')\\nstopwords = nltk.corpus.stopwords.words('english')\", \"import nltk\\n# nltk.download('stopwords')\\nstopwords = nltk.corpus.stopwords.words('english')\", \"# features_names = list(set(explore.columns) - {...or'})\\n# print('numeric features', features_names)\", \"from nltk.stem import WordNetLemmatizer\\nfrom nlt....split()])) \\n\\nexplore[['stemmed', 'text']].head()\", ...], ...}\n        self.user_ns = {'Adam': <class 'keras.optimizers.Adam'>, 'CUSTOM_FILTERS': [<function strip_punctuation>, <function remove_stopwords>], 'Concatenate': <class 'keras.layers.merge.Concatenate'>, 'Conv2D': <class 'keras.layers.convolutional.Conv2D'>, 'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'Dense': <class 'keras.layers.core.Dense'>, 'Dropout': <class 'keras.layers.core.Dropout'>, 'FILENAME': 'processed_texts.pickle', 'HashingVectorizer': <class 'sklearn.feature_extraction.text.HashingVectorizer'>, 'In': ['', \"# corpus = [dictionary.doc2bow(text) for text in...k\\ncorpus = corpora.mmcorpus.MmCorpus('spooky.mm')\", \"import numpy as np\\nimport matplotlib.pyplot as p...lib', 'inline')\\n\\npd.options.display.max_rows = 10\", '# Dataset\\n\\n# use this if in fmi-hw... repo\\n# tra...pe)\\nprint(set(train.columns) - set(test.columns))', 'train.head(5)', 'from sklearn.pipeline import Pipeline\\nfrom sklea...om sklearn.model_selection import cross_val_score', \"pipeline = Pipeline([\\n    ('features', CountVect...peline, train.text, train.author, cv=3, n_jobs=3)\", 'from sklearn.model_selection import cross_val_pr...peline, train.text, train.author, cv=3, n_jobs=3)', \"import itertools\\nfrom sklearn.metrics import con...l('True label')\\n    plt.xlabel('Predicted label')\", 'pipeline.fit(train.text, train.author)\\nauthors =...matrix(train.author, prediction, classes=authors)', 'plot_confusion_matrix(train.author, prediction, classes=authors, normalize=True)', \"from sklearn.ensemble import RandomForestClassif...         cv=3, n_jobs=3, scoring='neg_log_loss'))\", \"from sklearn.linear_model import LogisticRegress...log_loss'))\\n\\n# Получихме малко по-добри резултати\", 'explore = train.copy()', \"# # бр. думи в текста\\n# explore['words'] = explo... sum([c in string.punctuation for c in str(s)]) )\", \"import nltk\\n# nltk.download('stopwords')\\nstopwords = nltk.corpus.stopwords.words('english')\", \"import nltk\\nnltk.download('stopwords')\\nstopwords = nltk.corpus.stopwords.words('english')\", \"import nltk\\n# nltk.download('stopwords')\\nstopwords = nltk.corpus.stopwords.words('english')\", \"# features_names = list(set(explore.columns) - {...or'})\\n# print('numeric features', features_names)\", \"from nltk.stem import WordNetLemmatizer\\nfrom nlt....split()])) \\n\\nexplore[['stemmed', 'text']].head()\", ...], ...}\n   2911             finally:\n   2912                 # Reset our crash handler in place\n   2913                 sys.excepthook = old_excepthook\n   2914         except SystemExit as e:\n\n...........................................................................\n/home/bob/gits/spooky/<ipython-input-118-3fa43c63eb93> in <module>()\n----> 1 get_ipython().run_cell_magic('time', '', \"print(cross_val_score(pipeline, train.text, train.author, \\n                cv=3, n_jobs=3, scoring='neg_log_loss'))\")\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_cell_magic(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, magic_name='time', line='', cell=\"print(cross_val_score(pipeline, train.text, trai...         cv=3, n_jobs=3, scoring='neg_log_loss'))\")\n   2126             # This will need to be updated if the internal calling logic gets\n   2127             # refactored, or else we'll be expanding the wrong variables.\n   2128             stack_depth = 2\n   2129             magic_arg_s = self.var_expand(line, stack_depth)\n   2130             with self.builtin_trap:\n-> 2131                 result = fn(magic_arg_s, cell)\n        result = undefined\n        fn = <bound method ExecutionMagics.time of <IPython.core.magics.execution.ExecutionMagics object>>\n        magic_arg_s = ''\n        cell = \"print(cross_val_score(pipeline, train.text, trai...         cv=3, n_jobs=3, scoring='neg_log_loss'))\"\n   2132             return result\n   2133 \n   2134     def find_line_magic(self, magic_name):\n   2135         \"\"\"Find and return a line magic by name.\n\n...........................................................................\n/home/bob/gits/spooky/<decorator-gen-62> in time(self=<IPython.core.magics.execution.ExecutionMagics object>, line='', cell=\"print(cross_val_score(pipeline, train.text, trai...         cv=3, n_jobs=3, scoring='neg_log_loss'))\", local_ns=None)\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/IPython/core/magic.py in <lambda>(f=<function ExecutionMagics.time>, *a=(<IPython.core.magics.execution.ExecutionMagics object>, '', \"print(cross_val_score(pipeline, train.text, trai...         cv=3, n_jobs=3, scoring='neg_log_loss'))\", None), **k={})\n    182     validate_type(magic_kind)\n    183 \n    184     # This is a closure to capture the magic_kind.  We could also use a class,\n    185     # but it's overkill for just that one bit of state.\n    186     def magic_deco(arg):\n--> 187         call = lambda f, *a, **k: f(*a, **k)\n        f = <function ExecutionMagics.time>\n        a = (<IPython.core.magics.execution.ExecutionMagics object>, '', \"print(cross_val_score(pipeline, train.text, trai...         cv=3, n_jobs=3, scoring='neg_log_loss'))\", None)\n        k = {}\n    188 \n    189         if callable(arg):\n    190             # \"Naked\" decorator call (just @foo, no args)\n    191             func = arg\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/IPython/core/magics/execution.py in time(self=<IPython.core.magics.execution.ExecutionMagics object>, line='', cell=\"print(cross_val_score(pipeline, train.text, trai...         cv=3, n_jobs=3, scoring='neg_log_loss'))\", local_ns=None)\n   1225         # time execution\n   1226         wall_st = wtime()\n   1227         if mode=='eval':\n   1228             st = clock2()\n   1229             try:\n-> 1230                 out = eval(code, glob, local_ns)\n        out = undefined\n        code = <code object <module> at 0x7f6025716ed0, file \"<timed eval>\", line 1>\n        glob = {'Adam': <class 'keras.optimizers.Adam'>, 'CUSTOM_FILTERS': [<function strip_punctuation>, <function remove_stopwords>], 'Concatenate': <class 'keras.layers.merge.Concatenate'>, 'Conv2D': <class 'keras.layers.convolutional.Conv2D'>, 'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'Dense': <class 'keras.layers.core.Dense'>, 'Dropout': <class 'keras.layers.core.Dropout'>, 'FILENAME': 'processed_texts.pickle', 'HashingVectorizer': <class 'sklearn.feature_extraction.text.HashingVectorizer'>, 'In': ['', \"# corpus = [dictionary.doc2bow(text) for text in...k\\ncorpus = corpora.mmcorpus.MmCorpus('spooky.mm')\", \"import numpy as np\\nimport matplotlib.pyplot as p...lib', 'inline')\\n\\npd.options.display.max_rows = 10\", '# Dataset\\n\\n# use this if in fmi-hw... repo\\n# tra...pe)\\nprint(set(train.columns) - set(test.columns))', 'train.head(5)', 'from sklearn.pipeline import Pipeline\\nfrom sklea...om sklearn.model_selection import cross_val_score', \"pipeline = Pipeline([\\n    ('features', CountVect...peline, train.text, train.author, cv=3, n_jobs=3)\", 'from sklearn.model_selection import cross_val_pr...peline, train.text, train.author, cv=3, n_jobs=3)', \"import itertools\\nfrom sklearn.metrics import con...l('True label')\\n    plt.xlabel('Predicted label')\", 'pipeline.fit(train.text, train.author)\\nauthors =...matrix(train.author, prediction, classes=authors)', 'plot_confusion_matrix(train.author, prediction, classes=authors, normalize=True)', \"from sklearn.ensemble import RandomForestClassif...         cv=3, n_jobs=3, scoring='neg_log_loss'))\", \"from sklearn.linear_model import LogisticRegress...log_loss'))\\n\\n# Получихме малко по-добри резултати\", 'explore = train.copy()', \"# # бр. думи в текста\\n# explore['words'] = explo... sum([c in string.punctuation for c in str(s)]) )\", \"import nltk\\n# nltk.download('stopwords')\\nstopwords = nltk.corpus.stopwords.words('english')\", \"import nltk\\nnltk.download('stopwords')\\nstopwords = nltk.corpus.stopwords.words('english')\", \"import nltk\\n# nltk.download('stopwords')\\nstopwords = nltk.corpus.stopwords.words('english')\", \"# features_names = list(set(explore.columns) - {...or'})\\n# print('numeric features', features_names)\", \"from nltk.stem import WordNetLemmatizer\\nfrom nlt....split()])) \\n\\nexplore[['stemmed', 'text']].head()\", ...], ...}\n        local_ns = None\n   1231             except:\n   1232                 self.shell.showtraceback()\n   1233                 return\n   1234             end = clock2()\n\n...........................................................................\n/home/bob/gits/spooky/<timed eval> in <module>()\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in cross_val_score(estimator=Pipeline(memory=None,\n     steps=[('bow', Text2B...B(alpha=1.0, class_prior=None, fit_prior=True))]), X=id\nid26305    This process, however, afforded me...d it ...\nName: text, Length: 19579, dtype: object, y=id\nid26305    EAP\nid17569    HPL\nid11008    EAP\n...   HPL\nName: author, Length: 19579, dtype: object, groups=None, scoring='neg_log_loss', cv=3, n_jobs=3, verbose=0, fit_params=None, pre_dispatch='2*n_jobs')\n    337     cv_results = cross_validate(estimator=estimator, X=X, y=y, groups=groups,\n    338                                 scoring={'score': scorer}, cv=cv,\n    339                                 return_train_score=False,\n    340                                 n_jobs=n_jobs, verbose=verbose,\n    341                                 fit_params=fit_params,\n--> 342                                 pre_dispatch=pre_dispatch)\n        pre_dispatch = '2*n_jobs'\n    343     return cv_results['test_score']\n    344 \n    345 \n    346 def _fit_and_score(estimator, X, y, scorer, train, test, verbose,\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in cross_validate(estimator=Pipeline(memory=None,\n     steps=[('bow', Text2B...B(alpha=1.0, class_prior=None, fit_prior=True))]), X=id\nid26305    This process, however, afforded me...d it ...\nName: text, Length: 19579, dtype: object, y=id\nid26305    EAP\nid17569    HPL\nid11008    EAP\n...   HPL\nName: author, Length: 19579, dtype: object, groups=None, scoring={'score': make_scorer(log_loss, greater_is_better=False, needs_proba=True)}, cv=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=3, verbose=0, fit_params=None, pre_dispatch='2*n_jobs', return_train_score=False)\n    201     scores = parallel(\n    202         delayed(_fit_and_score)(\n    203             clone(estimator), X, y, scorers, train, test, verbose, None,\n    204             fit_params, return_train_score=return_train_score,\n    205             return_times=True)\n--> 206         for train, test in cv.split(X, y, groups))\n        cv.split = <bound method StratifiedKFold.split of Stratifie...ld(n_splits=3, random_state=None, shuffle=False)>\n        X = id\nid26305    This process, however, afforded me...d it ...\nName: text, Length: 19579, dtype: object\n        y = id\nid26305    EAP\nid17569    HPL\nid11008    EAP\n...   HPL\nName: author, Length: 19579, dtype: object\n        groups = None\n    207 \n    208     if return_train_score:\n    209         train_scores, test_scores, fit_times, score_times = zip(*scores)\n    210         train_scores = _aggregate_score_dicts(train_scores)\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=3), iterable=<generator object cross_validate.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=3)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nValueError                                         Wed Jan  3 06:40:41 2018\nPID: 18886                                   Python 3.6.3: /usr/bin/python3\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(memory=None,\n     steps=[('bow', Text2B...B(alpha=1.0, class_prior=None, fit_prior=True))]), id\nid26305    This process, however, afforded me...d it ...\nName: text, Length: 19579, dtype: object, id\nid26305    EAP\nid17569    HPL\nid11008    EAP\n...   HPL\nName: author, Length: 19579, dtype: object, {'score': make_scorer(log_loss, greater_is_better=False, needs_proba=True)}, array([ 6475,  6476,  6478, ..., 19576, 19577, 19578]), array([   0,    1,    2, ..., 6604, 6605, 6606]), 0, None, None), {'return_times': True, 'return_train_score': False})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(memory=None,\n     steps=[('bow', Text2B...B(alpha=1.0, class_prior=None, fit_prior=True))]), id\nid26305    This process, however, afforded me...d it ...\nName: text, Length: 19579, dtype: object, id\nid26305    EAP\nid17569    HPL\nid11008    EAP\n...   HPL\nName: author, Length: 19579, dtype: object, {'score': make_scorer(log_loss, greater_is_better=False, needs_proba=True)}, array([ 6475,  6476,  6478, ..., 19576, 19577, 19578]), array([   0,    1,    2, ..., 6604, 6605, 6606]), 0, None, None)\n        kwargs = {'return_times': True, 'return_train_score': False}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(memory=None,\n     steps=[('bow', Text2B...B(alpha=1.0, class_prior=None, fit_prior=True))]), X=id\nid26305    This process, however, afforded me...d it ...\nName: text, Length: 19579, dtype: object, y=id\nid26305    EAP\nid17569    HPL\nid11008    EAP\n...   HPL\nName: author, Length: 19579, dtype: object, scorer={'score': make_scorer(log_loss, greater_is_better=False, needs_proba=True)}, train=array([ 6475,  6476,  6478, ..., 19576, 19577, 19578]), test=array([   0,    1,    2, ..., 6604, 6605, 6606]), verbose=0, parameters=None, fit_params={}, return_train_score=False, return_parameters=False, return_n_test_samples=False, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(memory=No...(alpha=1.0, class_prior=None, fit_prior=True))])>\n        X_train = id\nid15739    Its close resemblance to the medic...d it ...\nName: text, Length: 13051, dtype: object\n        y_train = id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   HPL\nName: author, Length: 13051, dtype: object\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/sklearn/pipeline.py in fit(self=Pipeline(memory=None,\n     steps=[('bow', Text2B...B(alpha=1.0, class_prior=None, fit_prior=True))]), X=id\nid15739    Its close resemblance to the medic...d it ...\nName: text, Length: 13051, dtype: object, y=id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   HPL\nName: author, Length: 13051, dtype: object, **fit_params={})\n    245         self : Pipeline\n    246             This estimator\n    247         \"\"\"\n    248         Xt, fit_params = self._fit(X, y, **fit_params)\n    249         if self._final_estimator is not None:\n--> 250             self._final_estimator.fit(Xt, y, **fit_params)\n        self._final_estimator.fit = <bound method BaseDiscreteNB.fit of MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)>\n        Xt = [[(0, 0.2664714100608566), (1, 0.10480362539462479), (2, 0.2604873336049293), (3, 0.2319432706761858), (4, 0.24213524135011738), (5, 0.41116785474457307), (6, 0.39218741029198784), (7, 0.32007681200061067), (8, 0.35974012618542445), (9, 0.3057814279583449), (10, 0.033103432446463785), (11, 0.07685359149299217), (12, 0.19308643986019788), (13, 0.2126248353265933)], [(10, 0.024133907128526488), (11, 0.056029761946444906), (14, 0.2713402113709543), (15, 0.3420085240826053), (16, 0.5718449017195636), (17, 0.2614985439879363), (18, 0.1808821162765368), (19, 0.036987739317106506), (20, 0.42712530611518384), (21, 0.3189304655793621), (22, 0.09341436285112015), (23, 0.27476798475726655), (24, 0.088498637497393)], [(1, 0.07932158066320133), (10, 0.06263658764531839), (14, 0.14084602912179758), (18, 0.09389145710463107), (19, 0.01919942563133236), (22, 0.048489097889497096), (25, 0.2722725986774012), (26, 0.2620800981425273), (27, 0.24487868329638712), (28, 0.2722725986774012), (29, 0.03505679663903942), (30, 0.08624781176709262), (31, 0.07149215764800944), (32, 0.31119614453633054), (33, 0.2729933988114348), (34, 0.1733099699406774), (35, 0.17712329620393766), (36, 0.06842298368233549), (37, 0.1420511395070622), (38, 0.14386314798381059), ...], [(10, 0.015973364734577944), (19, 0.0489617074991789), (23, 0.18185904232327854), (30, 0.1099730849576353), (47, 0.046070957177769906), (48, 0.2975393193706599), (49, 0.11089059498724518), (50, 0.3968007922798119), (51, 0.19827784646150795), (52, 0.07722160044388235), (53, 0.04857698480295758), (54, 0.42811430060066746), (55, 0.3236593651896635), (56, 0.07556909941755309), (57, 0.24915443219781466), (58, 0.42811430060066746), (59, 0.3240930775601281)], [(10, 0.03745602951273969), (36, 0.10229058576225049), (47, 0.10803203710707177), (60, 0.23619581984220353), (61, 0.03133470722492), (62, 0.385564388719468), (63, 0.3015083410363086), (64, 0.11103716027435812), (65, 0.1905968735808948), (66, 0.24536394387695537), (67, 0.5308001436909793), (68, 0.3703268648059433), (69, 0.23994513095105036), (70, 0.3078975881412962)], [(47, 0.07100101906724518), (53, 0.07486311627769447), (61, 0.04118771069619043), (71, 0.15202575486829917), (72, 0.14256352538466183), (73, 0.4667449719251434), (74, 0.2503651118140704), (75, 0.37022902619505615), (76, 0.5477253203876611), (77, 0.4760416328690717)], [(10, 0.06462580383426865), (11, 0.06430138284840814), (18, 0.06919519082403108), (22, 0.03573501237295617), (31, 0.10537515645034605), (37, 0.15703115077885102), (46, 0.07518496273136628), (64, 0.0547374449573064), (65, 0.04697880354149405), (78, 0.1734637042645781), (79, 0.21124370752304877), (80, 0.1237755155749664), (81, 0.15694812556791132), (82, 0.14377383733539228), (83, 0.21875527504189865), (84, 0.24744076215980707), (85, 0.10989353196818279), (86, 0.20541728310317606), (87, 0.1788334923026323), (88, 0.3009539216891794), ...], [(10, 0.01715833256303342), (19, 0.026296941004117013), (22, 0.06641422358292039), (24, 0.06291932116373902), (29, 0.0480163589636599), (30, 0.11813132649510569), (47, 0.049488684312218235), (51, 0.21298688698342833), (53, 0.052180619049802826), (56, 0.08117511625389999), (61, 0.028708399385876993), (102, 0.23403001292583833), (113, 0.20423929132245422), (115, 0.4262370557703321), (116, 0.2690731650842324), (117, 0.3589641175494357), (118, 0.17450791755009787), (119, 0.2001630105211242), (120, 0.303670599600059), (121, 0.4262370557703321), ...], [(10, 0.021322122227474257), (11, 0.04950186582869004), (30, 0.14679809784366432), (31, 0.12168323506929181), (47, 0.12299607457840946), (52, 0.20615924454556003), (61, 0.035675028352090576), (91, 0.14487860845275446), (98, 0.27840822828588596), (103, 0.23291869434274284), (124, 0.13839751535552838), (125, 0.3007913272538198), (126, 0.39081820673094264), (127, 0.2462174723129556), (128, 0.4123268436706392), (129, 0.22158417220596172), (130, 0.3435567610008015), (131, 0.18912652719692835), (132, 0.2306072899995085)], [(10, 0.060204206982548865), (19, 0.055361550083157234), (22, 0.09321221974181244), (49, 0.08359015703908927), (53, 0.036617693219445066), (61, 0.04029217669993592), (104, 0.0754428098308278), (133, 0.26790813350704007), (134, 0.18687533762152778), (135, 0.24430377753540683), (136, 0.23809501978731895), (137, 0.2755070388560312), (138, 0.21903792681162104), (139, 0.12747319308243932), (140, 0.12362178362577428), (141, 0.14439454632496657), (142, 0.22244615763028036), (143, 0.21903792681162104), (144, 0.23809501978731895), (145, 0.07431384829899074), ...], [(10, 0.017868951261395715), (11, 0.08296982996385277), (61, 0.02989736837972952), (72, 0.10348412582038423), (155, 0.31426513521849647), (156, 0.1699525813091037), (157, 0.1701766922652076), (158, 0.27640100939491724), (159, 0.2978192836150871), (160, 0.24276889717607655), (161, 0.33880121349699616), (162, 0.23450847942150704), (163, 0.3625537576651779), (164, 0.3183102485560416), (165, 0.24137147345766863), (166, 0.3738307494342447)], [(10, 0.04520429241093692), (11, 0.02623679756093562), (19, 0.017320077669206606), (22, 0.08748542354155527), (29, 0.06325047970506475), (53, 0.034367966016593844), (57, 0.08813773738110729), (61, 0.018908347820537255), (167, 0.10458059386108247), (168, 0.19113441661345087), (169, 0.3028886579507455), (170, 0.25858036692787534), (171, 0.13969408373222703), (172, 0.16532352454213142), (173, 0.16532352454213142), (174, 0.3028886579507455), (175, 0.19313267199821985), (176, 0.17097852648678477), (177, 0.2807345124393104), (178, 0.09429094471794976), ...], [(10, 0.009856156567640691), (15, 0.13967442332789326), (19, 0.04531677542111343), (24, 0.10842708839707535), (29, 0.11032697962119518), (41, 0.07260064341795866), (49, 0.06842359667072355), (56, 0.04662892808818644), (57, 0.1537374957667718), (61, 0.016490791171816524), (165, 0.13313568315005253), (190, 0.16755438923335436), (191, 0.13904086957076212), (192, 0.2448407583411407), (193, 0.12430484550933028), (194, 0.18687598151030094), (195, 0.15050032010750405), (196, 0.09143544591163232), (197, 0.12269104128755341), (198, 0.2641623506180873), ...], [(10, 0.05128666371555671), (11, 0.02381362904652451), (19, 0.047161308888427914), (22, 0.07940547692065152), (24, 0.037613469186254155), (30, 0.07061946834251158), (47, 0.05916914130722203), (61, 0.03432403515216796), (65, 0.05219494905222182), (66, 0.13438581975978053), (71, 0.06334580468218413), (72, 0.05940310075515709), (103, 0.05602454869351825), (111, 0.08056360687311669), (124, 0.0665782397585325), (131, 0.09098220614776903), (217, 0.13014327126494443), (218, 0.14707666165679775), (219, 0.1425038820600952), (220, 0.27491457850702283), ...], [(10, 0.03373914624704608), (11, 0.03916473868965497), (19, 0.05170877386415411), (22, 0.03264826570583768), (24, 0.06186044508432571), (29, 0.047208286451325736), (36, 0.046069979621768835), (45, 0.08087235171979418), (47, 0.024327916938460356), (53, 0.07695369862930257), (56, 0.07980901141908836), (61, 0.028225262619209712), (69, 0.1080672987732272), (112, 0.04557089762967087), (127, 0.09740080300544345), (241, 0.1796469603779444), (242, 0.08992259466649655), (243, 0.11166235358548782), (244, 0.15848012162533082), (245, 0.26548969682262125), ...], [(10, 0.014878436612043862), (11, 0.0690841526047578), (19, 0.022802761770881638), (24, 0.05455898049349969), (30, 0.10243474689030971), (45, 0.071326888352145), (47, 0.08582585164937677), (53, 0.045247172477722745), (56, 0.07038905541794807), (69, 0.09531198228070385), (104, 0.09322197900786375), (112, 0.08038421681121939), (131, 0.13197124641419022), (182, 0.17819993890438107), (185, 0.13080027223004068), (270, 0.18048741343046848), (271, 0.12834700317270165), (277, 0.21653463756000216), (278, 0.15765046010508932), (279, 0.3696012414193431), ...], [(10, 0.03791716652108541), (11, 0.05868616859386632), (22, 0.04892159860155609), (29, 0.1061084619891196), (36, 0.13806657118911214), (53, 0.03843693658103161), (64, 0.07493612378605678), (65, 0.06431446407942601), (91, 0.08587929261825251), (105, 0.3574063109864729), (141, 0.15156836850641137), (145, 0.07800591525639525), (213, 0.12076867033953673), (219, 0.17559286809079885), (222, 0.09905080713055688), (290, 0.17634543810717943), (291, 0.2354304202988956), (292, 0.23747359098140175), (293, 0.21838240905413028), (294, 0.2068871594723075), ...], [(10, 0.01568731468793941), (29, 0.08779964258937602), (37, 0.0889416637703053), (53, 0.04770707110604152), (61, 0.02624717137862027), (64, 0.09300905076906602), (237, 0.1379113333775812), (305, 0.30664896585612444), (306, 0.2833078792717857), (307, 0.20730843748050645), (308, 0.2233412719391391), (309, 0.38969490776368737), (310, 0.2695472861513626), (311, 0.21271630313917306), (312, 0.4204476556771751), (313, 0.20658700960294096), (314, 0.4204476556771751), (315, 0.14264507531499204)], [(10, 0.027389551203659726), (316, 0.7340881994597976), (317, 0.37735752159971553), (318, 0.5638844108421996)], [(18, 0.35789169848754704), (36, 0.13040599539512807), (52, 0.11542397475108049), (92, 0.1830990480525225), (130, 0.19234978763494665), (319, 0.37310009228736857), (320, 0.5931025045589059), (321, 0.2808020052524273), (322, 0.44859490237601146)], ...]\n        y = id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   HPL\nName: author, Length: 13051, dtype: object\n        fit_params = {}\n    251         return self\n    252 \n    253     def fit_transform(self, X, y=None, **fit_params):\n    254         \"\"\"Fit the model and transform with the final estimator\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/sklearn/naive_bayes.py in fit(self=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), X=[[(0, 0.2664714100608566), (1, 0.10480362539462479), (2, 0.2604873336049293), (3, 0.2319432706761858), (4, 0.24213524135011738), (5, 0.41116785474457307), (6, 0.39218741029198784), (7, 0.32007681200061067), (8, 0.35974012618542445), (9, 0.3057814279583449), (10, 0.033103432446463785), (11, 0.07685359149299217), (12, 0.19308643986019788), (13, 0.2126248353265933)], [(10, 0.024133907128526488), (11, 0.056029761946444906), (14, 0.2713402113709543), (15, 0.3420085240826053), (16, 0.5718449017195636), (17, 0.2614985439879363), (18, 0.1808821162765368), (19, 0.036987739317106506), (20, 0.42712530611518384), (21, 0.3189304655793621), (22, 0.09341436285112015), (23, 0.27476798475726655), (24, 0.088498637497393)], [(1, 0.07932158066320133), (10, 0.06263658764531839), (14, 0.14084602912179758), (18, 0.09389145710463107), (19, 0.01919942563133236), (22, 0.048489097889497096), (25, 0.2722725986774012), (26, 0.2620800981425273), (27, 0.24487868329638712), (28, 0.2722725986774012), (29, 0.03505679663903942), (30, 0.08624781176709262), (31, 0.07149215764800944), (32, 0.31119614453633054), (33, 0.2729933988114348), (34, 0.1733099699406774), (35, 0.17712329620393766), (36, 0.06842298368233549), (37, 0.1420511395070622), (38, 0.14386314798381059), ...], [(10, 0.015973364734577944), (19, 0.0489617074991789), (23, 0.18185904232327854), (30, 0.1099730849576353), (47, 0.046070957177769906), (48, 0.2975393193706599), (49, 0.11089059498724518), (50, 0.3968007922798119), (51, 0.19827784646150795), (52, 0.07722160044388235), (53, 0.04857698480295758), (54, 0.42811430060066746), (55, 0.3236593651896635), (56, 0.07556909941755309), (57, 0.24915443219781466), (58, 0.42811430060066746), (59, 0.3240930775601281)], [(10, 0.03745602951273969), (36, 0.10229058576225049), (47, 0.10803203710707177), (60, 0.23619581984220353), (61, 0.03133470722492), (62, 0.385564388719468), (63, 0.3015083410363086), (64, 0.11103716027435812), (65, 0.1905968735808948), (66, 0.24536394387695537), (67, 0.5308001436909793), (68, 0.3703268648059433), (69, 0.23994513095105036), (70, 0.3078975881412962)], [(47, 0.07100101906724518), (53, 0.07486311627769447), (61, 0.04118771069619043), (71, 0.15202575486829917), (72, 0.14256352538466183), (73, 0.4667449719251434), (74, 0.2503651118140704), (75, 0.37022902619505615), (76, 0.5477253203876611), (77, 0.4760416328690717)], [(10, 0.06462580383426865), (11, 0.06430138284840814), (18, 0.06919519082403108), (22, 0.03573501237295617), (31, 0.10537515645034605), (37, 0.15703115077885102), (46, 0.07518496273136628), (64, 0.0547374449573064), (65, 0.04697880354149405), (78, 0.1734637042645781), (79, 0.21124370752304877), (80, 0.1237755155749664), (81, 0.15694812556791132), (82, 0.14377383733539228), (83, 0.21875527504189865), (84, 0.24744076215980707), (85, 0.10989353196818279), (86, 0.20541728310317606), (87, 0.1788334923026323), (88, 0.3009539216891794), ...], [(10, 0.01715833256303342), (19, 0.026296941004117013), (22, 0.06641422358292039), (24, 0.06291932116373902), (29, 0.0480163589636599), (30, 0.11813132649510569), (47, 0.049488684312218235), (51, 0.21298688698342833), (53, 0.052180619049802826), (56, 0.08117511625389999), (61, 0.028708399385876993), (102, 0.23403001292583833), (113, 0.20423929132245422), (115, 0.4262370557703321), (116, 0.2690731650842324), (117, 0.3589641175494357), (118, 0.17450791755009787), (119, 0.2001630105211242), (120, 0.303670599600059), (121, 0.4262370557703321), ...], [(10, 0.021322122227474257), (11, 0.04950186582869004), (30, 0.14679809784366432), (31, 0.12168323506929181), (47, 0.12299607457840946), (52, 0.20615924454556003), (61, 0.035675028352090576), (91, 0.14487860845275446), (98, 0.27840822828588596), (103, 0.23291869434274284), (124, 0.13839751535552838), (125, 0.3007913272538198), (126, 0.39081820673094264), (127, 0.2462174723129556), (128, 0.4123268436706392), (129, 0.22158417220596172), (130, 0.3435567610008015), (131, 0.18912652719692835), (132, 0.2306072899995085)], [(10, 0.060204206982548865), (19, 0.055361550083157234), (22, 0.09321221974181244), (49, 0.08359015703908927), (53, 0.036617693219445066), (61, 0.04029217669993592), (104, 0.0754428098308278), (133, 0.26790813350704007), (134, 0.18687533762152778), (135, 0.24430377753540683), (136, 0.23809501978731895), (137, 0.2755070388560312), (138, 0.21903792681162104), (139, 0.12747319308243932), (140, 0.12362178362577428), (141, 0.14439454632496657), (142, 0.22244615763028036), (143, 0.21903792681162104), (144, 0.23809501978731895), (145, 0.07431384829899074), ...], [(10, 0.017868951261395715), (11, 0.08296982996385277), (61, 0.02989736837972952), (72, 0.10348412582038423), (155, 0.31426513521849647), (156, 0.1699525813091037), (157, 0.1701766922652076), (158, 0.27640100939491724), (159, 0.2978192836150871), (160, 0.24276889717607655), (161, 0.33880121349699616), (162, 0.23450847942150704), (163, 0.3625537576651779), (164, 0.3183102485560416), (165, 0.24137147345766863), (166, 0.3738307494342447)], [(10, 0.04520429241093692), (11, 0.02623679756093562), (19, 0.017320077669206606), (22, 0.08748542354155527), (29, 0.06325047970506475), (53, 0.034367966016593844), (57, 0.08813773738110729), (61, 0.018908347820537255), (167, 0.10458059386108247), (168, 0.19113441661345087), (169, 0.3028886579507455), (170, 0.25858036692787534), (171, 0.13969408373222703), (172, 0.16532352454213142), (173, 0.16532352454213142), (174, 0.3028886579507455), (175, 0.19313267199821985), (176, 0.17097852648678477), (177, 0.2807345124393104), (178, 0.09429094471794976), ...], [(10, 0.009856156567640691), (15, 0.13967442332789326), (19, 0.04531677542111343), (24, 0.10842708839707535), (29, 0.11032697962119518), (41, 0.07260064341795866), (49, 0.06842359667072355), (56, 0.04662892808818644), (57, 0.1537374957667718), (61, 0.016490791171816524), (165, 0.13313568315005253), (190, 0.16755438923335436), (191, 0.13904086957076212), (192, 0.2448407583411407), (193, 0.12430484550933028), (194, 0.18687598151030094), (195, 0.15050032010750405), (196, 0.09143544591163232), (197, 0.12269104128755341), (198, 0.2641623506180873), ...], [(10, 0.05128666371555671), (11, 0.02381362904652451), (19, 0.047161308888427914), (22, 0.07940547692065152), (24, 0.037613469186254155), (30, 0.07061946834251158), (47, 0.05916914130722203), (61, 0.03432403515216796), (65, 0.05219494905222182), (66, 0.13438581975978053), (71, 0.06334580468218413), (72, 0.05940310075515709), (103, 0.05602454869351825), (111, 0.08056360687311669), (124, 0.0665782397585325), (131, 0.09098220614776903), (217, 0.13014327126494443), (218, 0.14707666165679775), (219, 0.1425038820600952), (220, 0.27491457850702283), ...], [(10, 0.03373914624704608), (11, 0.03916473868965497), (19, 0.05170877386415411), (22, 0.03264826570583768), (24, 0.06186044508432571), (29, 0.047208286451325736), (36, 0.046069979621768835), (45, 0.08087235171979418), (47, 0.024327916938460356), (53, 0.07695369862930257), (56, 0.07980901141908836), (61, 0.028225262619209712), (69, 0.1080672987732272), (112, 0.04557089762967087), (127, 0.09740080300544345), (241, 0.1796469603779444), (242, 0.08992259466649655), (243, 0.11166235358548782), (244, 0.15848012162533082), (245, 0.26548969682262125), ...], [(10, 0.014878436612043862), (11, 0.0690841526047578), (19, 0.022802761770881638), (24, 0.05455898049349969), (30, 0.10243474689030971), (45, 0.071326888352145), (47, 0.08582585164937677), (53, 0.045247172477722745), (56, 0.07038905541794807), (69, 0.09531198228070385), (104, 0.09322197900786375), (112, 0.08038421681121939), (131, 0.13197124641419022), (182, 0.17819993890438107), (185, 0.13080027223004068), (270, 0.18048741343046848), (271, 0.12834700317270165), (277, 0.21653463756000216), (278, 0.15765046010508932), (279, 0.3696012414193431), ...], [(10, 0.03791716652108541), (11, 0.05868616859386632), (22, 0.04892159860155609), (29, 0.1061084619891196), (36, 0.13806657118911214), (53, 0.03843693658103161), (64, 0.07493612378605678), (65, 0.06431446407942601), (91, 0.08587929261825251), (105, 0.3574063109864729), (141, 0.15156836850641137), (145, 0.07800591525639525), (213, 0.12076867033953673), (219, 0.17559286809079885), (222, 0.09905080713055688), (290, 0.17634543810717943), (291, 0.2354304202988956), (292, 0.23747359098140175), (293, 0.21838240905413028), (294, 0.2068871594723075), ...], [(10, 0.01568731468793941), (29, 0.08779964258937602), (37, 0.0889416637703053), (53, 0.04770707110604152), (61, 0.02624717137862027), (64, 0.09300905076906602), (237, 0.1379113333775812), (305, 0.30664896585612444), (306, 0.2833078792717857), (307, 0.20730843748050645), (308, 0.2233412719391391), (309, 0.38969490776368737), (310, 0.2695472861513626), (311, 0.21271630313917306), (312, 0.4204476556771751), (313, 0.20658700960294096), (314, 0.4204476556771751), (315, 0.14264507531499204)], [(10, 0.027389551203659726), (316, 0.7340881994597976), (317, 0.37735752159971553), (318, 0.5638844108421996)], [(18, 0.35789169848754704), (36, 0.13040599539512807), (52, 0.11542397475108049), (92, 0.1830990480525225), (130, 0.19234978763494665), (319, 0.37310009228736857), (320, 0.5931025045589059), (321, 0.2808020052524273), (322, 0.44859490237601146)], ...], y=id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   HPL\nName: author, Length: 13051, dtype: object, sample_weight=None)\n    574         Returns\n    575         -------\n    576         self : object\n    577             Returns self.\n    578         \"\"\"\n--> 579         X, y = check_X_y(X, y, 'csr')\n        X = [[(0, 0.2664714100608566), (1, 0.10480362539462479), (2, 0.2604873336049293), (3, 0.2319432706761858), (4, 0.24213524135011738), (5, 0.41116785474457307), (6, 0.39218741029198784), (7, 0.32007681200061067), (8, 0.35974012618542445), (9, 0.3057814279583449), (10, 0.033103432446463785), (11, 0.07685359149299217), (12, 0.19308643986019788), (13, 0.2126248353265933)], [(10, 0.024133907128526488), (11, 0.056029761946444906), (14, 0.2713402113709543), (15, 0.3420085240826053), (16, 0.5718449017195636), (17, 0.2614985439879363), (18, 0.1808821162765368), (19, 0.036987739317106506), (20, 0.42712530611518384), (21, 0.3189304655793621), (22, 0.09341436285112015), (23, 0.27476798475726655), (24, 0.088498637497393)], [(1, 0.07932158066320133), (10, 0.06263658764531839), (14, 0.14084602912179758), (18, 0.09389145710463107), (19, 0.01919942563133236), (22, 0.048489097889497096), (25, 0.2722725986774012), (26, 0.2620800981425273), (27, 0.24487868329638712), (28, 0.2722725986774012), (29, 0.03505679663903942), (30, 0.08624781176709262), (31, 0.07149215764800944), (32, 0.31119614453633054), (33, 0.2729933988114348), (34, 0.1733099699406774), (35, 0.17712329620393766), (36, 0.06842298368233549), (37, 0.1420511395070622), (38, 0.14386314798381059), ...], [(10, 0.015973364734577944), (19, 0.0489617074991789), (23, 0.18185904232327854), (30, 0.1099730849576353), (47, 0.046070957177769906), (48, 0.2975393193706599), (49, 0.11089059498724518), (50, 0.3968007922798119), (51, 0.19827784646150795), (52, 0.07722160044388235), (53, 0.04857698480295758), (54, 0.42811430060066746), (55, 0.3236593651896635), (56, 0.07556909941755309), (57, 0.24915443219781466), (58, 0.42811430060066746), (59, 0.3240930775601281)], [(10, 0.03745602951273969), (36, 0.10229058576225049), (47, 0.10803203710707177), (60, 0.23619581984220353), (61, 0.03133470722492), (62, 0.385564388719468), (63, 0.3015083410363086), (64, 0.11103716027435812), (65, 0.1905968735808948), (66, 0.24536394387695537), (67, 0.5308001436909793), (68, 0.3703268648059433), (69, 0.23994513095105036), (70, 0.3078975881412962)], [(47, 0.07100101906724518), (53, 0.07486311627769447), (61, 0.04118771069619043), (71, 0.15202575486829917), (72, 0.14256352538466183), (73, 0.4667449719251434), (74, 0.2503651118140704), (75, 0.37022902619505615), (76, 0.5477253203876611), (77, 0.4760416328690717)], [(10, 0.06462580383426865), (11, 0.06430138284840814), (18, 0.06919519082403108), (22, 0.03573501237295617), (31, 0.10537515645034605), (37, 0.15703115077885102), (46, 0.07518496273136628), (64, 0.0547374449573064), (65, 0.04697880354149405), (78, 0.1734637042645781), (79, 0.21124370752304877), (80, 0.1237755155749664), (81, 0.15694812556791132), (82, 0.14377383733539228), (83, 0.21875527504189865), (84, 0.24744076215980707), (85, 0.10989353196818279), (86, 0.20541728310317606), (87, 0.1788334923026323), (88, 0.3009539216891794), ...], [(10, 0.01715833256303342), (19, 0.026296941004117013), (22, 0.06641422358292039), (24, 0.06291932116373902), (29, 0.0480163589636599), (30, 0.11813132649510569), (47, 0.049488684312218235), (51, 0.21298688698342833), (53, 0.052180619049802826), (56, 0.08117511625389999), (61, 0.028708399385876993), (102, 0.23403001292583833), (113, 0.20423929132245422), (115, 0.4262370557703321), (116, 0.2690731650842324), (117, 0.3589641175494357), (118, 0.17450791755009787), (119, 0.2001630105211242), (120, 0.303670599600059), (121, 0.4262370557703321), ...], [(10, 0.021322122227474257), (11, 0.04950186582869004), (30, 0.14679809784366432), (31, 0.12168323506929181), (47, 0.12299607457840946), (52, 0.20615924454556003), (61, 0.035675028352090576), (91, 0.14487860845275446), (98, 0.27840822828588596), (103, 0.23291869434274284), (124, 0.13839751535552838), (125, 0.3007913272538198), (126, 0.39081820673094264), (127, 0.2462174723129556), (128, 0.4123268436706392), (129, 0.22158417220596172), (130, 0.3435567610008015), (131, 0.18912652719692835), (132, 0.2306072899995085)], [(10, 0.060204206982548865), (19, 0.055361550083157234), (22, 0.09321221974181244), (49, 0.08359015703908927), (53, 0.036617693219445066), (61, 0.04029217669993592), (104, 0.0754428098308278), (133, 0.26790813350704007), (134, 0.18687533762152778), (135, 0.24430377753540683), (136, 0.23809501978731895), (137, 0.2755070388560312), (138, 0.21903792681162104), (139, 0.12747319308243932), (140, 0.12362178362577428), (141, 0.14439454632496657), (142, 0.22244615763028036), (143, 0.21903792681162104), (144, 0.23809501978731895), (145, 0.07431384829899074), ...], [(10, 0.017868951261395715), (11, 0.08296982996385277), (61, 0.02989736837972952), (72, 0.10348412582038423), (155, 0.31426513521849647), (156, 0.1699525813091037), (157, 0.1701766922652076), (158, 0.27640100939491724), (159, 0.2978192836150871), (160, 0.24276889717607655), (161, 0.33880121349699616), (162, 0.23450847942150704), (163, 0.3625537576651779), (164, 0.3183102485560416), (165, 0.24137147345766863), (166, 0.3738307494342447)], [(10, 0.04520429241093692), (11, 0.02623679756093562), (19, 0.017320077669206606), (22, 0.08748542354155527), (29, 0.06325047970506475), (53, 0.034367966016593844), (57, 0.08813773738110729), (61, 0.018908347820537255), (167, 0.10458059386108247), (168, 0.19113441661345087), (169, 0.3028886579507455), (170, 0.25858036692787534), (171, 0.13969408373222703), (172, 0.16532352454213142), (173, 0.16532352454213142), (174, 0.3028886579507455), (175, 0.19313267199821985), (176, 0.17097852648678477), (177, 0.2807345124393104), (178, 0.09429094471794976), ...], [(10, 0.009856156567640691), (15, 0.13967442332789326), (19, 0.04531677542111343), (24, 0.10842708839707535), (29, 0.11032697962119518), (41, 0.07260064341795866), (49, 0.06842359667072355), (56, 0.04662892808818644), (57, 0.1537374957667718), (61, 0.016490791171816524), (165, 0.13313568315005253), (190, 0.16755438923335436), (191, 0.13904086957076212), (192, 0.2448407583411407), (193, 0.12430484550933028), (194, 0.18687598151030094), (195, 0.15050032010750405), (196, 0.09143544591163232), (197, 0.12269104128755341), (198, 0.2641623506180873), ...], [(10, 0.05128666371555671), (11, 0.02381362904652451), (19, 0.047161308888427914), (22, 0.07940547692065152), (24, 0.037613469186254155), (30, 0.07061946834251158), (47, 0.05916914130722203), (61, 0.03432403515216796), (65, 0.05219494905222182), (66, 0.13438581975978053), (71, 0.06334580468218413), (72, 0.05940310075515709), (103, 0.05602454869351825), (111, 0.08056360687311669), (124, 0.0665782397585325), (131, 0.09098220614776903), (217, 0.13014327126494443), (218, 0.14707666165679775), (219, 0.1425038820600952), (220, 0.27491457850702283), ...], [(10, 0.03373914624704608), (11, 0.03916473868965497), (19, 0.05170877386415411), (22, 0.03264826570583768), (24, 0.06186044508432571), (29, 0.047208286451325736), (36, 0.046069979621768835), (45, 0.08087235171979418), (47, 0.024327916938460356), (53, 0.07695369862930257), (56, 0.07980901141908836), (61, 0.028225262619209712), (69, 0.1080672987732272), (112, 0.04557089762967087), (127, 0.09740080300544345), (241, 0.1796469603779444), (242, 0.08992259466649655), (243, 0.11166235358548782), (244, 0.15848012162533082), (245, 0.26548969682262125), ...], [(10, 0.014878436612043862), (11, 0.0690841526047578), (19, 0.022802761770881638), (24, 0.05455898049349969), (30, 0.10243474689030971), (45, 0.071326888352145), (47, 0.08582585164937677), (53, 0.045247172477722745), (56, 0.07038905541794807), (69, 0.09531198228070385), (104, 0.09322197900786375), (112, 0.08038421681121939), (131, 0.13197124641419022), (182, 0.17819993890438107), (185, 0.13080027223004068), (270, 0.18048741343046848), (271, 0.12834700317270165), (277, 0.21653463756000216), (278, 0.15765046010508932), (279, 0.3696012414193431), ...], [(10, 0.03791716652108541), (11, 0.05868616859386632), (22, 0.04892159860155609), (29, 0.1061084619891196), (36, 0.13806657118911214), (53, 0.03843693658103161), (64, 0.07493612378605678), (65, 0.06431446407942601), (91, 0.08587929261825251), (105, 0.3574063109864729), (141, 0.15156836850641137), (145, 0.07800591525639525), (213, 0.12076867033953673), (219, 0.17559286809079885), (222, 0.09905080713055688), (290, 0.17634543810717943), (291, 0.2354304202988956), (292, 0.23747359098140175), (293, 0.21838240905413028), (294, 0.2068871594723075), ...], [(10, 0.01568731468793941), (29, 0.08779964258937602), (37, 0.0889416637703053), (53, 0.04770707110604152), (61, 0.02624717137862027), (64, 0.09300905076906602), (237, 0.1379113333775812), (305, 0.30664896585612444), (306, 0.2833078792717857), (307, 0.20730843748050645), (308, 0.2233412719391391), (309, 0.38969490776368737), (310, 0.2695472861513626), (311, 0.21271630313917306), (312, 0.4204476556771751), (313, 0.20658700960294096), (314, 0.4204476556771751), (315, 0.14264507531499204)], [(10, 0.027389551203659726), (316, 0.7340881994597976), (317, 0.37735752159971553), (318, 0.5638844108421996)], [(18, 0.35789169848754704), (36, 0.13040599539512807), (52, 0.11542397475108049), (92, 0.1830990480525225), (130, 0.19234978763494665), (319, 0.37310009228736857), (320, 0.5931025045589059), (321, 0.2808020052524273), (322, 0.44859490237601146)], ...]\n        y = id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   HPL\nName: author, Length: 13051, dtype: object\n    580         _, n_features = X.shape\n    581 \n    582         labelbin = LabelBinarizer()\n    583         Y = labelbin.fit_transform(y)\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/sklearn/utils/validation.py in check_X_y(X=[[(0, 0.2664714100608566), (1, 0.10480362539462479), (2, 0.2604873336049293), (3, 0.2319432706761858), (4, 0.24213524135011738), (5, 0.41116785474457307), (6, 0.39218741029198784), (7, 0.32007681200061067), (8, 0.35974012618542445), (9, 0.3057814279583449), (10, 0.033103432446463785), (11, 0.07685359149299217), (12, 0.19308643986019788), (13, 0.2126248353265933)], [(10, 0.024133907128526488), (11, 0.056029761946444906), (14, 0.2713402113709543), (15, 0.3420085240826053), (16, 0.5718449017195636), (17, 0.2614985439879363), (18, 0.1808821162765368), (19, 0.036987739317106506), (20, 0.42712530611518384), (21, 0.3189304655793621), (22, 0.09341436285112015), (23, 0.27476798475726655), (24, 0.088498637497393)], [(1, 0.07932158066320133), (10, 0.06263658764531839), (14, 0.14084602912179758), (18, 0.09389145710463107), (19, 0.01919942563133236), (22, 0.048489097889497096), (25, 0.2722725986774012), (26, 0.2620800981425273), (27, 0.24487868329638712), (28, 0.2722725986774012), (29, 0.03505679663903942), (30, 0.08624781176709262), (31, 0.07149215764800944), (32, 0.31119614453633054), (33, 0.2729933988114348), (34, 0.1733099699406774), (35, 0.17712329620393766), (36, 0.06842298368233549), (37, 0.1420511395070622), (38, 0.14386314798381059), ...], [(10, 0.015973364734577944), (19, 0.0489617074991789), (23, 0.18185904232327854), (30, 0.1099730849576353), (47, 0.046070957177769906), (48, 0.2975393193706599), (49, 0.11089059498724518), (50, 0.3968007922798119), (51, 0.19827784646150795), (52, 0.07722160044388235), (53, 0.04857698480295758), (54, 0.42811430060066746), (55, 0.3236593651896635), (56, 0.07556909941755309), (57, 0.24915443219781466), (58, 0.42811430060066746), (59, 0.3240930775601281)], [(10, 0.03745602951273969), (36, 0.10229058576225049), (47, 0.10803203710707177), (60, 0.23619581984220353), (61, 0.03133470722492), (62, 0.385564388719468), (63, 0.3015083410363086), (64, 0.11103716027435812), (65, 0.1905968735808948), (66, 0.24536394387695537), (67, 0.5308001436909793), (68, 0.3703268648059433), (69, 0.23994513095105036), (70, 0.3078975881412962)], [(47, 0.07100101906724518), (53, 0.07486311627769447), (61, 0.04118771069619043), (71, 0.15202575486829917), (72, 0.14256352538466183), (73, 0.4667449719251434), (74, 0.2503651118140704), (75, 0.37022902619505615), (76, 0.5477253203876611), (77, 0.4760416328690717)], [(10, 0.06462580383426865), (11, 0.06430138284840814), (18, 0.06919519082403108), (22, 0.03573501237295617), (31, 0.10537515645034605), (37, 0.15703115077885102), (46, 0.07518496273136628), (64, 0.0547374449573064), (65, 0.04697880354149405), (78, 0.1734637042645781), (79, 0.21124370752304877), (80, 0.1237755155749664), (81, 0.15694812556791132), (82, 0.14377383733539228), (83, 0.21875527504189865), (84, 0.24744076215980707), (85, 0.10989353196818279), (86, 0.20541728310317606), (87, 0.1788334923026323), (88, 0.3009539216891794), ...], [(10, 0.01715833256303342), (19, 0.026296941004117013), (22, 0.06641422358292039), (24, 0.06291932116373902), (29, 0.0480163589636599), (30, 0.11813132649510569), (47, 0.049488684312218235), (51, 0.21298688698342833), (53, 0.052180619049802826), (56, 0.08117511625389999), (61, 0.028708399385876993), (102, 0.23403001292583833), (113, 0.20423929132245422), (115, 0.4262370557703321), (116, 0.2690731650842324), (117, 0.3589641175494357), (118, 0.17450791755009787), (119, 0.2001630105211242), (120, 0.303670599600059), (121, 0.4262370557703321), ...], [(10, 0.021322122227474257), (11, 0.04950186582869004), (30, 0.14679809784366432), (31, 0.12168323506929181), (47, 0.12299607457840946), (52, 0.20615924454556003), (61, 0.035675028352090576), (91, 0.14487860845275446), (98, 0.27840822828588596), (103, 0.23291869434274284), (124, 0.13839751535552838), (125, 0.3007913272538198), (126, 0.39081820673094264), (127, 0.2462174723129556), (128, 0.4123268436706392), (129, 0.22158417220596172), (130, 0.3435567610008015), (131, 0.18912652719692835), (132, 0.2306072899995085)], [(10, 0.060204206982548865), (19, 0.055361550083157234), (22, 0.09321221974181244), (49, 0.08359015703908927), (53, 0.036617693219445066), (61, 0.04029217669993592), (104, 0.0754428098308278), (133, 0.26790813350704007), (134, 0.18687533762152778), (135, 0.24430377753540683), (136, 0.23809501978731895), (137, 0.2755070388560312), (138, 0.21903792681162104), (139, 0.12747319308243932), (140, 0.12362178362577428), (141, 0.14439454632496657), (142, 0.22244615763028036), (143, 0.21903792681162104), (144, 0.23809501978731895), (145, 0.07431384829899074), ...], [(10, 0.017868951261395715), (11, 0.08296982996385277), (61, 0.02989736837972952), (72, 0.10348412582038423), (155, 0.31426513521849647), (156, 0.1699525813091037), (157, 0.1701766922652076), (158, 0.27640100939491724), (159, 0.2978192836150871), (160, 0.24276889717607655), (161, 0.33880121349699616), (162, 0.23450847942150704), (163, 0.3625537576651779), (164, 0.3183102485560416), (165, 0.24137147345766863), (166, 0.3738307494342447)], [(10, 0.04520429241093692), (11, 0.02623679756093562), (19, 0.017320077669206606), (22, 0.08748542354155527), (29, 0.06325047970506475), (53, 0.034367966016593844), (57, 0.08813773738110729), (61, 0.018908347820537255), (167, 0.10458059386108247), (168, 0.19113441661345087), (169, 0.3028886579507455), (170, 0.25858036692787534), (171, 0.13969408373222703), (172, 0.16532352454213142), (173, 0.16532352454213142), (174, 0.3028886579507455), (175, 0.19313267199821985), (176, 0.17097852648678477), (177, 0.2807345124393104), (178, 0.09429094471794976), ...], [(10, 0.009856156567640691), (15, 0.13967442332789326), (19, 0.04531677542111343), (24, 0.10842708839707535), (29, 0.11032697962119518), (41, 0.07260064341795866), (49, 0.06842359667072355), (56, 0.04662892808818644), (57, 0.1537374957667718), (61, 0.016490791171816524), (165, 0.13313568315005253), (190, 0.16755438923335436), (191, 0.13904086957076212), (192, 0.2448407583411407), (193, 0.12430484550933028), (194, 0.18687598151030094), (195, 0.15050032010750405), (196, 0.09143544591163232), (197, 0.12269104128755341), (198, 0.2641623506180873), ...], [(10, 0.05128666371555671), (11, 0.02381362904652451), (19, 0.047161308888427914), (22, 0.07940547692065152), (24, 0.037613469186254155), (30, 0.07061946834251158), (47, 0.05916914130722203), (61, 0.03432403515216796), (65, 0.05219494905222182), (66, 0.13438581975978053), (71, 0.06334580468218413), (72, 0.05940310075515709), (103, 0.05602454869351825), (111, 0.08056360687311669), (124, 0.0665782397585325), (131, 0.09098220614776903), (217, 0.13014327126494443), (218, 0.14707666165679775), (219, 0.1425038820600952), (220, 0.27491457850702283), ...], [(10, 0.03373914624704608), (11, 0.03916473868965497), (19, 0.05170877386415411), (22, 0.03264826570583768), (24, 0.06186044508432571), (29, 0.047208286451325736), (36, 0.046069979621768835), (45, 0.08087235171979418), (47, 0.024327916938460356), (53, 0.07695369862930257), (56, 0.07980901141908836), (61, 0.028225262619209712), (69, 0.1080672987732272), (112, 0.04557089762967087), (127, 0.09740080300544345), (241, 0.1796469603779444), (242, 0.08992259466649655), (243, 0.11166235358548782), (244, 0.15848012162533082), (245, 0.26548969682262125), ...], [(10, 0.014878436612043862), (11, 0.0690841526047578), (19, 0.022802761770881638), (24, 0.05455898049349969), (30, 0.10243474689030971), (45, 0.071326888352145), (47, 0.08582585164937677), (53, 0.045247172477722745), (56, 0.07038905541794807), (69, 0.09531198228070385), (104, 0.09322197900786375), (112, 0.08038421681121939), (131, 0.13197124641419022), (182, 0.17819993890438107), (185, 0.13080027223004068), (270, 0.18048741343046848), (271, 0.12834700317270165), (277, 0.21653463756000216), (278, 0.15765046010508932), (279, 0.3696012414193431), ...], [(10, 0.03791716652108541), (11, 0.05868616859386632), (22, 0.04892159860155609), (29, 0.1061084619891196), (36, 0.13806657118911214), (53, 0.03843693658103161), (64, 0.07493612378605678), (65, 0.06431446407942601), (91, 0.08587929261825251), (105, 0.3574063109864729), (141, 0.15156836850641137), (145, 0.07800591525639525), (213, 0.12076867033953673), (219, 0.17559286809079885), (222, 0.09905080713055688), (290, 0.17634543810717943), (291, 0.2354304202988956), (292, 0.23747359098140175), (293, 0.21838240905413028), (294, 0.2068871594723075), ...], [(10, 0.01568731468793941), (29, 0.08779964258937602), (37, 0.0889416637703053), (53, 0.04770707110604152), (61, 0.02624717137862027), (64, 0.09300905076906602), (237, 0.1379113333775812), (305, 0.30664896585612444), (306, 0.2833078792717857), (307, 0.20730843748050645), (308, 0.2233412719391391), (309, 0.38969490776368737), (310, 0.2695472861513626), (311, 0.21271630313917306), (312, 0.4204476556771751), (313, 0.20658700960294096), (314, 0.4204476556771751), (315, 0.14264507531499204)], [(10, 0.027389551203659726), (316, 0.7340881994597976), (317, 0.37735752159971553), (318, 0.5638844108421996)], [(18, 0.35789169848754704), (36, 0.13040599539512807), (52, 0.11542397475108049), (92, 0.1830990480525225), (130, 0.19234978763494665), (319, 0.37310009228736857), (320, 0.5931025045589059), (321, 0.2808020052524273), (322, 0.44859490237601146)], ...], y=id\nid15739    EAP\nid19588    EAP\nid11059    EAP\n...   HPL\nName: author, Length: 13051, dtype: object, accept_sparse='csr', dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, multi_output=False, ensure_min_samples=1, ensure_min_features=1, y_numeric=False, warn_on_dtype=False, estimator=None)\n    568     y_converted : object\n    569         The converted and validated y.\n    570     \"\"\"\n    571     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n    572                     ensure_2d, allow_nd, ensure_min_samples,\n--> 573                     ensure_min_features, warn_on_dtype, estimator)\n        ensure_min_features = 1\n        warn_on_dtype = False\n        estimator = None\n    574     if multi_output:\n    575         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n    576                         dtype=None)\n    577     else:\n\n...........................................................................\n/home/bob/.local/lib/python3.6/site-packages/sklearn/utils/validation.py in check_array(array=array([ list([(0, 0.2664714100608566), (1, 0.104...), (11075, 0.41154013028457553)])], dtype=object), accept_sparse='csr', dtype=None, order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=False, estimator=None)\n    436             if array.ndim == 1:\n    437                 raise ValueError(\n    438                     \"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\n    439                     \"Reshape your data either using array.reshape(-1, 1) if \"\n    440                     \"your data has a single feature or array.reshape(1, -1) \"\n--> 441                     \"if it contains a single sample.\".format(array))\n        array = array([ list([(0, 0.2664714100608566), (1, 0.104...), (11075, 0.41154013028457553)])], dtype=object)\n    442             array = np.atleast_2d(array)\n    443             # To ensure that array flags are maintained\n    444             array = np.array(array, dtype=dtype, order=order, copy=copy)\n    445 \n\nValueError: Expected 2D array, got 1D array instead:\narray=[ list([(0, 0.2664714100608566), (1, 0.10480362539462479), (2, 0.2604873336049293), (3, 0.2319432706761858), (4, 0.24213524135011738), (5, 0.41116785474457307), (6, 0.39218741029198784), (7, 0.32007681200061067), (8, 0.35974012618542445), (9, 0.3057814279583449), (10, 0.033103432446463785), (11, 0.07685359149299217), (12, 0.19308643986019788), (13, 0.2126248353265933)])\n list([(10, 0.024133907128526488), (11, 0.056029761946444906), (14, 0.2713402113709543), (15, 0.3420085240826053), (16, 0.5718449017195636), (17, 0.2614985439879363), (18, 0.1808821162765368), (19, 0.036987739317106506), (20, 0.42712530611518384), (21, 0.3189304655793621), (22, 0.09341436285112015), (23, 0.27476798475726655), (24, 0.088498637497393)])\n list([(1, 0.07932158066320133), (10, 0.06263658764531839), (14, 0.14084602912179758), (18, 0.09389145710463107), (19, 0.01919942563133236), (22, 0.048489097889497096), (25, 0.2722725986774012), (26, 0.2620800981425273), (27, 0.24487868329638712), (28, 0.2722725986774012), (29, 0.03505679663903942), (30, 0.08624781176709262), (31, 0.07149215764800944), (32, 0.31119614453633054), (33, 0.2729933988114348), (34, 0.1733099699406774), (35, 0.17712329620393766), (36, 0.06842298368233549), (37, 0.1420511395070622), (38, 0.14386314798381059), (39, 0.2050581341267459), (40, 0.23334905281847185), (41, 0.09227646767474229), (42, 0.21187381993571314), (43, 0.3357541677332322), (44, 0.3357541677332322), (45, 0.06005567668474601), (46, 0.10201902211900157)])\n ...,\n list([(11, 0.03483379824389391), (22, 0.05807586817427632), (29, 0.041987869208477496), (145, 0.09260247785729721), (208, 0.1530751157324031), (1037, 0.16584589431194527), (6507, 0.28722118181246425), (8178, 0.3338402944907435), (16710, 0.37272267484996136), (20408, 0.37272267484996136), (20543, 0.37272267484996136), (23339, 0.4021360601237881), (23340, 0.4021360601237881)])\n list([(19, 0.028810064274408482), (24, 0.06893234032603549), (49, 0.13050056185493572), (60, 0.2370797936612917), (65, 0.19131019122706705), (69, 0.12042156837049621), (154, 0.19044775173097933), (237, 0.16525911588786907), (1154, 0.18801448431613588), (3699, 0.28355351691441877), (8329, 0.4085638403294151), (8330, 0.35641827923263075), (11702, 0.44541485037215933), (15323, 0.44541485037215933)])\n list([(11, 0.04032298873683362), (19, 0.02661899933307242), (22, 0.13445519560495678), (24, 0.06368989335423944), (29, 0.048604414750374345), (56, 0.08216926695338246), (61, 0.029059990817433712), (65, 0.08838032786353116), (103, 0.09486488773052526), (104, 0.10882347594433836), (178, 0.14491451149923437), (230, 0.10170684012562291), (360, 0.15235494384947212), (395, 0.363360346899261), (988, 0.17705848089319678), (1512, 0.2984337453903016), (3656, 0.39740876145355286), (3947, 0.3699196063748875), (8512, 0.25681322148061364), (10438, 0.3159541460973265), (11075, 0.41154013028457553)])].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n___________________________________________________________________________"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(cross_val_score(pipeline, train.text, train.author, \n",
    "                cv=3, n_jobs=3, scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-42-a2f04ec58aab>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-42-a2f04ec58aab>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    fail here\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "fail here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Откри приблизително същите параметри, но не успя да стигне напълно до същия резултат.\n",
    "\n",
    "Ще използвам следния модел:\n",
    "\n",
    "TfIdf + MultinomialNB, без стеминг на текста.\n",
    "\n",
    "Mean validation score: -0.423 (std: 0.003)\n",
    "\n",
    "Ще ползвам и следните параметри:\n",
    "\n",
    "Parameters: {'features__stop_words': None, 'features__ngram_range': (1, 2), 'features__min_df': 2, 'features__max_df': 0.8, 'features__lowercase': False, 'features__analyzer': 'word', 'clf__alpha': 0.01}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Последна проверка на този модел за `LogLoss` и `Accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', TfidfVectorizer(ngram_range=(1, 2), min_df=2,\n",
    "                                 max_df=0.8, lowercase=False)),\n",
    "    ('clf', MultinomialNB(alpha=0.01))\n",
    "])\n",
    "\n",
    "print(cross_val_score(pipeline, train.text, train.author, cv=3, n_jobs=3))\n",
    "print(cross_val_score(pipeline, train.text, train.author, cv=3, n_jobs=3, \n",
    "                      scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Трениране на модел и събмит\n",
    "\n",
    "Първо да видим в какъв формат трябва да се подадат резултатите за тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"data/spooky-authors/sample_submission.zip\")\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = pipeline.fit(train.text, train.author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(pipeline.predict_proba(test[:10].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "test_predictions = pipeline.predict_proba(test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(pipeline.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "submit_file = pd.DataFrame(test_predictions, columns=['EAP', 'MWS', 'HPL'], index=test.index)\n",
    "submit_file.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "submit_file.to_csv(\"data/spooky-authors/submit_Tfidf_MNB_text.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Очакванията за събмита са да имаме скор някъде около 0.41 - 0.42.\n",
    "\n",
    "Може да е малко по-добър защото при крос-валидацията тренирахме на 13к и тествахме 6к.\n",
    "\n",
    "Сега трейн сета е целия: 19.5к"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![submit-result.png](attachment:submit-result.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Да хакнем ранкинга в кагъл?\n",
    "\n",
    "print(test.text[:5].values)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "none",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
