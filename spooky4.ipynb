{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import mglearn\n",
    "from IPython.display import display\n",
    "\n",
    "#%matplotlib notebook\n",
    "\n",
    "import sklearn\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_rows = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 2) (8392, 1) (8392, 3)\n",
      "{'author'}\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "\n",
    "# use this if in fmi-hw... repo\n",
    "# train = pd.read_csv(\"data/spooky/train.zip\", index_col=['id'])\n",
    "# test = pd.read_csv(\"data/spooky/test.zip\", index_col=['id'])\n",
    "# sample_submission = pd.read_csv(\"data/spooky/sample_submission.zip\", index_col=['id'])\n",
    "\n",
    "train = pd.read_csv(\"data/train.zip\", index_col=['id'])\n",
    "test = pd.read_csv(\"data/test.zip\", index_col=['id'])\n",
    "sample_submission = pd.read_csv(\"data/sample_submission.zip\", index_col=['id'])\n",
    "\n",
    "\n",
    "print(train.shape, test.shape, sample_submission.shape)\n",
    "print(set(train.columns) - set(test.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ~~Първо - baseline модел~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stemmed</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id26305</th>\n",
       "      <td>thi process, however, afford me no mean of asc...</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id17569</th>\n",
       "      <td>It never onc occur to me that the fumbl might ...</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id11008</th>\n",
       "      <td>In hi left hand wa a gold snuff box, from whic...</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id27763</th>\n",
       "      <td>how love is spring As we look from windsor ter...</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id12958</th>\n",
       "      <td>find noth else, not even gold, the superintend...</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   stemmed  \\\n",
       "id                                                           \n",
       "id26305  thi process, however, afford me no mean of asc...   \n",
       "id17569  It never onc occur to me that the fumbl might ...   \n",
       "id11008  In hi left hand wa a gold snuff box, from whic...   \n",
       "id27763  how love is spring As we look from windsor ter...   \n",
       "id12958  find noth else, not even gold, the superintend...   \n",
       "\n",
       "                                                      text  \n",
       "id                                                          \n",
       "id26305  This process, however, afforded me no means of...  \n",
       "id17569  It never once occurred to me that the fumbling...  \n",
       "id11008  In his left hand was a gold snuff box, from wh...  \n",
       "id27763  How lovely is spring As we looked from Windsor...  \n",
       "id12958  Finding nothing else, not even gold, the Super...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explore = train.copy()\n",
    "stem = PorterStemmer()\n",
    "explore['stemmed'] = explore.text.apply(lambda t: \" \".join([stem.stem(w) for w in t.split()])) \n",
    "explore[['stemmed', 'text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ~~Допълнителните фичъри не сработиха, стеминга също.~~\n",
    "\n",
    "Остават да пробвам:\n",
    "\n",
    "* ~~Оптимизиране на модела с CountVectorizer.~~\n",
    "* Добавяне на още фичъри, от латентни пространства (LDA) - topic modeling.\n",
    "* Word embeddings с невронни мрежи.\n",
    "* Стакинг на класификатори.\n",
    "\n",
    "За сега ще разгледаме само оптимизирането на модела."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim import sklearn_api\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "documents = train.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire processing of the texts has to be automated via pipeline interface - tokenization, create dict, doc2bow/text2bow, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text),\n",
    "                 'num_sentences': text.count('.')}\n",
    "                for text in posts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "              \n",
    "            # Pipeline for gensim LDA\n",
    "            ('gensim', Pipeline([\n",
    "                ('bows', sklearn_api.text2bow.Text2BowTransformer()),\n",
    "                ('tfidf', sklearn_api.tfidf.TfIdfTransformer()),\n",
    "                ('lda', sklearn_api.ldamodel.LdaTransformer()),\n",
    "#                 ('selector', ItemSelector(key='subject')),\n",
    "#                 ('tfidf', TfidfVectorizer(min_df=50)),\n",
    "            ])),\n",
    "\n",
    "            # Pipeline for standard bag-of-words model for body\n",
    "            ('scikit', Pipeline([\n",
    "                ('tfidf', TfidfVectorizer()),\n",
    "#                 ('best', TruncatedSVD(n_components=50)),\n",
    "            ])),\n",
    "\n",
    "#             # Pipeline for pulling ad hoc features from stats\n",
    "#             ('text_stats', Pipeline([\n",
    "#                 ('stats', TextStats()),  # returns a list of dicts\n",
    "#                 ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "#             ])),\n",
    "\n",
    "        ],\n",
    "\n",
    "#         # weight components in FeatureUnion\n",
    "#         transformer_weights={\n",
    "#             'gensim': 0.8,\n",
    "#             'scikit': 0.5,\n",
    "#             'text_stats': 1.0,\n",
    "#         },\n",
    "    )),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bob/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n",
      "/home/bob/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n",
      "/home/bob/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5791973   0.57906834  0.59279693]\n",
      "CPU times: user 233 ms, sys: 105 ms, total: 338 ms\n",
      "Wall time: 2min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(cross_val_score(pipeline, train.text, train.author, cv=3, n_jobs=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ 0.59803922  0.59653693  0.59509579]<br>\n",
    "CPU times: user 329 ms, sys: 1.26 s, total: 1.59 s<br>\n",
    "Wall time: 2min 40s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "                    \n",
    "            \n",
    "            ('features', TfidfVectorizer()),\n",
    "            \n",
    "            # Pipeline for gensim LDA\n",
    "            ('gensim', Pipeline([\n",
    "                ('bows', sklearn_api.text2bow.Text2BowTransformer()),\n",
    "                ('tfidf', sklearn_api.tfidf.TfIdfTransformer()),\n",
    "                ('lda', sklearn_api.ldamodel.LdaTransformer()),\n",
    "            ])),\n",
    "\n",
    "            # Pipeline for standard bag-of-words model for body\n",
    "            ('scikit', Pipeline([\n",
    "                ('tfidf', TfidfVectorizer()),\n",
    "#                 ('best', TruncatedSVD(n_components=50)),\n",
    "            ])),\n",
    "\n",
    "            # Pipeline for pulling ad hoc features from stats\n",
    "            ('text_stats', Pipeline([\n",
    "                ('stats', TextStats()),  # returns a list of dicts\n",
    "                ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "            ])),\n",
    "\n",
    "        ],\n",
    "\n",
    "#         # weight components in FeatureUnion\n",
    "#         transformer_weights={\n",
    "#             'gensim': 0.8,\n",
    "#             'scikit': 0.5,\n",
    "#             'text_stats': 1.0,\n",
    "#         },\n",
    "    )),\n",
    "    ('clf', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bob/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n",
      "/home/bob/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n",
      "/home/bob/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.72717525  0.71835734  0.72904215]\n",
      "CPU times: user 246 ms, sys: 126 ms, total: 372 ms\n",
      "Wall time: 2min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(cross_val_score(pipeline, train.text, train.author, cv=3, n_jobs=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bows = sklearn_api.text2bow.Text2BowTransformer().fit_transform(train.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(train.text[656], bows[656])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda = sklearn_api.ldamodel.LdaTransformer().fit_transform(bows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(train.text[656], bows[656], lda[bows[656]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fail here\n",
    "\n",
    "for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ 0.48360907  0.48850751  0.48613027]<br>\n",
    "[-2.1033608  -2.09791398 -2.30065565]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преди да пообработим текста да видим Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', sklearn_api.text2bow.Text2BowTransformer()),\n",
    "    ('lda', sklearn_api.ldamodel.LdaTransformer()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# print(cross_val_score(pipeline, train.text, train.author, cv=3, n_jobs=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ 0.53722426  0.53463071  0.53563218]<br>\n",
    "CPU times: user 430 ms, sys: 169 ms, total: 599 ms<br>\n",
    "Wall time: 3min 41s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# print(cross_val_score(pipeline, train.text, train.author, \n",
    "#                 cv=3, n_jobs=3, scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[-0.96678405 -0.97080789 -0.97363342]<br>\n",
    "CPU times: user 453 ms, sys: 121 ms, total: 573 ms<br>\n",
    "Wall time: 3min 39s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Още един опит преди да окастрим текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', sklearn_api.text2bow.Text2BowTransformer()),\n",
    "    ('lda', sklearn_api.ldamodel.LdaTransformer(num_topics=21)),\n",
    "    ('clf', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# print(cross_val_score(pipeline, train.text, train.author, \n",
    "#                 cv=3, n_jobs=3, scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[-0.98861154 -0.97565475 -0.99821168]<br>\n",
    "CPU times: user 381 ms, sys: 99.9 ms, total: 481 ms<br>\n",
    "Wall time: 2min 8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "explore.stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import preprocess_string, strip_punctuation, remove_stopwords\n",
    "\n",
    "# CUSTOM_FILTERS = [strip_punctuation, remove_stopwords]\n",
    "# explore['stemmed2'] = [ preprocess_string(s, CUSTOM_FILTERS) for s in explore.stemmed]\n",
    "# explore.stemmed2\n",
    "\n",
    "explore['stemmed2'] = [ strip_punctuation(remove_stopwords(s.lower())) for s in explore.stemmed]\n",
    "explore.stemmed2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(explore.loc['id17569'].stemmed, '|||', explore.loc['id17569'].stemmed2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline-a си е същия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# print(cross_val_score(pipeline, explore.stemmed2, train.author, \n",
    "#                 cv=3, n_jobs=3, scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[-1.05550683 -1.02313271 -1.02899542]<br>\n",
    "CPU times: user 347 ms, sys: 92.8 ms, total: 440 ms<br>\n",
    "Wall time: 1min 27s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поне е по-бързо. Да видим tfidf и повече фичъри."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow', sklearn_api.text2bow.Text2BowTransformer()),\n",
    "    ('features', sklearn_api.tfidf.TfIdfTransformer()),\n",
    "    ('lda', sklearn_api.ldamodel.LdaTransformer(num_topics=501)),\n",
    "    ('clf', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# print(cross_val_score(pipeline, train.text, train.author, \n",
    "#                 cv=3, n_jobs=3, scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[-0.94941507 -0.95248997 -0.95026712]<br>\n",
    "CPU times: user 1.18 s, sys: 223 ms, total: 1.4 s<br>\n",
    "Wall time: 19min 4s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is frustrating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow', sklearn_api.text2bow.Text2BowTransformer()),\n",
    "    ('features', sklearn_api.tfidf.TfIdfTransformer()),\n",
    "    ('lda', sklearn_api.ldamodel.LdaTransformer()),\n",
    "    ('clf', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# print(cross_val_score(pipeline, train.text, train.author, \n",
    "#                 cv=3, n_jobs=3, scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[-1.03061917 -1.03984701 -1.03884094]<br>\n",
    "CPU times: user 387 ms, sys: 124 ms, total: 511 ms<br>\n",
    "Wall time: 2min 33s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow', sklearn_api.text2bow.Text2BowTransformer()),\n",
    "    ('features', sklearn_api.tfidf.TfIdfTransformer()),\n",
    "    ('lda', sklearn_api.ldamodel.LdaTransformer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# print(cross_val_score(pipeline, train.text, train.author, \n",
    "#                 cv=3, n_jobs=3, scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[-1.06731359 -1.07019065 -1.07019197]<br>\n",
    "CPU times: user 377 ms, sys: 142 ms, total: 519 ms<br>\n",
    "Wall time: 2min 29s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "От къде е тая разлика? Може ли text2bow да чупи? Или е от липсата на хипер парамерти?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corpus е минат с word2vec; ще прескоча tfidf за момента и ще пусна LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "# lda = LdaModel(corpus=corpus, id2word=dictionary)\n",
    "# lda.print_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# id2word=dict([(i, s) for i, s in enumerate(dictionary)])\n",
    "# print([ id2word[z] for z in range(10) ])\n",
    "# print(list(id2word.items())[:10])\n",
    "# what = list(enumerate(dictionary))\n",
    "# print(what[:10])\n",
    "# print(list(dictionary)[:10])\n",
    "# print(type(dictionary))\n",
    "# wdk = dictionary.keys()\n",
    "# print(wdk[:10])\n",
    "# print(dictionary[wdk[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Горното даваше topic с референции към IDта вместо към реални думи - не знам защо сега работи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "lda = LdaModel(corpus=corpus, id2word=dict(dictionary.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представянето на topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda.print_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda.show_topics(num_topics=10, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представянето на запис в латентното пространство. Векторите са 100, но Gensim показва само най-значимите:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda[corpus[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda.get_document_topics(corpus[1], minimum_probability=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(corpus), len(train.author))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply logistic regression, but first encode topics in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "latently = train.copy()\n",
    "for t in range(100):\n",
    "    latently[t]=0\n",
    "\n",
    "print(latently[list(range(100))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for i, doc in enumerate(corpus):\n",
    "    temp = [0]*100\n",
    "    for t in lda[doc]:\n",
    "        temp[t[0]] = t[1]\n",
    "    data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for t in range(100):\n",
    "    latently[t] = [ data[z][t] for z in range(len(data)) ]\n",
    "\n",
    "print(latently[list(range(100))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "AUTHOR_ENCODING = {'EAP': 0, 'MWS': 1, 'HPL': 2}\n",
    "latently['encoded_author'] = [ AUTHOR_ENCODING[z] for z in train.author ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "latently.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(latently[list(range(100))], latently.encoded_author, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Train score: {:.2f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test  score: {:.2f}\".format(logreg.score(X_test, y_test)))\n",
    "prediction = logreg.predict_proba(X_test)\n",
    "logloss = log_loss(y_test, prediction, labels=[0, 1, 2])\n",
    "print('Log Loss score', logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(prediction[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print([ list(z).index(max(z)) for z in prediction[:10] ], y_test[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we have way too many topics for this to work, let's see what the efect of only 3 topics is. I'll use the manual approach instead of Pipleline, since I don't know how data conversion is being handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "lda3 = LdaModel(corpus=corpus, num_topics=3, id2word=dict(dictionary.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "late3 = train.copy()\n",
    "\n",
    "data = []\n",
    "for i, doc in enumerate(corpus):\n",
    "    temp = [0, 0, 0]\n",
    "    for t in lda3[doc]:\n",
    "        temp[t[0]] = t[1]\n",
    "    data.append(temp)\n",
    "\n",
    "late3[0] = [ data[z][0] for z in range(len(data)) ]\n",
    "late3[1] = [ data[z][1] for z in range(len(data)) ]\n",
    "late3[2] = [ data[z][2] for z in range(len(data)) ]\n",
    "\n",
    "print(late3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "late3['encoded_author'] = [ AUTHOR_ENCODING[z] for z in train.author ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(late3[[0,1,2]], late3.encoded_author, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Train score: {:.2f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test  score: {:.2f}\".format(logreg.score(X_test, y_test)))\n",
    "prediction = logreg.predict_proba(X_test)\n",
    "logloss = log_loss(y_test, prediction, labels=[0, 1, 2])\n",
    "print('Log Loss score', logloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we dont use the LDA on it's own, but as extra features on top of the CountVectorizer? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorized = vectorizer.fit_transform(train.text).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will use approach from http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html in another NB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fail here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Dropout, Lambda, Concatenate\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras import regularizers, objectives, metrics\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Откри приблизително същите параметри, но не успя да стигне напълно до същия резултат.\n",
    "\n",
    "Ще използвам следния модел:\n",
    "\n",
    "TfIdf + MultinomialNB, без стеминг на текста.\n",
    "\n",
    "Mean validation score: -0.423 (std: 0.003)\n",
    "\n",
    "Ще ползвам и следните параметри:\n",
    "\n",
    "Parameters: {'features__stop_words': None, 'features__ngram_range': (1, 2), 'features__min_df': 2, 'features__max_df': 0.8, 'features__lowercase': False, 'features__analyzer': 'word', 'clf__alpha': 0.01}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Последна проверка на този модел за `LogLoss` и `Accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', TfidfVectorizer(ngram_range=(1, 2), min_df=2,\n",
    "                                 max_df=0.8, lowercase=False)),\n",
    "    ('clf', MultinomialNB(alpha=0.01))\n",
    "])\n",
    "\n",
    "print(cross_val_score(pipeline, train.text, train.author, cv=3, n_jobs=3))\n",
    "print(cross_val_score(pipeline, train.text, train.author, cv=3, n_jobs=3, \n",
    "                      scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Трениране на модел и събмит\n",
    "\n",
    "Първо да видим в какъв формат трябва да се подадат резултатите за тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"data/spooky-authors/sample_submission.zip\")\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = pipeline.fit(train.text, train.author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(pipeline.predict_proba(test[:10].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "test_predictions = pipeline.predict_proba(test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(pipeline.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "submit_file = pd.DataFrame(test_predictions, columns=['EAP', 'MWS', 'HPL'], index=test.index)\n",
    "submit_file.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "submit_file.to_csv(\"data/spooky-authors/submit_Tfidf_MNB_text.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Очакванията за събмита са да имаме скор някъде около 0.41 - 0.42.\n",
    "\n",
    "Може да е малко по-добър защото при крос-валидацията тренирахме на 13к и тествахме 6к.\n",
    "\n",
    "Сега трейн сета е целия: 19.5к"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![submit-result.png](attachment:submit-result.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Да хакнем ранкинга в кагъл?\n",
    "\n",
    "print(test.text[:5].values)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "none",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
