{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import mglearn\n",
    "from IPython.display import display\n",
    "\n",
    "#%matplotlib notebook\n",
    "\n",
    "import sklearn\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_rows = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 2) (8392, 1) (8392, 3)\n",
      "{'author'}\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "\n",
    "# use this if in fmi-hw... repo\n",
    "# train = pd.read_csv(\"data/spooky/train.zip\", index_col=['id'])\n",
    "# test = pd.read_csv(\"data/spooky/test.zip\", index_col=['id'])\n",
    "# sample_submission = pd.read_csv(\"data/spooky/sample_submission.zip\", index_col=['id'])\n",
    "\n",
    "train = pd.read_csv(\"data/train.zip\", index_col=['id'])\n",
    "test = pd.read_csv(\"data/test.zip\", index_col=['id'])\n",
    "sample_submission = pd.read_csv(\"data/sample_submission.zip\", index_col=['id'])\n",
    "\n",
    "\n",
    "print(train.shape, test.shape, sample_submission.shape)\n",
    "print(set(train.columns) - set(test.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ~~Първо - baseline модел~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stemmed</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id26305</th>\n",
       "      <td>thi process, however, afford me no mean of asc...</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id17569</th>\n",
       "      <td>It never onc occur to me that the fumbl might ...</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id11008</th>\n",
       "      <td>In hi left hand wa a gold snuff box, from whic...</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id27763</th>\n",
       "      <td>how love is spring As we look from windsor ter...</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id12958</th>\n",
       "      <td>find noth else, not even gold, the superintend...</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   stemmed  \\\n",
       "id                                                           \n",
       "id26305  thi process, however, afford me no mean of asc...   \n",
       "id17569  It never onc occur to me that the fumbl might ...   \n",
       "id11008  In hi left hand wa a gold snuff box, from whic...   \n",
       "id27763  how love is spring As we look from windsor ter...   \n",
       "id12958  find noth else, not even gold, the superintend...   \n",
       "\n",
       "                                                      text  \n",
       "id                                                          \n",
       "id26305  This process, however, afforded me no means of...  \n",
       "id17569  It never once occurred to me that the fumbling...  \n",
       "id11008  In his left hand was a gold snuff box, from wh...  \n",
       "id27763  How lovely is spring As we looked from Windsor...  \n",
       "id12958  Finding nothing else, not even gold, the Super...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explore = train.copy()\n",
    "stem = PorterStemmer()\n",
    "explore['stemmed'] = explore.text.apply(lambda t: \" \".join([stem.stem(w) for w in t.split()])) \n",
    "explore[['stemmed', 'text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ~~Допълнителните фичъри не сработиха, стеминга също.~~\n",
    "\n",
    "Остават да пробвам:\n",
    "\n",
    "* ~~Оптимизиране на модела с CountVectorizer.~~\n",
    "* Добавяне на още фичъри, от латентни пространства (LDA) - topic modeling.\n",
    "* Word embeddings с невронни мрежи.\n",
    "* Стакинг на класификатори.\n",
    "\n",
    "За сега ще разгледаме само оптимизирането на модела."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from gensim import corpora\n",
    "\n",
    "documents = train.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire processing of the texts has to be automated via pipeline interface - tokenization, create dict, doc2bow/text2bow, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['process,',\n",
      "  'however,',\n",
      "  'afforded',\n",
      "  'means',\n",
      "  'ascertaining',\n",
      "  'dimensions',\n",
      "  'might',\n",
      "  'make',\n",
      "  'return',\n",
      "  'point',\n",
      "  'whence',\n",
      "  'set',\n",
      "  'out,',\n",
      "  'without',\n",
      "  'aware',\n",
      "  'perfectly',\n",
      "  'uniform',\n",
      "  'seemed',\n",
      "  'wall.'],\n",
      " ['never', 'occurred', 'fumbling', 'might', 'mere', 'mistake.']]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "from pprint import pprint  # pretty-printer\n",
    "\n",
    "TOKENIZED_FILENAME = \"processed_texts.pickle\"\n",
    "\n",
    "try:\n",
    "    # depicke\n",
    "    texts = pickle.load(open(TOKENIZED_FILENAME, \"rb\"))   \n",
    "except FileNotFoundError:\n",
    "    # remove common words and tokenize\n",
    "    # stoplist = set('for a of the and to in'.split())\n",
    "    texts = [[word for word in document.lower().split() if word not in stopwords] #was stoplist\n",
    "             for document in documents]\n",
    "\n",
    "    # remove words that appear only once\n",
    "    from collections import defaultdict\n",
    "    frequency = defaultdict(int)\n",
    "    for text in texts:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "\n",
    "    texts = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "    \n",
    "    # create pickle\n",
    "    pickle.dump(texts, open(TOKENIZED_FILENAME, \"wb\" ))\n",
    "\n",
    "pprint(texts[:2])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22328\n"
     ]
    }
   ],
   "source": [
    "DICT_FILENAME = 'spooky.dict'\n",
    "\n",
    "try:\n",
    "    dictionary = corpora.Dictionary.load(DICT_FILENAME)\n",
    "except FileNotFoundError:\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    dictionary.save(DICT_FILENAME)  # store the dictionary, for future reference\n",
    "\n",
    "print(len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(545, 1.0), (1137, 1.0), (1598, 1.0), (1599, 1.0), (1600, 1.0), (1601, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "CORPUS_FILENAME = 'spooky.mm'\n",
    "\n",
    "try:\n",
    "    #restore from disk\n",
    "    corpus = corpora.mmcorpus.MmCorpus(CORPUS_FILENAME)\n",
    "except FileNotFoundError:\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    corpora.MmCorpus.serialize('spooky.mm', corpus)  # store to disk, for later use\n",
    "\n",
    "print(corpus[156])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in df:      This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall. \n",
      "\n",
      "as doc:     This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall. \n",
      "\n",
      "as tokens:  ['process,', 'however,', 'afforded', 'means', 'ascertaining', 'dimensions', 'might', 'make', 'return', 'point', 'whence', 'set', 'out,', 'without', 'aware', 'perfectly', 'uniform', 'seemed', 'wall.'] \n",
      "\n",
      "as vec:     [(0, 1.0), (1, 1.0), (2, 1.0), (3, 1.0), (4, 1.0), (5, 1.0), (6, 1.0), (7, 1.0), (8, 1.0), (9, 1.0), (10, 1.0), (11, 1.0), (12, 1.0), (13, 1.0), (14, 1.0), (15, 1.0), (16, 1.0), (17, 1.0), (18, 1.0)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('in df:     ', train.text[0],'\\n')\n",
    "print('as doc:    ', documents[0],'\\n')\n",
    "print('as tokens: ', texts[0],'\\n')\n",
    "print('as vec:    ', corpus[0],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim import sklearn_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', sklearn_api.text2bow.Text2BowTransformer()),\n",
    "    ('lda', sklearn_api.ldamodel.LdaTransformer()),\n",
    "#     ('clf', RandomForestClassifier())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bob/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n",
      "/home/bob/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n",
      "/home/bob/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-58670.90445705 -64169.14896674 -65322.97842153]\n",
      "CPU times: user 321 ms, sys: 154 ms, total: 474 ms\n",
      "Wall time: 2min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(cross_val_score(pipeline, train.text, train.author, cv=3, n_jobs=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "running only Text2BowTransformer()) + LdaTransformer()) gives\n",
    "\n",
    "[-58670.90445705 -64169.14896674 -65322.97842153]<br>\n",
    "CPU times: user 321 ms, sys: 154 ms, total: 474 ms<br>\n",
    "Wall time: 2min 39s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the tricky part is the outpurt given by the LDA, and how is that used by the subsequent Pipes. The [docs](https://radimrehurek.com/gensim/sklearn_api/ldamodel.html) state:\n",
    "\n",
    "*Returns a matrix of topic distribution for the given document bow, where a_ij indicates (topic_i, topic_probability_j). The input docs should be in BOW format and can be a list of documents like [[(4, 1), (7, 1)], [(9, 1), (13, 1)], [(2, 1), (6, 1)]]*\n",
    "\n",
    "It's totally unclear what a LogisticRegression will pick from a \"matrix of topic distribution\". Let's try to see it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bows = sklearn_api.text2bow.Text2BowTransformer().fit_transform(train.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping of its own accord upon his exit or perhaps purposely closed, it had become fastened by the spring; and it was the retention of this spring which had been mistaken by the police for that of the nail, farther inquiry being thus considered unnecessary. [(3, 1), (7, 1), (13, 1), (20, 3), (29, 4), (44, 1), (60, 1), (70, 1), (71, 1), (78, 2), (95, 2), (133, 1), (143, 2), (174, 1), (178, 1), (201, 2), (220, 1), (321, 1), (323, 1), (352, 1), (403, 1), (542, 1), (1485, 1), (1779, 1), (1798, 1), (1873, 1), (2219, 1), (2889, 1), (4672, 1), (4673, 1), (4674, 1), (4675, 1), (4676, 1), (4677, 1), (4678, 1), (4679, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(train.text[656], bows[656])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bob/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    }
   ],
   "source": [
    "lda = sklearn_api.ldamodel.LdaTransformer().fit_transform(bows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping of its own accord upon his exit or perhaps purposely closed, it had become fastened by the spring; and it was the retention of this spring which had been mistaken by the police for that of the nail, farther inquiry being thus considered unnecessary. [(3, 1), (7, 1), (13, 1), (20, 3), (29, 4), (44, 1), (60, 1), (70, 1), (71, 1), (78, 2), (95, 2), (133, 1), (143, 2), (174, 1), (178, 1), (201, 2), (220, 1), (321, 1), (323, 1), (352, 1), (403, 1), (542, 1), (1485, 1), (1779, 1), (1798, 1), (1873, 1), (2219, 1), (2889, 1), (4672, 1), (4673, 1), (4674, 1), (4675, 1), (4676, 1), (4677, 1), (4678, 1), (4679, 1)] [[[ 0.          0.03539791  0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.10721884]]\n",
      "\n",
      " [[ 0.          0.55708617  0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.10721884]]\n",
      "\n",
      " [[ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.10721884]]\n",
      "\n",
      " ..., \n",
      " [[ 0.          0.          0.         ...,  0.          0.18530692  0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.10721884]]\n",
      "\n",
      " [[ 0.          0.          0.         ...,  0.          0.          0.03727628]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.10721884]]\n",
      "\n",
      " [[ 0.          0.          0.         ...,  0.          0.          0.04265947]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.10721884]]]\n"
     ]
    }
   ],
   "source": [
    "print(train.text[656], bows[656], lda[bows[656]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.10721884],\n",
       "       [ 0.09975109,  0.02642765,  0.        , ...,  0.        ,\n",
       "         0.03326323,  0.        ],\n",
       "       ..., \n",
       "       [ 0.20021059,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.07886007],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-b80ac4c78243>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-b80ac4c78243>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    fail here\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "fail here\n",
    "\n",
    "for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ 0.48360907  0.48850751  0.48613027]<br>\n",
    "[-2.1033608  -2.09791398 -2.30065565]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преди да пообработим текста да видим Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', sklearn_api.text2bow.Text2BowTransformer()),\n",
    "    ('lda', sklearn_api.ldamodel.LdaTransformer()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# print(cross_val_score(pipeline, train.text, train.author, cv=3, n_jobs=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ 0.53722426  0.53463071  0.53563218]<br>\n",
    "CPU times: user 430 ms, sys: 169 ms, total: 599 ms<br>\n",
    "Wall time: 3min 41s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# print(cross_val_score(pipeline, train.text, train.author, \n",
    "#                 cv=3, n_jobs=3, scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[-0.96678405 -0.97080789 -0.97363342]<br>\n",
    "CPU times: user 453 ms, sys: 121 ms, total: 573 ms<br>\n",
    "Wall time: 3min 39s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Още един опит преди да окастрим текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', sklearn_api.text2bow.Text2BowTransformer()),\n",
    "    ('lda', sklearn_api.ldamodel.LdaTransformer(num_topics=21)),\n",
    "    ('clf', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# print(cross_val_score(pipeline, train.text, train.author, \n",
    "#                 cv=3, n_jobs=3, scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[-0.98861154 -0.97565475 -0.99821168]<br>\n",
    "CPU times: user 381 ms, sys: 99.9 ms, total: 481 ms<br>\n",
    "Wall time: 2min 8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "explore.stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import preprocess_string, strip_punctuation, remove_stopwords\n",
    "\n",
    "# CUSTOM_FILTERS = [strip_punctuation, remove_stopwords]\n",
    "# explore['stemmed2'] = [ preprocess_string(s, CUSTOM_FILTERS) for s in explore.stemmed]\n",
    "# explore.stemmed2\n",
    "\n",
    "explore['stemmed2'] = [ strip_punctuation(remove_stopwords(s.lower())) for s in explore.stemmed]\n",
    "explore.stemmed2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(explore.loc['id17569'].stemmed, '|||', explore.loc['id17569'].stemmed2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline-a си е същия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# print(cross_val_score(pipeline, explore.stemmed2, train.author, \n",
    "#                 cv=3, n_jobs=3, scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[-1.05550683 -1.02313271 -1.02899542]<br>\n",
    "CPU times: user 347 ms, sys: 92.8 ms, total: 440 ms<br>\n",
    "Wall time: 1min 27s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поне е по-бързо. Да видим tfidf и повече фичъри."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow', sklearn_api.text2bow.Text2BowTransformer()),\n",
    "    ('features', sklearn_api.tfidf.TfIdfTransformer()),\n",
    "    ('lda', sklearn_api.ldamodel.LdaTransformer(num_topics=501)),\n",
    "    ('clf', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# print(cross_val_score(pipeline, train.text, train.author, \n",
    "#                 cv=3, n_jobs=3, scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[-0.94941507 -0.95248997 -0.95026712]<br>\n",
    "CPU times: user 1.18 s, sys: 223 ms, total: 1.4 s<br>\n",
    "Wall time: 19min 4s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is frustrating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow', sklearn_api.text2bow.Text2BowTransformer()),\n",
    "    ('features', sklearn_api.tfidf.TfIdfTransformer()),\n",
    "    ('lda', sklearn_api.ldamodel.LdaTransformer()),\n",
    "    ('clf', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# print(cross_val_score(pipeline, train.text, train.author, \n",
    "#                 cv=3, n_jobs=3, scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[-1.03061917 -1.03984701 -1.03884094]<br>\n",
    "CPU times: user 387 ms, sys: 124 ms, total: 511 ms<br>\n",
    "Wall time: 2min 33s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow', sklearn_api.text2bow.Text2BowTransformer()),\n",
    "    ('features', sklearn_api.tfidf.TfIdfTransformer()),\n",
    "    ('lda', sklearn_api.ldamodel.LdaTransformer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# print(cross_val_score(pipeline, train.text, train.author, \n",
    "#                 cv=3, n_jobs=3, scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[-1.06731359 -1.07019065 -1.07019197]<br>\n",
    "CPU times: user 377 ms, sys: 142 ms, total: 519 ms<br>\n",
    "Wall time: 2min 29s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "От къде е тая разлика? Може ли text2bow да чупи? Или е от липсата на хипер парамерти?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corpus е минат с word2vec; ще прескоча tfidf за момента и ще пусна LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "# lda = LdaModel(corpus=corpus, id2word=dictionary)\n",
    "# lda.print_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# id2word=dict([(i, s) for i, s in enumerate(dictionary)])\n",
    "# print([ id2word[z] for z in range(10) ])\n",
    "# print(list(id2word.items())[:10])\n",
    "# what = list(enumerate(dictionary))\n",
    "# print(what[:10])\n",
    "# print(list(dictionary)[:10])\n",
    "# print(type(dictionary))\n",
    "# wdk = dictionary.keys()\n",
    "# print(wdk[:10])\n",
    "# print(dictionary[wdk[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Горното даваше topic с референции към IDта вместо към реални думи - не знам защо сега работи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "lda = LdaModel(corpus=corpus, id2word=dict(dictionary.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представянето на topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda.print_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda.show_topics(num_topics=10, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представянето на запис в латентното пространство. Векторите са 100, но Gensim показва само най-значимите:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda[corpus[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda.get_document_topics(corpus[1], minimum_probability=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(corpus), len(train.author))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply logistic regression, but first encode topics in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "latently = train.copy()\n",
    "for t in range(100):\n",
    "    latently[t]=0\n",
    "\n",
    "print(latently[list(range(100))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for i, doc in enumerate(corpus):\n",
    "    temp = [0]*100\n",
    "    for t in lda[doc]:\n",
    "        temp[t[0]] = t[1]\n",
    "    data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for t in range(100):\n",
    "    latently[t] = [ data[z][t] for z in range(len(data)) ]\n",
    "\n",
    "print(latently[list(range(100))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "AUTHOR_ENCODING = {'EAP': 0, 'MWS': 1, 'HPL': 2}\n",
    "latently['encoded_author'] = [ AUTHOR_ENCODING[z] for z in train.author ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "latently.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(latently[list(range(100))], latently.encoded_author, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Train score: {:.2f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test  score: {:.2f}\".format(logreg.score(X_test, y_test)))\n",
    "prediction = logreg.predict_proba(X_test)\n",
    "logloss = log_loss(y_test, prediction, labels=[0, 1, 2])\n",
    "print('Log Loss score', logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(prediction[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print([ list(z).index(max(z)) for z in prediction[:10] ], y_test[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we have way too many topics for this to work, let's see what the efect of only 3 topics is. I'll use the manual approach instead of Pipleline, since I don't know how data conversion is being handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "lda3 = LdaModel(corpus=corpus, num_topics=3, id2word=dict(dictionary.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "late3 = train.copy()\n",
    "\n",
    "data = []\n",
    "for i, doc in enumerate(corpus):\n",
    "    temp = [0, 0, 0]\n",
    "    for t in lda3[doc]:\n",
    "        temp[t[0]] = t[1]\n",
    "    data.append(temp)\n",
    "\n",
    "late3[0] = [ data[z][0] for z in range(len(data)) ]\n",
    "late3[1] = [ data[z][1] for z in range(len(data)) ]\n",
    "late3[2] = [ data[z][2] for z in range(len(data)) ]\n",
    "\n",
    "print(late3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "late3['encoded_author'] = [ AUTHOR_ENCODING[z] for z in train.author ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(late3[[0,1,2]], late3.encoded_author, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Train score: {:.2f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test  score: {:.2f}\".format(logreg.score(X_test, y_test)))\n",
    "prediction = logreg.predict_proba(X_test)\n",
    "logloss = log_loss(y_test, prediction, labels=[0, 1, 2])\n",
    "print('Log Loss score', logloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we dont use the LDA on it's own, but as extra features on top of the CountVectorizer? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorized = vectorizer.fit_transform(train.text).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will use approach from http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html in another NB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fail here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Dropout, Lambda, Concatenate\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras import regularizers, objectives, metrics\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Откри приблизително същите параметри, но не успя да стигне напълно до същия резултат.\n",
    "\n",
    "Ще използвам следния модел:\n",
    "\n",
    "TfIdf + MultinomialNB, без стеминг на текста.\n",
    "\n",
    "Mean validation score: -0.423 (std: 0.003)\n",
    "\n",
    "Ще ползвам и следните параметри:\n",
    "\n",
    "Parameters: {'features__stop_words': None, 'features__ngram_range': (1, 2), 'features__min_df': 2, 'features__max_df': 0.8, 'features__lowercase': False, 'features__analyzer': 'word', 'clf__alpha': 0.01}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Последна проверка на този модел за `LogLoss` и `Accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', TfidfVectorizer(ngram_range=(1, 2), min_df=2,\n",
    "                                 max_df=0.8, lowercase=False)),\n",
    "    ('clf', MultinomialNB(alpha=0.01))\n",
    "])\n",
    "\n",
    "print(cross_val_score(pipeline, train.text, train.author, cv=3, n_jobs=3))\n",
    "print(cross_val_score(pipeline, train.text, train.author, cv=3, n_jobs=3, \n",
    "                      scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Трениране на модел и събмит\n",
    "\n",
    "Първо да видим в какъв формат трябва да се подадат резултатите за тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"data/spooky-authors/sample_submission.zip\")\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = pipeline.fit(train.text, train.author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(pipeline.predict_proba(test[:10].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "test_predictions = pipeline.predict_proba(test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(pipeline.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "submit_file = pd.DataFrame(test_predictions, columns=['EAP', 'MWS', 'HPL'], index=test.index)\n",
    "submit_file.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "submit_file.to_csv(\"data/spooky-authors/submit_Tfidf_MNB_text.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Очакванията за събмита са да имаме скор някъде около 0.41 - 0.42.\n",
    "\n",
    "Може да е малко по-добър защото при крос-валидацията тренирахме на 13к и тествахме 6к.\n",
    "\n",
    "Сега трейн сета е целия: 19.5к"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![submit-result.png](attachment:submit-result.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Да хакнем ранкинга в кагъл?\n",
    "\n",
    "print(test.text[:5].values)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "none",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
