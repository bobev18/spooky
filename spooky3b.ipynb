{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import mglearn\n",
    "from IPython.display import display\n",
    "\n",
    "#%matplotlib notebook\n",
    "\n",
    "import sklearn\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_rows = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 2) (8392, 1) (8392, 3)\n",
      "{'author'}\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "\n",
    "# use this if in fmi-hw... repo\n",
    "# train = pd.read_csv(\"data/spooky/train.zip\", index_col=['id'])\n",
    "# test = pd.read_csv(\"data/spooky/test.zip\", index_col=['id'])\n",
    "# sample_submission = pd.read_csv(\"data/spooky/sample_submission.zip\", index_col=['id'])\n",
    "\n",
    "train = pd.read_csv(\"data/train.zip\", index_col=['id'])\n",
    "test = pd.read_csv(\"data/test.zip\", index_col=['id'])\n",
    "sample_submission = pd.read_csv(\"data/sample_submission.zip\", index_col=['id'])\n",
    "\n",
    "\n",
    "print(train.shape, test.shape, sample_submission.shape)\n",
    "print(set(train.columns) - set(test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id26305</th>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id17569</th>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id11008</th>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id27763</th>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id12958</th>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text author\n",
       "id                                                               \n",
       "id26305  This process, however, afforded me no means of...    EAP\n",
       "id17569  It never once occurred to me that the fumbling...    HPL\n",
       "id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Идеи за фичъри:\n",
    "    \n",
    "* ~~CountVectorizer, Tfidf~~\n",
    "* ~~Preprocessing - stop words, lematization~~\n",
    "* ~~Други фичъри - бр. думи , бр. стоп думи, бр. пунктуация, бр. ГЛАВНИ букви и т.н.~~\n",
    "* Намиране на общи теми чрез LDA\n",
    "* Word Embeddings с невронни мрежи\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Първо - baseline модел"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.78783701,  0.79635305,  0.79509579])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', CountVectorizer()),\n",
    "    ('clf', LinearSVC())\n",
    "])\n",
    "\n",
    "cross_val_score(pipeline, train.text, train.author, cv=3, n_jobs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Да пробваме с RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "pipeline = Pipeline([\n",
    "    ('features', CountVectorizer()),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# print(cross_val_score(pipeline, train.text, train.author, cv=3, n_jobs=3))\n",
    "# print(cross_val_score(pipeline, train.text, train.author, \n",
    "#                 cv=3, n_jobs=3, scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ 0.62300858  0.61798958  0.60551724]\n",
    "\n",
    "[-1.36418837 -1.38716468 -1.43783028]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', CountVectorizer()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "# print(cross_val_score(pipeline, train.text, train.author, cv=3, n_jobs=3))\n",
    "# print(cross_val_score(pipeline, train.text, train.author, cv=3, n_jobs=3, \n",
    "#                       scoring='neg_log_loss'))\n",
    "\n",
    "# Получихме малко по-добри резултати"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ 0.81449142  0.81673307  0.81348659]\n",
    "\n",
    "[-0.47678328 -0.47558895 -0.47131481]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следващото е за да имаме \"explore\" датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "explore = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stemmed</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id26305</th>\n",
       "      <td>thi process, however, afford me no mean of asc...</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id17569</th>\n",
       "      <td>It never onc occur to me that the fumbl might ...</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id11008</th>\n",
       "      <td>In hi left hand wa a gold snuff box, from whic...</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id27763</th>\n",
       "      <td>how love is spring As we look from windsor ter...</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id12958</th>\n",
       "      <td>find noth else, not even gold, the superintend...</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   stemmed  \\\n",
       "id                                                           \n",
       "id26305  thi process, however, afford me no mean of asc...   \n",
       "id17569  It never onc occur to me that the fumbl might ...   \n",
       "id11008  In hi left hand wa a gold snuff box, from whic...   \n",
       "id27763  how love is spring As we look from windsor ter...   \n",
       "id12958  find noth else, not even gold, the superintend...   \n",
       "\n",
       "                                                      text  \n",
       "id                                                          \n",
       "id26305  This process, however, afforded me no means of...  \n",
       "id17569  It never once occurred to me that the fumbling...  \n",
       "id11008  In his left hand was a gold snuff box, from wh...  \n",
       "id27763  How lovely is spring As we looked from Windsor...  \n",
       "id12958  Finding nothing else, not even gold, the Super...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stem = PorterStemmer()\n",
    "\n",
    "explore['stemmed'] = explore.text.apply(lambda t: \" \".join([stem.stem(w) for w in t.split()])) \n",
    "\n",
    "explore[['stemmed', 'text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Допълнителните фичъри не сработиха, стеминга също. \n",
    "\n",
    "Остават да пробвам:\n",
    "\n",
    "* ~~Оптимизиране на модела с CountVectorizer.~~\n",
    "* Добавяне на още фичъри, от латентни пространства (LDA) - topic modeling.\n",
    "* Word embeddings с невронни мрежи.\n",
    "* Стакинг на класификатори.\n",
    "\n",
    "За сега ще разгледаме само оптимизирането на модела."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Първо да опишем параметрите за търсене в трансфомацията (CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "\n",
    "params_count_word = {\"features__ngram_range\": [(1,1), (1,2), (1,3)],\n",
    "                      \"features__analyzer\": ['word'],\n",
    "                      \"features__max_df\":[1.0, 0.9, 0.8, 0.7, 0.6, 0.5],\n",
    "                      \"features__min_df\":[2, 3, 5, 10],\n",
    "                      \"features__lowercase\": [False, True],\n",
    "                      \"features__stop_words\": [None, stopwords]}\n",
    "\n",
    "params_count_char = {\"features__ngram_range\": [(1,4), (1,5), (1,6)],\n",
    "                      \"features__analyzer\": ['char'],\n",
    "                      \"features__max_df\":[1.0, 0.9, 0.8, 0.7, 0.6, 0.5],\n",
    "                      \"features__min_df\":[2, 3, 5, 10],\n",
    "                      \"features__lowercase\": [False, True],\n",
    "                      \"features__stop_words\": [None, stopwords]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def report(results, n_top=5):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def random_search():\n",
    "    params = {\n",
    "        \"clf__alpha\": [0.001, 0.005, 0.01, 0.05, 0.1, 0.3]\n",
    "    }\n",
    "\n",
    "    params.update(params_count_word)\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('features', TfidfVectorizer()),\n",
    "        ('clf', MultinomialNB())\n",
    "    ])\n",
    "\n",
    "    random_search = RandomizedSearchCV(pipeline, param_distributions=params, \n",
    "                                       scoring='neg_log_loss',\n",
    "                                       n_iter=20, cv=3, n_jobs=4)\n",
    "\n",
    "    random_search.fit(explore.stemmed, train.author)\n",
    "    report(random_search.cv_results_)\n",
    "    \n",
    "# random_search()  # -0.423"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Model with rank: 1\n",
    "Mean validation score: -0.438 (std: 0.002)\n",
    "Parameters: {'features__stop_words': None, 'features__ngram_range': (1, 2), 'features__min_df': 2, 'features__max_df': 0.6, 'features__lowercase': False, 'features__analyzer': 'word', 'clf__alpha': 0.01}\n",
    "\n",
    "Model with rank: 2\n",
    "Mean validation score: -0.443 (std: 0.004)\n",
    "Parameters: {'features__stop_words': None, 'features__ngram_range': (1, 3), 'features__min_df': 3, 'features__max_df': 0.6, 'features__lowercase': True, 'features__analyzer': 'word', 'clf__alpha': 0.05}\n",
    "\n",
    "Model with rank: 3\n",
    "Mean validation score: -0.453 (std: 0.002)\n",
    "Parameters: {'features__stop_words': None, 'features__ngram_range': (1, 3), 'features__min_df': 2, 'features__max_df': 1.0, 'features__lowercase': False, 'features__analyzer': 'word', 'clf__alpha': 0.01}\n",
    "\n",
    "Model with rank: 4\n",
    "Mean validation score: -0.471 (std: 0.003)\n",
    "Parameters: {'features__stop_words': None, 'features__ngram_range': (1, 2), 'features__min_df': 5, 'features__max_df': 1.0, 'features__lowercase': False, 'features__analyzer': 'word', 'clf__alpha': 0.01}\n",
    "\n",
    "Model with rank: 5\n",
    "Mean validation score: -0.472 (std: 0.004)\n",
    "Parameters: {'features__stop_words': None, 'features__ngram_range': (1, 3), 'features__min_df': 5, 'features__max_df': 0.5, 'features__lowercase': False, 'features__analyzer': 'word', 'clf__alpha': 0.05}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import keras\n",
    "# from keras.datasets import mnist\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Dropout, Lambda, Concatenate\n",
    "# from keras.optimizers import RMSprop, Adam\n",
    "# from keras import regularizers, objectives, metrics\n",
    "# from keras.models import Model\n",
    "# from keras import backend as K\n",
    "\n",
    "# from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "documents = train.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # remove common words and tokenize\n",
    "# # stoplist = set('for a of the and to in'.split())\n",
    "# texts = [[word for word in document.lower().split() if word not in stopwords] #was stoplist\n",
    "#          for document in documents]\n",
    "\n",
    "# # remove words that appear only once\n",
    "# from collections import defaultdict\n",
    "# frequency = defaultdict(int)\n",
    "# for text in texts:\n",
    "#     for token in text:\n",
    "#         frequency[token] += 1\n",
    "\n",
    "# texts = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "\n",
    "# from pprint import pprint  # pretty-printer\n",
    "# pprint(texts[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['process,',\n",
      "  'however,',\n",
      "  'afforded',\n",
      "  'means',\n",
      "  'ascertaining',\n",
      "  'dimensions',\n",
      "  'might',\n",
      "  'make',\n",
      "  'return',\n",
      "  'point',\n",
      "  'whence',\n",
      "  'set',\n",
      "  'out,',\n",
      "  'without',\n",
      "  'aware',\n",
      "  'perfectly',\n",
      "  'uniform',\n",
      "  'seemed',\n",
      "  'wall.'],\n",
      " ['never', 'occurred', 'fumbling', 'might', 'mere', 'mistake.']]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "from pprint import pprint  # pretty-printer\n",
    "\n",
    "TOKENIZED_FILENAME = \"processed_texts.pickle\"\n",
    "\n",
    "try:\n",
    "    # depicke\n",
    "    texts = pickle.load(open(TOKENIZED_FILENAME, \"rb\"))   \n",
    "except FileNotFoundError:\n",
    "    # remove common words and tokenize\n",
    "    # stoplist = set('for a of the and to in'.split())\n",
    "    texts = [[word for word in document.lower().split() if word not in stopwords] #was stoplist\n",
    "             for document in documents]\n",
    "\n",
    "    # remove words that appear only once\n",
    "    from collections import defaultdict\n",
    "    frequency = defaultdict(int)\n",
    "    for text in texts:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "\n",
    "    texts = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "\n",
    "    from pprint import pprint  # pretty-printer\n",
    "    \n",
    "    # create pickle\n",
    "    pickle.dump(texts, open(TOKENIZED_FILENAME, \"wb\" ))\n",
    "\n",
    "pprint(texts[:2])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22328\n"
     ]
    }
   ],
   "source": [
    "DICT_FILENAME = 'spooky.dict'\n",
    "\n",
    "try:\n",
    "    dictionary = corpora.Dictionary.load(DICT_FILENAME)\n",
    "except FileNotFoundError:\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    dictionary.save(DICT_FILENAME)  # store the dictionary, for future reference\n",
    "\n",
    "print(len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(545, 1.0), (1137, 1.0), (1598, 1.0), (1599, 1.0), (1600, 1.0), (1601, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "CORPUS_FILENAME = 'spooky.mm'\n",
    "\n",
    "try:\n",
    "    #restore from disk\n",
    "    corpus = corpora.mmcorpus.MmCorpus(CORPUS_FILENAME)\n",
    "except FileNotFoundError:\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    corpora.MmCorpus.serialize('spooky.mm', corpus)  # store to disk, for later use\n",
    "\n",
    "print(corpus[156])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in df:      This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall. \n",
      "\n",
      "as doc:     This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall. \n",
      "\n",
      "as tokens:  ['process,', 'however,', 'afforded', 'means', 'ascertaining', 'dimensions', 'might', 'make', 'return', 'point', 'whence', 'set', 'out,', 'without', 'aware', 'perfectly', 'uniform', 'seemed', 'wall.'] \n",
      "\n",
      "as vec:     [(0, 1.0), (1, 1.0), (2, 1.0), (3, 1.0), (4, 1.0), (5, 1.0), (6, 1.0), (7, 1.0), (8, 1.0), (9, 1.0), (10, 1.0), (11, 1.0), (12, 1.0), (13, 1.0), (14, 1.0), (15, 1.0), (16, 1.0), (17, 1.0), (18, 1.0)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('in df:     ', train.text[0],'\\n')\n",
    "print('as doc:    ', documents[0],'\\n')\n",
    "print('as tokens: ', texts[0],'\\n')\n",
    "print('as vec:    ', corpus[0],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## similarities.docsim.Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "\n",
    "index = similarities.docsim.Similarity('/home/bob/tmp', corpus, num_features=len(dictionary)) \n",
    "sims = index[corpus[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.99999982),\n",
       " (14771, 0.260133),\n",
       " (8578, 0.22941573),\n",
       " (12208, 0.22941573),\n",
       " (12524, 0.22941573)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "results = list(enumerate(sims))\n",
    "top = sorted(results, key=itemgetter(1), reverse=True)\n",
    "top[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing to document:  This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall. \n",
      "\n",
      "\n",
      "1.0 | EAP | This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall. \n",
      "\n",
      "0.260133 | EAP | The expression of his smile, however, was by no means unpleasing, as might be supposed; but it had no variation whatever. \n",
      "\n",
      "0.229416 | EAP | Much, however, might be ascertained. \n",
      "\n",
      "0.229416 | EAP | I reapproached the wall. \n",
      "\n",
      "0.229416 | HPL | They seemed to hate and fear him at the same time, and he seemed to return these sentiments. \n",
      "\n",
      "0.229416 | EAP | There was nothing in this, however, to make him sob. \n",
      "\n",
      "0.216295 | EAP | At first he stared at me as if he found it impossible to comprehend the witticism of my remark; but as its point seemed slowly to make its way into his brain, his eyes, in the same proportion, seemed protruding from their sockets. \n",
      "\n",
      "0.205196 | EAP | With my aversion to this cat, however, its partiality for myself seemed to increase. \n",
      "\n",
      "0.205196 | EAP | For the sisters, however, I could make no excuse. \n",
      "\n",
      "0.205196 | EAP | I did not care, however, to contest the point with him. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Comparing to document: ', documents[0], '\\n\\n')\n",
    "for sim in top[:10]:\n",
    "    print(sim[1], '|', train.author[sim[0]], '|', documents[sim[0]],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чисто технически може да се направи някакво осредняване на резултати - например резултатите от сравнение към documents[0] и сравнение към documents[2] - като и двата записа са \"EAP\". Но не ми се вижда правилно.\n",
    "\n",
    "Не мисля, че посоката е подходяща за нашия случай, защото сравненията са директни, вместо да се извлича \"пространство\" което да генерализира стила/автора. По-добре да проверим какво друго има в gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim import sklearn_api\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "pipeline = Pipeline([\n",
    "    ('features', sklearn_api.text2bow.Text2BowTransformer()),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "try:\n",
    "    print(cross_val_score(pipeline, train.text, train.author, cv=3, n_jobs=3))\n",
    "    print(cross_val_score(pipeline, train.text, train.author, \n",
    "                    cv=3, n_jobs=3, scoring='neg_log_loss'))\n",
    "except ValueError as e:\n",
    "#     print(e)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ValueError: setting an array element with a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.sklearn_api.text2bow.Text2BowTransformer'>\n",
      "<class 'list'>\n",
      "This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall. [(0, 2), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 3), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 4), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1)] \n",
      " Dropping of its own accord upon his exit or perhaps purposely closed, it had become fastened by the spring; and it was the retention of this spring which had been mistaken by the police for that of the nail, farther inquiry being thus considered unnecessary. [(3, 1), (7, 1), (13, 1), (20, 3), (29, 4), (44, 1), (60, 1), (70, 1), (71, 1), (78, 2), (95, 2), (133, 1), (143, 2), (174, 1), (178, 1), (201, 2), (220, 1), (321, 1), (323, 1), (352, 1), (403, 1), (542, 1), (1485, 1), (1779, 1), (1798, 1), (1873, 1), (2219, 1), (2889, 1), (4672, 1), (4673, 1), (4674, 1), (4675, 1), (4676, 1), (4677, 1), (4678, 1), (4679, 1)]\n"
     ]
    }
   ],
   "source": [
    "something = sklearn_api.text2bow.Text2BowTransformer()\n",
    "print(type(something))\n",
    "other = something.fit_transform(train.text)\n",
    "print(type(other))\n",
    "print(train.text[0], other[0], '\\n', train.text[656], other[656])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is this supposed to work?!?\n",
    "\n",
    "`tfidf.fit_transform(corpus)` returns matrix, while the `sklearn_api.text2bow.Text2BowTransformer().fit_transform(corpus)` returns some \"BOW\" format\n",
    "\n",
    "OK - this might work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', sklearn_api.text2bow.Text2BowTransformer()),\n",
    "    ('lda', sklearn_api.ldamodel.LdaTransformer()),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# print(cross_val_score(pipeline, train.text, train.author, cv=3, n_jobs=3))\n",
    "# print(cross_val_score(pipeline, train.text, train.author, \n",
    "#                 cv=3, n_jobs=3, scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ 0.48360907  0.48850751  0.48613027]<br>\n",
    "[-2.1033608  -2.09791398 -2.30065565]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преди да пообработим текста да видим Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', sklearn_api.text2bow.Text2BowTransformer()),\n",
    "    ('lda', sklearn_api.ldamodel.LdaTransformer()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# print(cross_val_score(pipeline, train.text, train.author, cv=3, n_jobs=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ 0.53722426  0.53463071  0.53563218]<br>\n",
    "CPU times: user 430 ms, sys: 169 ms, total: 599 ms<br>\n",
    "Wall time: 3min 41s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# print(cross_val_score(pipeline, train.text, train.author, \n",
    "#                 cv=3, n_jobs=3, scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[-0.96678405 -0.97080789 -0.97363342]<br>\n",
    "CPU times: user 453 ms, sys: 121 ms, total: 573 ms<br>\n",
    "Wall time: 3min 39s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Още един опит преди да окастрим текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', sklearn_api.text2bow.Text2BowTransformer()),\n",
    "    ('lda', sklearn_api.ldamodel.LdaTransformer(num_topics=21)),\n",
    "    ('clf', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# print(cross_val_score(pipeline, train.text, train.author, \n",
    "#                 cv=3, n_jobs=3, scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[-0.98861154 -0.97565475 -0.99821168]<br>\n",
    "CPU times: user 381 ms, sys: 99.9 ms, total: 481 ms<br>\n",
    "Wall time: 2min 8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "id26305    thi process, however, afford me no mean of asc...\n",
       "id17569    It never onc occur to me that the fumbl might ...\n",
       "id11008    In hi left hand wa a gold snuff box, from whic...\n",
       "id27763    how love is spring As we look from windsor ter...\n",
       "id12958    find noth else, not even gold, the superintend...\n",
       "                                 ...                        \n",
       "id17718    I could have fancied, while I look at it, that...\n",
       "id08973     the lid clench themselv togeth as if in a spasm.\n",
       "id05267    mai il faut agir that is to say, a frenchman n...\n",
       "id17513    for an item of news like this, it strike us it...\n",
       "id00393    He laid a gnarl claw on my shoulder, and it se...\n",
       "Name: stemmed, Length: 19579, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explore.stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "id26305    thi process  however  afford mean ascertain di...\n",
       "id17569                        onc occur fumbl mere mistake \n",
       "id11008    hi left hand wa gold snuff box  which  caper h...\n",
       "id27763    love spring look windsor terrac sixteen fertil...\n",
       "id12958    noth else  gold  superintend abandon hi attemp...\n",
       "                                 ...                        \n",
       "id17718    fancied  look it  emin landscap painter built ...\n",
       "id08973                    lid clench themselv togeth spasm \n",
       "id05267      mai il faut agir say  frenchman faint outright \n",
       "id17513    item news like this  strike wa veri coolli rec...\n",
       "id00393    laid gnarl claw shoulder  shake wa altogeth mi...\n",
       "Name: stemmed2, Length: 19579, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import preprocess_string, strip_punctuation, remove_stopwords\n",
    "\n",
    "# CUSTOM_FILTERS = [strip_punctuation, remove_stopwords]\n",
    "# explore['stemmed2'] = [ preprocess_string(s, CUSTOM_FILTERS) for s in explore.stemmed]\n",
    "# explore.stemmed2\n",
    "\n",
    "explore['stemmed2'] = [ strip_punctuation(remove_stopwords(s.lower())) for s in explore.stemmed]\n",
    "explore.stemmed2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It never onc occur to me that the fumbl might be a mere mistake. ||| onc occur fumbl mere mistake \n"
     ]
    }
   ],
   "source": [
    "print(explore.loc['id17569'].stemmed, '|||', explore.loc['id17569'].stemmed2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline-a си е същия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# print(cross_val_score(pipeline, explore.stemmed2, train.author, \n",
    "#                 cv=3, n_jobs=3, scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[-1.05550683 -1.02313271 -1.02899542]<br>\n",
    "CPU times: user 347 ms, sys: 92.8 ms, total: 440 ms<br>\n",
    "Wall time: 1min 27s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поне е по-бързо. Да видим tfidf и повече фичъри."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow', sklearn_api.text2bow.Text2BowTransformer()),\n",
    "    ('features', sklearn_api.tfidf.TfIdfTransformer()),\n",
    "    ('lda', sklearn_api.ldamodel.LdaTransformer(num_topics=501)),\n",
    "    ('clf', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# print(cross_val_score(pipeline, train.text, train.author, \n",
    "#                 cv=3, n_jobs=3, scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[-0.94941507 -0.95248997 -0.95026712]<br>\n",
    "CPU times: user 1.18 s, sys: 223 ms, total: 1.4 s<br>\n",
    "Wall time: 19min 4s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is frustrating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow', sklearn_api.text2bow.Text2BowTransformer()),\n",
    "    ('features', sklearn_api.tfidf.TfIdfTransformer()),\n",
    "    ('lda', sklearn_api.ldamodel.LdaTransformer()),\n",
    "    ('clf', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# print(cross_val_score(pipeline, train.text, train.author, \n",
    "#                 cv=3, n_jobs=3, scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[-1.03061917 -1.03984701 -1.03884094]<br>\n",
    "CPU times: user 387 ms, sys: 124 ms, total: 511 ms<br>\n",
    "Wall time: 2min 33s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow', sklearn_api.text2bow.Text2BowTransformer()),\n",
    "    ('features', sklearn_api.tfidf.TfIdfTransformer()),\n",
    "    ('lda', sklearn_api.ldamodel.LdaTransformer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# print(cross_val_score(pipeline, train.text, train.author, \n",
    "#                 cv=3, n_jobs=3, scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[-1.06731359 -1.07019065 -1.07019197]<br>\n",
    "CPU times: user 377 ms, sys: 142 ms, total: 519 ms<br>\n",
    "Wall time: 2min 29s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "От къде е тая разлика? Може ли text2bow да чупи? Или е от липсата на хипер парамерти?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(19579 documents, 22328 features, 240166 non-zero entries)\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corpus е минат с word2vec; ще прескоча tfidf за момента и ще пусна LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "# lda = LdaModel(corpus=corpus, id2word=dictionary)\n",
    "# lda.print_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# id2word=dict([(i, s) for i, s in enumerate(dictionary)])\n",
    "# print([ id2word[z] for z in range(10) ])\n",
    "# print(list(id2word.items())[:10])\n",
    "# what = list(enumerate(dictionary))\n",
    "# print(what[:10])\n",
    "# print(list(dictionary)[:10])\n",
    "# print(type(dictionary))\n",
    "# wdk = dictionary.keys()\n",
    "# print(wdk[:10])\n",
    "# print(dictionary[wdk[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Горното даваше topic с референции към IDта вместо към реални думи - не знам защо сега работи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bob/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 44s, sys: 1.48 s, total: 1min 45s\n",
      "Wall time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda = LdaModel(corpus=corpus, id2word=dict(dictionary.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представянето на topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.081*\"said,\" + 0.071*\"nature\" + 0.041*\"given\" + 0.029*\"silent\" + 0.028*\"gone\" + 0.026*\"visited\" + 0.024*\"winter\" + 0.022*\"determined\" + 0.020*\"sent\" + 0.019*\"could\"'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(47,\n",
       "  '0.043*\"words\" + 0.038*\"beheld\" + 0.037*\"trees\" + 0.028*\"endeavoured\" + 0.026*\"big\" + 0.024*\"rapidly\" + 0.022*\"water,\" + 0.022*\"listened\" + 0.019*\"tone\" + 0.019*\"angles\"'),\n",
       " (12,\n",
       "  '0.052*\"men\" + 0.039*\"large\" + 0.032*\"whilst\" + 0.027*\"one\" + 0.025*\"quiet\" + 0.022*\"almost\" + 0.022*\"things\" + 0.017*\"spring\" + 0.017*\"wandered\" + 0.017*\"supply\"'),\n",
       " (29,\n",
       "  '0.093*\"place\" + 0.051*\"continued\" + 0.041*\"usual\" + 0.041*\"moment\" + 0.041*\"course,\" + 0.040*\"cold\" + 0.035*\"easily\" + 0.030*\"effect\" + 0.027*\"time.\" + 0.025*\"position\"'),\n",
       " (78,\n",
       "  '0.049*\"dreams\" + 0.045*\"sound\" + 0.039*\"black\" + 0.031*\"arose\" + 0.030*\"which,\" + 0.029*\"distant\" + 0.028*\"instant\" + 0.025*\"age\" + 0.024*\"wholly\" + 0.022*\"whose\"'),\n",
       " (2,\n",
       "  '0.091*\"mind\" + 0.075*\"full\" + 0.064*\"less\" + 0.053*\"vast\" + 0.049*\"hope\" + 0.042*\"reached\" + 0.042*\"set\" + 0.036*\"sight\" + 0.035*\"forth\" + 0.034*\"portion\"'),\n",
       " (48,\n",
       "  '0.071*\"him,\" + 0.070*\"feel\" + 0.059*\"taken\" + 0.031*\"love,\" + 0.027*\"carefully\" + 0.026*\"toward\" + 0.020*\"mind,\" + 0.019*\"myself.\" + 0.019*\"own,\" + 0.015*\"one\"'),\n",
       " (50,\n",
       "  '0.062*\"went\" + 0.052*\"read\" + 0.049*\"terrible\" + 0.039*\"morning\" + 0.036*\"sat\" + 0.035*\"wild\" + 0.030*\"all,\" + 0.029*\"old\" + 0.025*\"mad\" + 0.023*\"mere\"'),\n",
       " (98,\n",
       "  '0.043*\"unknown\" + 0.035*\"top\" + 0.035*\"lady\" + 0.033*\"use\" + 0.025*\"attempt\" + 0.019*\"well,\" + 0.018*\"fixed\" + 0.018*\"fancied\" + 0.018*\"lofty\" + 0.017*\"ultimate\"'),\n",
       " (94,\n",
       "  '0.090*\"human\" + 0.087*\"thing\" + 0.044*\"kind\" + 0.043*\"give\" + 0.032*\"west\" + 0.027*\"one\" + 0.024*\"windows\" + 0.022*\"left\" + 0.021*\"especially\" + 0.020*\"presence\"'),\n",
       " (64,\n",
       "  '0.078*\"go\" + 0.065*\"this,\" + 0.046*\"together\" + 0.042*\"blood\" + 0.040*\"doubt\" + 0.032*\"hands\" + 0.030*\"things,\" + 0.020*\"raymond,\" + 0.019*\"region\" + 0.019*\"last\"')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.show_topics(num_topics=10, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представянето на запис в латентното пространство. Векторите са 100, но Gensim показва само най-значимите:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 0.046983603),\n",
       " (8, 0.057991676),\n",
       " (9, 0.091455139),\n",
       " (27, 0.056095801),\n",
       " (31, 0.48109618),\n",
       " (34, 0.055483889),\n",
       " (52, 0.041123584),\n",
       " (80, 0.070071869),\n",
       " (87, 0.054198224)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda[corpus[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.0014285713),\n",
       " (1, 0.0014285713),\n",
       " (2, 0.0014285713),\n",
       " (3, 0.0014285713),\n",
       " (4, 0.0014285713),\n",
       " (5, 0.0014285713),\n",
       " (6, 0.0014285713),\n",
       " (7, 0.0014285713),\n",
       " (8, 0.0014285713),\n",
       " (9, 0.0014285713),\n",
       " (10, 0.0014285713),\n",
       " (11, 0.0014285713),\n",
       " (12, 0.0014285713),\n",
       " (13, 0.0014285713),\n",
       " (14, 0.0014285713),\n",
       " (15, 0.0014285713),\n",
       " (16, 0.0014285713),\n",
       " (17, 0.0014285713),\n",
       " (18, 0.0014285713),\n",
       " (19, 0.0014285713),\n",
       " (20, 0.0014285713),\n",
       " (21, 0.0014285713),\n",
       " (22, 0.0014285713),\n",
       " (23, 0.0014285713),\n",
       " (24, 0.0014285713),\n",
       " (25, 0.0014285713),\n",
       " (26, 0.0014285713),\n",
       " (27, 0.0014285713),\n",
       " (28, 0.0014285713),\n",
       " (29, 0.0014285713),\n",
       " (30, 0.0014285713),\n",
       " (31, 0.0014285713),\n",
       " (32, 0.0014285713),\n",
       " (33, 0.0014285713),\n",
       " (34, 0.0014285713),\n",
       " (35, 0.0014285713),\n",
       " (36, 0.0014285713),\n",
       " (37, 0.0014285713),\n",
       " (38, 0.0014285713),\n",
       " (39, 0.0014285713),\n",
       " (40, 0.0014285713),\n",
       " (41, 0.0014285713),\n",
       " (42, 0.0014285713),\n",
       " (43, 0.0014285713),\n",
       " (44, 0.0014285713),\n",
       " (45, 0.0014285713),\n",
       " (46, 0.0014285713),\n",
       " (47, 0.0014285713),\n",
       " (48, 0.0014285713),\n",
       " (49, 0.0014285713),\n",
       " (50, 0.85857147),\n",
       " (51, 0.0014285713),\n",
       " (52, 0.0014285713),\n",
       " (53, 0.0014285713),\n",
       " (54, 0.0014285713),\n",
       " (55, 0.0014285713),\n",
       " (56, 0.0014285713),\n",
       " (57, 0.0014285713),\n",
       " (58, 0.0014285713),\n",
       " (59, 0.0014285713),\n",
       " (60, 0.0014285713),\n",
       " (61, 0.0014285713),\n",
       " (62, 0.0014285713),\n",
       " (63, 0.0014285713),\n",
       " (64, 0.0014285713),\n",
       " (65, 0.0014285713),\n",
       " (66, 0.0014285713),\n",
       " (67, 0.0014285713),\n",
       " (68, 0.0014285713),\n",
       " (69, 0.0014285713),\n",
       " (70, 0.0014285713),\n",
       " (71, 0.0014285713),\n",
       " (72, 0.0014285713),\n",
       " (73, 0.0014285713),\n",
       " (74, 0.0014285713),\n",
       " (75, 0.0014285713),\n",
       " (76, 0.0014285713),\n",
       " (77, 0.0014285713),\n",
       " (78, 0.0014285713),\n",
       " (79, 0.0014285713),\n",
       " (80, 0.0014285713),\n",
       " (81, 0.0014285713),\n",
       " (82, 0.0014285713),\n",
       " (83, 0.0014285713),\n",
       " (84, 0.0014285713),\n",
       " (85, 0.0014285713),\n",
       " (86, 0.0014285713),\n",
       " (87, 0.0014285713),\n",
       " (88, 0.0014285713),\n",
       " (89, 0.0014285713),\n",
       " (90, 0.0014285713),\n",
       " (91, 0.0014285713),\n",
       " (92, 0.0014285713),\n",
       " (93, 0.0014285713),\n",
       " (94, 0.0014285713),\n",
       " (95, 0.0014285713),\n",
       " (96, 0.0014285713),\n",
       " (97, 0.0014285713),\n",
       " (98, 0.0014285713),\n",
       " (99, 0.0014285713)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.get_document_topics(corpus[1], minimum_probability=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19579 19579\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus), len(train.author))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply logistic regression, but first encode topics in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0   1   2   3   4   5   6   7   8   9  ...  90  91  92  93  94  95  \\\n",
      "id                                              ...                           \n",
      "id26305   0   0   0   0   0   0   0   0   0   0 ...   0   0   0   0   0   0   \n",
      "id17569   0   0   0   0   0   0   0   0   0   0 ...   0   0   0   0   0   0   \n",
      "id11008   0   0   0   0   0   0   0   0   0   0 ...   0   0   0   0   0   0   \n",
      "id27763   0   0   0   0   0   0   0   0   0   0 ...   0   0   0   0   0   0   \n",
      "id12958   0   0   0   0   0   0   0   0   0   0 ...   0   0   0   0   0   0   \n",
      "...      ..  ..  ..  ..  ..  ..  ..  ..  ..  .. ...  ..  ..  ..  ..  ..  ..   \n",
      "id17718   0   0   0   0   0   0   0   0   0   0 ...   0   0   0   0   0   0   \n",
      "id08973   0   0   0   0   0   0   0   0   0   0 ...   0   0   0   0   0   0   \n",
      "id05267   0   0   0   0   0   0   0   0   0   0 ...   0   0   0   0   0   0   \n",
      "id17513   0   0   0   0   0   0   0   0   0   0 ...   0   0   0   0   0   0   \n",
      "id00393   0   0   0   0   0   0   0   0   0   0 ...   0   0   0   0   0   0   \n",
      "\n",
      "         96  97  98  99  \n",
      "id                       \n",
      "id26305   0   0   0   0  \n",
      "id17569   0   0   0   0  \n",
      "id11008   0   0   0   0  \n",
      "id27763   0   0   0   0  \n",
      "id12958   0   0   0   0  \n",
      "...      ..  ..  ..  ..  \n",
      "id17718   0   0   0   0  \n",
      "id08973   0   0   0   0  \n",
      "id05267   0   0   0   0  \n",
      "id17513   0   0   0   0  \n",
      "id00393   0   0   0   0  \n",
      "\n",
      "[19579 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "latently = train.copy()\n",
    "for t in range(100):\n",
    "    latently[t]=0\n",
    "\n",
    "print(latently[list(range(100))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for i, doc in enumerate(corpus):\n",
    "    temp = [0]*100\n",
    "    for t in lda[doc]:\n",
    "        temp[t[0]] = t[1]\n",
    "    data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2    3         4      5    6    7         8   \\\n",
      "id                                                                           \n",
      "id26305  0.0  0.000000  0.046983  0.0  0.000000  0.000  0.0  0.0  0.057969   \n",
      "id17569  0.0  0.000000  0.000000  0.0  0.000000  0.000  0.0  0.0  0.000000   \n",
      "id11008  0.0  0.000000  0.000000  0.0  0.000000  0.000  0.0  0.0  0.000000   \n",
      "id27763  0.0  0.000000  0.000000  0.0  0.000000  0.000  0.0  0.0  0.000000   \n",
      "id12958  0.0  0.746294  0.000000  0.0  0.000000  0.000  0.0  0.0  0.000000   \n",
      "...      ...       ...       ...  ...       ...    ...  ...  ...       ...   \n",
      "id17718  0.0  0.000000  0.000000  0.0  0.000000  0.000  0.0  0.0  0.000000   \n",
      "id08973  0.0  0.000000  0.000000  0.0  0.336667  0.000  0.0  0.0  0.000000   \n",
      "id05267  0.0  0.000000  0.000000  0.0  0.000000  0.000  0.0  0.0  0.000000   \n",
      "id17513  0.0  0.000000  0.000000  0.0  0.000000  0.202  0.0  0.0  0.000000   \n",
      "id00393  0.0  0.000000  0.000000  0.0  0.000000  0.000  0.0  0.0  0.000000   \n",
      "\n",
      "               9  ...    90        91        92   93   94       95   96  \\\n",
      "id                ...                                                     \n",
      "id26305  0.091297 ...   0.0  0.000000  0.000000  0.0  0.0  0.00000  0.0   \n",
      "id17569  0.000000 ...   0.0  0.000000  0.000000  0.0  0.0  0.00000  0.0   \n",
      "id11008  0.000000 ...   0.0  0.000000  0.000000  0.0  0.0  0.00000  0.0   \n",
      "id27763  0.000000 ...   0.0  0.000000  0.091735  0.0  0.0  0.00000  0.0   \n",
      "id12958  0.000000 ...   0.0  0.000000  0.000000  0.0  0.0  0.00000  0.0   \n",
      "...           ... ...   ...       ...       ...  ...  ...      ...  ...   \n",
      "id17718  0.000000 ...   0.0  0.000000  0.000000  0.0  0.0  0.12728  0.0   \n",
      "id08973  0.000000 ...   0.0  0.000000  0.000000  0.0  0.0  0.00000  0.0   \n",
      "id05267  0.000000 ...   0.0  0.385181  0.000000  0.0  0.0  0.00000  0.0   \n",
      "id17513  0.000000 ...   0.0  0.000000  0.000000  0.0  0.0  0.00000  0.0   \n",
      "id00393  0.000000 ...   0.0  0.000000  0.000000  0.0  0.0  0.00000  0.0   \n",
      "\n",
      "               97   98   99  \n",
      "id                           \n",
      "id26305  0.000000  0.0  0.0  \n",
      "id17569  0.000000  0.0  0.0  \n",
      "id11008  0.045156  0.0  0.0  \n",
      "id27763  0.000000  0.0  0.0  \n",
      "id12958  0.000000  0.0  0.0  \n",
      "...           ...  ...  ...  \n",
      "id17718  0.000000  0.0  0.0  \n",
      "id08973  0.000000  0.0  0.0  \n",
      "id05267  0.202000  0.0  0.0  \n",
      "id17513  0.000000  0.0  0.0  \n",
      "id00393  0.000000  0.0  0.0  \n",
      "\n",
      "[19579 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "for t in range(100):\n",
    "    latently[t] = [ data[z][t] for z in range(len(data)) ]\n",
    "\n",
    "print(latently[list(range(100))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "AUTHOR_ENCODING = {'EAP': 0, 'MWS': 1, 'HPL': 2}\n",
    "latently['encoded_author'] = [ AUTHOR_ENCODING[z] for z in train.author ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19579, 103)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latently.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(latently[list(range(100))], latently.encoded_author, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 77.2 ms, sys: 12.1 ms, total: 89.3 ms\n",
      "Wall time: 88.1 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.41\n",
      "Test  score: 0.40\n",
      "Log Loss score 1.08282020218\n"
     ]
    }
   ],
   "source": [
    "print(\"Train score: {:.2f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test  score: {:.2f}\".format(logreg.score(X_test, y_test)))\n",
    "prediction = logreg.predict_proba(X_test)\n",
    "logloss = log_loss(y_test, prediction, labels=[0, 1, 2])\n",
    "print('Log Loss score', logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.40762581  0.29947673  0.29289746]\n",
      " [ 0.401099    0.31041542  0.28848558]\n",
      " [ 0.4100876   0.30027008  0.28964232]\n",
      " [ 0.41722321  0.28798965  0.29478714]\n",
      " [ 0.40455119  0.30231512  0.29313369]\n",
      " [ 0.40296533  0.30548759  0.29154708]\n",
      " [ 0.41068999  0.30681649  0.28249352]\n",
      " [ 0.39780867  0.31690806  0.28528327]\n",
      " [ 0.40133745  0.30606794  0.29259461]\n",
      " [ 0.40204636  0.30801855  0.28993509]]\n"
     ]
    }
   ],
   "source": [
    "print(prediction[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0         1         2         3    4         5         6    7   \\\n",
      "id                                                                             \n",
      "id15695  0.12625  0.000000  0.000000  0.000000  0.0  0.000000  0.000000  0.0   \n",
      "id07954  0.00000  0.000000  0.000000  0.000000  0.0  0.000000  0.000000  0.0   \n",
      "id16303  0.00000  0.000000  0.000000  0.000000  0.0  0.043915  0.041232  0.0   \n",
      "id07932  0.00000  0.000000  0.092798  0.000000  0.0  0.000000  0.000000  0.0   \n",
      "id20875  0.00000  0.000000  0.000000  0.000000  0.0  0.000000  0.000000  0.0   \n",
      "id14743  0.00000  0.076833  0.000000  0.077333  0.0  0.000000  0.000000  0.0   \n",
      "id07281  0.00000  0.000000  0.000000  0.000000  0.0  0.000000  0.000000  0.0   \n",
      "id09240  0.00000  0.252500  0.000000  0.000000  0.0  0.000000  0.000000  0.0   \n",
      "id23995  0.00000  0.000000  0.000000  0.000000  0.0  0.000000  0.000000  0.0   \n",
      "id15141  0.50500  0.000000  0.000000  0.000000  0.0  0.000000  0.000000  0.0   \n",
      "\n",
      "               8    9     ...         90   91        92   93   94   95   96  \\\n",
      "id                        ...                                                 \n",
      "id15695  0.000000  0.0    ...     0.0000  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
      "id07954  0.000000  0.0    ...     0.0000  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
      "id16303  0.000000  0.0    ...     0.0000  0.0  0.058261  0.0  0.0  0.0  0.0   \n",
      "id07932  0.000000  0.0    ...     0.0000  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
      "id20875  0.091732  0.0    ...     0.0000  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
      "id14743  0.000000  0.0    ...     0.0000  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
      "id07281  0.000000  0.0    ...     0.0000  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
      "id09240  0.000000  0.0    ...     0.2525  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
      "id23995  0.000000  0.0    ...     0.0000  0.0  0.112222  0.0  0.0  0.0  0.0   \n",
      "id15141  0.000000  0.0    ...     0.0000  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
      "\n",
      "          97   98        99  \n",
      "id                           \n",
      "id15695  0.0  0.0  0.000000  \n",
      "id07954  0.0  0.0  0.000000  \n",
      "id16303  0.0  0.0  0.051561  \n",
      "id07932  0.0  0.0  0.000000  \n",
      "id20875  0.0  0.0  0.000000  \n",
      "id14743  0.0  0.0  0.000000  \n",
      "id07281  0.0  0.0  0.000000  \n",
      "id09240  0.0  0.0  0.000000  \n",
      "id23995  0.0  0.0  0.112222  \n",
      "id15141  0.0  0.0  0.000000  \n",
      "\n",
      "[10 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0] id\n",
      "id15695    0\n",
      "id07954    1\n",
      "id16303    1\n",
      "id07932    0\n",
      "id20875    2\n",
      "id14743    1\n",
      "id07281    0\n",
      "id09240    0\n",
      "id23995    0\n",
      "id15141    0\n",
      "Name: encoded_author, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print([ list(z).index(max(z)) for z in prediction[:10] ], y_test[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we have way too many topics for this to work, let's see what the efect of only 3 topics is. I'll use the manual approach instead of Pipleline, since I don't know how data conversion is being handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45 s, sys: 556 ms, total: 45.6 s\n",
      "Wall time: 45.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda3 = LdaModel(corpus=corpus, num_topics=3, id2word=dict(dictionary.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      text author         0  \\\n",
      "id                                                                            \n",
      "id26305  This process, however, afforded me no means of...    EAP  0.021857   \n",
      "id17569  It never once occurred to me that the fumbling...    HPL  0.524008   \n",
      "id11008  In his left hand was a gold snuff box, from wh...    EAP  0.019030   \n",
      "id27763  How lovely is spring As we looked from Windsor...    MWS  0.963659   \n",
      "id12958  Finding nothing else, not even gold, the Super...    HPL  0.138102   \n",
      "...                                                    ...    ...       ...   \n",
      "id17718  I could have fancied, while I looked at it, th...    EAP  0.048320   \n",
      "id08973  The lids clenched themselves together as if in...    EAP  0.095075   \n",
      "id05267  Mais il faut agir that is to say, a Frenchman ...    EAP  0.050181   \n",
      "id17513  For an item of news like this, it strikes us i...    EAP  0.377603   \n",
      "id00393  He laid a gnarled claw on my shoulder, and it ...    HPL  0.175415   \n",
      "\n",
      "                1         2  \n",
      "id                           \n",
      "id26305  0.209575  0.768568  \n",
      "id17569  0.420901  0.055091  \n",
      "id11008  0.019289  0.961682  \n",
      "id27763  0.019634  0.016707  \n",
      "id12958  0.022669  0.839228  \n",
      "...           ...       ...  \n",
      "id17718  0.046843  0.904837  \n",
      "id08973  0.325528  0.579397  \n",
      "id05267  0.049178  0.900641  \n",
      "id17513  0.211514  0.410883  \n",
      "id00393  0.043254  0.781330  \n",
      "\n",
      "[19579 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "late3 = train.copy()\n",
    "\n",
    "data = []\n",
    "for i, doc in enumerate(corpus):\n",
    "    temp = [0, 0, 0]\n",
    "    for t in lda3[doc]:\n",
    "        temp[t[0]] = t[1]\n",
    "    data.append(temp)\n",
    "\n",
    "late3[0] = [ data[z][0] for z in range(len(data)) ]\n",
    "late3[1] = [ data[z][1] for z in range(len(data)) ]\n",
    "late3[2] = [ data[z][2] for z in range(len(data)) ]\n",
    "\n",
    "print(late3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "late3['encoded_author'] = [ AUTHOR_ENCODING[z] for z in train.author ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(late3[[0,1,2]], late3.encoded_author, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.7 ms, sys: 76 µs, total: 43.8 ms\n",
      "Wall time: 43.6 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.41\n",
      "Test  score: 0.40\n",
      "Log Loss score 1.08617640427\n"
     ]
    }
   ],
   "source": [
    "print(\"Train score: {:.2f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test  score: {:.2f}\".format(logreg.score(X_test, y_test)))\n",
    "prediction = logreg.predict_proba(X_test)\n",
    "logloss = log_loss(y_test, prediction, labels=[0, 1, 2])\n",
    "print('Log Loss score', logloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we dont use the LDA on it's own, but as extra features on top of the CountVectorizer? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-8a787d4b3ea1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvectorized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36mtodense\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mshares\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \"\"\"\n\u001b[0;32m--> 792\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Output array must be C or F contiguous'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__numpy_ufunc__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vectorized = vectorizer.fit_transform(train.text).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will use approach from http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html in another NB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-64-a2f04ec58aab>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-64-a2f04ec58aab>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    fail here\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "fail here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Dropout, Lambda, Concatenate\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras import regularizers, objectives, metrics\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Откри приблизително същите параметри, но не успя да стигне напълно до същия резултат.\n",
    "\n",
    "Ще използвам следния модел:\n",
    "\n",
    "TfIdf + MultinomialNB, без стеминг на текста.\n",
    "\n",
    "Mean validation score: -0.423 (std: 0.003)\n",
    "\n",
    "Ще ползвам и следните параметри:\n",
    "\n",
    "Parameters: {'features__stop_words': None, 'features__ngram_range': (1, 2), 'features__min_df': 2, 'features__max_df': 0.8, 'features__lowercase': False, 'features__analyzer': 'word', 'clf__alpha': 0.01}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Последна проверка на този модел за `LogLoss` и `Accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', TfidfVectorizer(ngram_range=(1, 2), min_df=2,\n",
    "                                 max_df=0.8, lowercase=False)),\n",
    "    ('clf', MultinomialNB(alpha=0.01))\n",
    "])\n",
    "\n",
    "print(cross_val_score(pipeline, train.text, train.author, cv=3, n_jobs=3))\n",
    "print(cross_val_score(pipeline, train.text, train.author, cv=3, n_jobs=3, \n",
    "                      scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Трениране на модел и събмит\n",
    "\n",
    "Първо да видим в какъв формат трябва да се подадат резултатите за тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"data/spooky-authors/sample_submission.zip\")\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = pipeline.fit(train.text, train.author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(pipeline.predict_proba(test[:10].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "test_predictions = pipeline.predict_proba(test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(pipeline.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "submit_file = pd.DataFrame(test_predictions, columns=['EAP', 'MWS', 'HPL'], index=test.index)\n",
    "submit_file.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "submit_file.to_csv(\"data/spooky-authors/submit_Tfidf_MNB_text.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Очакванията за събмита са да имаме скор някъде около 0.41 - 0.42.\n",
    "\n",
    "Може да е малко по-добър защото при крос-валидацията тренирахме на 13к и тествахме 6к.\n",
    "\n",
    "Сега трейн сета е целия: 19.5к"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![submit-result.png](attachment:submit-result.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Да хакнем ранкинга в кагъл?\n",
    "\n",
    "print(test.text[:5].values)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "none",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
